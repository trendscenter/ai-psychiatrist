{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b812173d",
   "metadata": {},
   "source": [
    "# Imports + Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd2fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#!pip install --upgrade typing_extensions\n",
    "#!pip install pydantic\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from typing import Union\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ollama Config\n",
    "OLLAMA_NODE = \"arctrdagn041\"\n",
    "BASE_URL = f\"http://{OLLAMA_NODE}:11434/api/chat\"\n",
    "\n",
    "model = \"gemma3-optimized:27b\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f917ba",
   "metadata": {},
   "source": [
    "## Grabbing all participant IDs that have all the PHQ-8 questionare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8194eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant: <synch>\n",
      "Ellie: IntroV4Confirmation (hi i'm ellie thanks for coming in today i was created to talk to people in a safe and secure environment i'm not a therapist but i'm here to learn about people and would love to learn about you i'll ask a few questions to get us started and please feel free to tell me anything your answers are totally confidential are you ok with this)\n",
      "Participant: mm\n",
      "Participant: yes i am\n",
      "Ellie: okay_confirm (okay)\n",
      "Ellie: how_doingV (so how are you doing today)\n",
      "Participant: um i'm okay\n",
      "Ellie: thats_good (that's good)\n",
      "Ellie: where_originally (where are you from originally)\n",
      "Participant: chicago\n",
      "Ellie: really (really)\n",
      "Ellie: when_LA (when did you move to l_a)\n",
      "Participant: um back and forth over the years and traveled a little bit\n",
      "Ellie: often_backB (how often do you go back to your hometown)\n",
      "Participant: haven't been out there in over fifteen sixteen years\n",
      "Ellie: wow (wow)\n",
      "Ellie: why2 (why)\n",
      "Participant: um\n",
      "Participant: kinda got stuck out here <laughter> then i had my baby and i just stayed\n",
      "Ellie: compares_LA (how does it compare to l_a)\n",
      "Participant: uh\n",
      "Participant: different food um\n",
      "Participant: the people get along way um how do i get it it's kind of a culture shock\n",
      "Ellie: tell_about_that (can you tell me about that)\n",
      "Participant: uh\n",
      "Participant: out there just on the food basis they like to give you a lotta food\n",
      "Participant: that tastes real good for a little bit of money to keep a lifelong customer but out here they wanna go up there and give you a little bit of food for a whole lotta money so they just making money <laughter>\n",
      "Ellie: wild_laughter5 ((laughter))\n",
      "Ellie: yeah_downer (yeah)\n",
      "Ellie: why_LA (why did you move to l_a)\n",
      "Participant: um\n",
      "Participant: mm parent had business out here\n",
      "Ellie: adapted_LA (how easy was it for you to get used to living in l_a)\n",
      "Participant: um\n",
      "Participant: not hard\n",
      "Participant: i could get used to anywhere\n",
      "Ellie: thats_good (that's good)\n",
      "Ellie: like_about_LA (what are some things you really like about l_a)\n",
      "Participant: the views and the ocean\n",
      "Ellie: dont_like_LA (what are some things you don't really like about l_a)\n",
      "Participant: uh\n",
      "Participant: just uh\n",
      "Participant: a splitting of people that's it <laughter>\n",
      "Ellie: mhm (mhm)\n",
      "Ellie: travel_shoes (i'm sure you can tell by my shoes i'm not much of a world explorer do you travel a lot)\n",
      "Participant: um just a few places\n",
      "Ellie: travel_changed (what do you enjoy about traveling)\n",
      "Participant: getting to see different people and learn how different people get along and interact with each other\n",
      "Ellie: uh_huh (uh huh)\n",
      "Ellie: travel_trips (i'd love to hear about one of your trips)\n",
      "Participant: <laughter> um\n",
      "Participant: <laughter>\n",
      "Participant: uh\n",
      "Participant: <sigh>\n",
      "Participant: i remember a trip years ago where i went to mexico with my brother\n",
      "Ellie: nice (nice)\n",
      "Ellie: memorableB (what's one of your most memorable experiences)\n",
      "Participant: yeah\n",
      "Participant: um\n",
      "Participant: jumping into a lake <laughter> pretty much off the side of a mountain <laughter>\n",
      "Ellie: awesome (awesome)\n",
      "Participant: yeah\n",
      "Ellie: study (what did you study at school)\n",
      "Participant: everything\n",
      "Participant: <laughter>\n",
      "Participant: there's not a subject i don't like\n",
      "Ellie: dream_job (what's your dream job)\n",
      "Participant: um\n",
      "Participant: actually my dream job\n",
      "Participant: working with computers and helping people\n",
      "Ellie: really (really)\n",
      "Ellie: why2 (why)\n",
      "Participant: that's\n",
      "Participant: where <wh> where my heart is i like computers and i also like helping people\n",
      "Ellie: thats_great (that's great)\n",
      "Ellie: relax_fishtank (sometimes when i'm feeling tense i turn on the fish tank screensaver hey i know it's not hawaii but it's the best i've got what do you do to relax)\n",
      "Participant: <laughter>\n",
      "Participant: i meditate um work out\n",
      "Participant: i run i walk\n",
      "Participant: um\n",
      "Participant: i write read\n",
      "Ellie: uh_huh (uh huh)\n",
      "Ellie: shyoutgoing (do you consider yourself more shy or outgoing)\n",
      "Participant: in the median\n",
      "Ellie: okay_confirm (okay)\n",
      "Ellie: control_temper (how are you at controlling your temper)\n",
      "Participant: mm\n",
      "Participant: i'd say very good\n",
      "Ellie: last_argument (when was the last time you argued with someone and what was it about)\n",
      "Participant: oh yeah that doesn't make it seem that good <laughter> um last time i argued\n",
      "Participant: was\n",
      "Participant: over someone actually spitting out a racial slur\n",
      "Participant: towards my um wife\n",
      "Ellie: oh_no (oh no)\n",
      "Ellie: describe_felt (how did you feel in that moment)\n",
      "Participant: angry\n",
      "Participant: upset hurt\n",
      "Participant: disrespected\n",
      "Ellie: yeah_downer (yeah)\n",
      "Participant: and especially when i went up there to treat the person actually i was treating 'em with dignity and respect and trying to help the person at first\n",
      "Ellie: sorry_hear (i'm sorry to hear that)\n",
      "Participant: it's no problem people are people\n",
      "Ellie: situation_handled (tell me about a situation that you wish you had handled differently)\n",
      "Participant: mm\n",
      "Participant: i had a chance in school to really go a lot further than i did\n",
      "Participant: and i let something very small\n",
      "Participant: and meaningless stop me in the pursuit of it\n",
      "Ellie: change_directions (what made you decide to do that)\n",
      "Participant: um\n",
      "Participant: <laughter>\n",
      "Participant: ignorance\n",
      "Participant: <laughter> for lack of a better word\n",
      "Ellie: avoid (could you have done anything to avoid it)\n",
      "Participant: now that i think back on it i could've done a whole lot to avoid it but i was just young and foolish\n",
      "Ellie: yeah_downer (yeah)\n",
      "Ellie: hard_decisionB (tell me about the hardest decision you've ever had to make)\n",
      "Participant: to start living again\n",
      "Ellie: okay_confirm (okay)\n",
      "Ellie: memory_erase (tell me about an event or something that you wish you could erase from your memory)\n",
      "Participant: <deep breath> <sigh>\n",
      "Participant: um\n",
      "Participant: as much pain as i've seen in the world\n",
      "Ellie: tell_about_that (can you tell me about that)\n",
      "Participant: um\n",
      "Participant: a compilation of all the events uh just different bad things that i've seen\n",
      "Participant: i\n",
      "Participant: if i could change it i would but\n",
      "Participant: sometimes\n",
      "Participant: i guess things are there for a reason\n",
      "Participant: even though we may not understand it\n",
      "Ellie: yeah_downer (yeah)\n",
      "Ellie: see_mean (i see what you mean)\n",
      "Ellie: feelguilty (what's something you feel guilty about)\n",
      "Participant: <laughter>\n",
      "Participant: um\n",
      "Participant: i have no answer to that\n",
      "Ellie: okay_confirm (okay)\n",
      "Ellie: family_relationship (tell me about your relationship with your family)\n",
      "Participant: um\n",
      "Participant: blood family we really don't talk um\n",
      "Participant: my wife\n",
      "Participant: don't know if she really wants to be there with me or not\n",
      "Participant: she thinks that i'm doing everything in the world but that i'm not\n",
      "Participant: and really <laughter>\n",
      "Participant: i don't understand\n",
      "Participant: i know my daughter loves me but i don't know if she wants to be with me or not\n",
      "Ellie: why2 (why)\n",
      "Participant: because\n",
      "Participant: when somebody talks to you\n",
      "Participant: when they say something a certain amount of times and the tonage in they voice and the belief in their eyes\n",
      "Participant: if you say something so many times it's what really lies in your heart\n",
      "Participant: <sharp inhale> so you could say whatever through your mouth but it's what really is there\n",
      "Ellie: isee_downer (i see)\n",
      "Ellie: kids_elaborate (tell me about your kids)\n",
      "Participant: hmm i love my daughter\n",
      "Participant: she a angel i cut her umbilical cord\n",
      "Participant: i went out there and <laughter> put on her first shirt\n",
      "Participant: i seen her come out\n",
      "Participant: she looked at me right out the stomach <laughter> she wouldn't she was fighting with the doctors wouldn't let 'em touch her a bit\n",
      "Participant: i talked to her i told her to calm down or i told her it was daddy she looked up at me grabbed my fingers and like just calmed down and just let 'em do everything they needed to do\n",
      "Participant: <laughter>\n",
      "Ellie: aw (aw)\n",
      "Ellie: easy_parent (do you find it easy to be a parent)\n",
      "Participant: no it's constantly changing task um\n",
      "Participant: what you have to do on this day won't be the same thing as the next or the next after that it's constantly um\n",
      "Participant: what is a better word for it um\n",
      "Participant: <laughter> um\n",
      "Participant: adjusting\n",
      "Ellie: yeah3 (yeah)\n",
      "Participant: <laughter>\n",
      "Ellie: parent_hardest (what's the hardest thing about being a parent)\n",
      "Participant: um\n",
      "Participant: the hardest thing i'll probably say\n",
      "Participant: is\n",
      "Participant: scolding your child because you know you're doing it out of love for 'em not xxx learn the wrong lessons\n",
      "Ellie: parent_best (what's the best thing about being a parent)\n",
      "Participant: <laughter> seeing your baby smile in appreciation\n",
      "Ellie: parent_differences (what are some ways that you're different as a parent than your parents)\n",
      "Participant: i try to guard my baby a lot more from things that she sees\n",
      "Ellie: give_example (can you give me an example of that)\n",
      "Participant: um\n",
      "Participant: when i was younger there were a lotta things that i seen that no child or adult should've seen\n",
      "Participant: and um\n",
      "Participant: when <whe>\n",
      "Participant: i grew up\n",
      "Participant: i swore to myself that i'll always take care of my child and\n",
      "Participant: i wouldn't disappear or\n",
      "Participant: let her be around things like that\n",
      "Ellie: okay_confirm (okay)\n",
      "Ellie: influence_positive (who's someone that's been a positive influence in your life)\n",
      "Participant: um\n",
      "Participant: my old foster parents\n",
      "Ellie: tell_about_that (can you tell me about that)\n",
      "Participant: mm\n",
      "Participant: they basically showed me no matter where you come from\n",
      "Participant: you can always strive to get better\n",
      "Ellie: okay_confirm (okay)\n",
      "Ellie: easy_sleep (how easy is it for you to get a good night's sleep)\n",
      "Participant: um\n",
      "Participant: a good night's sleep\n",
      "Participant: eh um it comes\n",
      "Ellie: sleep_affects (what are you like when you don't sleep well)\n",
      "Participant: <laughter>\n",
      "Participant: the same just tired and yawning <laughter>\n",
      "Ellie: feel_lately (how have you been feeling lately)\n",
      "Participant: how have i been feeling\n",
      "Participant: mm\n",
      "Participant: like\n",
      "Participant: i have a wealth of queries and\n",
      "Participant: a wealth of gold\n",
      "Participant: within my\n",
      "Participant: grasp and it's just right out of my reach because i just need that one\n",
      "Participant: connector to really just\n",
      "Participant: see what i'm capable of and i know i'm capable of a lot it's just\n",
      "Participant: i need to make that connection\n",
      "Ellie: hmm_downer (hmm)\n",
      "Ellie: behavior_changes (have you noticed any changes in your behavior or thoughts lately)\n",
      "Participant: no\n",
      "Ellie: ptsd_diagnosed (have you ever been diagnosed with p_t_s_d)\n",
      "Participant: <sigh>\n",
      "Participant: um\n",
      "Participant: mm not p_t_s_d no\n",
      "Ellie: depression_diagnosed (have you been diagnosed with depression)\n",
      "Participant: uh\n",
      "Participant: i said yes but\n",
      "Participant: i don't figure i'm any more depressed or anything than any other person everyone has their different breaking points\n",
      "Ellie: okay_confirm (okay)\n",
      "Ellie: happy_lasttime (tell me about the last time you felt really happy)\n",
      "Participant: my daughter's birthday on the twenty sixth\n",
      "Ellie: tell_about_that (can you tell me about that)\n",
      "Participant: um took her out to chuck e cheese she had fun she played games she was on bikes <deep breath> she was crawling through puzzle games um\n",
      "Participant: man this girl did about everything <laughter>\n",
      "Ellie: great_situation (that sounds like a great situation)\n",
      "Ellie: BF_describe (how would your best friend describe you)\n",
      "Participant: um\n",
      "Participant: fun loyal caring\n",
      "Ellie: self_change (what are some things you wish you could change about yourself)\n",
      "Participant: <sigh>\n",
      "Participant: um\n",
      "Participant: something to change\n",
      "Participant: i would like to be more accurate\n",
      "Ellie: okay_confirm (okay)\n",
      "Ellie: regret (is there anything you regret)\n",
      "Participant: <sigh>\n",
      "Participant: not getting to know who my mom was\n",
      "Ellie: advice_back (what advice would you give to yourself ten or twenty years ago)\n",
      "Participant: stop thinking about whatever else has happened and just do what you need to do\n",
      "Ellie: hmm_downer (hmm)\n",
      "Ellie: Ellie17Dec2012_07 (what would you say are some of your best qualities)\n",
      "Participant: i'm positive i'm goal-oriented\n",
      "Participant: determined\n",
      "Participant: um\n",
      "Participant: i'm very imaginative very creative\n",
      "Ellie: ideal_weekendC (tell me how you spend your ideal weekend)\n",
      "Participant: <laughter> ideal\n",
      "Participant: i would probably\n",
      "Participant: be doing a little bit of everything\n",
      "Participant: rock climbing biking\n",
      "Participant: hiking going with my daughter places um\n",
      "Participant: <laughter> man\n",
      "Participant: i'll probably be skiing snowboarding and everything <laughter> in a ideal weekend\n",
      "Ellie: nice (nice)\n",
      "Ellie: Ellie17Dec2012_08 (what are you most proud of in your life)\n",
      "Participant: my daughter\n",
      "Ellie: asked_everything (okay i think i have asked everything i need to)\n",
      "Ellie: appreciate_open (thanks for sharing your thoughts with me)\n",
      "Participant: no problem\n",
      "Ellie: bye (goodbye)\n",
      "Participant: goodbye\n",
      "--------------------------------------------------\n",
      "                     89\n",
      "Participant_ID      420\n",
      "PHQ8_Binary           0\n",
      "PHQ8_Score            3\n",
      "Gender                1\n",
      "PHQ8_NoInterest       0\n",
      "PHQ8_Depressed        0\n",
      "PHQ8_Sleep            2\n",
      "PHQ8_Tired            0\n",
      "PHQ8_Appetite         1\n",
      "PHQ8_Failure          0\n",
      "PHQ8_Concentrating    0\n",
      "PHQ8_Moving           0\n"
     ]
    }
   ],
   "source": [
    "dev_split_phq8 = pd.read_csv(r\"/data/users4/user/ai-psychiatrist/datasets/daic_woz_dataset/dev_split_Depression_AVEC2017.csv\")\n",
    "train_split_phq8 = pd.read_csv(r\"/data/users4/user/ai-psychiatrist/datasets/daic_woz_dataset/train_split_Depression_AVEC2017.csv\")\n",
    "# Combining the 2 dataframes\n",
    "phq8_ground_truths = pd.concat([dev_split_phq8, train_split_phq8], ignore_index=True)\n",
    "# Sort by Participant_ID\n",
    "phq8_ground_truths = phq8_ground_truths.sort_values('Participant_ID').reset_index(drop=True)\n",
    "\n",
    "participant_id = 420\n",
    "current_transcript = pd.read_csv(fr\"/data/users4/user/ai-psychiatrist/datasets/daic_woz_dataset/{participant_id}_P/{participant_id}_TRANSCRIPT.csv\", sep=\"\\t\")\n",
    "# Reformatting transcript data to be a string with speaker name + text\n",
    "current_patient_transcript = '\\n'.join(current_transcript['speaker'] + ': ' + current_transcript['value'])\n",
    "\n",
    "print(current_patient_transcript)\n",
    "print(\"-\"*50)\n",
    "print(phq8_ground_truths.loc[phq8_ground_truths['Participant_ID'] == participant_id].T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57994a20",
   "metadata": {},
   "source": [
    "## Split data into stratified Train, Validation, and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7995b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify data based on Gender + PHQ8_Score\n",
    "phq8_ground_truths['strat_var'] = (phq8_ground_truths['Gender'].astype(str) + '_' + \n",
    "                                phq8_ground_truths['PHQ8_Score'].astype(str))\n",
    "\n",
    "print(f\"Stratification groups (Gender_PHQ8Score):\\n{phq8_ground_truths['strat_var'].value_counts().sort_index()}\")\n",
    "\n",
    "\n",
    "# Getting category sizes\n",
    "strat_counts = phq8_ground_truths['strat_var'].value_counts()\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(f\"Categories with >= 3 subjects: {(strat_counts >= 3).sum()}\")\n",
    "print(f\"Categories with 2 subjects: {(strat_counts == 2).sum()}\")\n",
    "print(f\"Categories with 1 subject: {(strat_counts == 1).sum()}\")\n",
    "\n",
    "# Separate participants into different groups based on category size\n",
    "categories_gte3 = strat_counts[strat_counts >= 3].index.tolist()\n",
    "categories_eq2 = strat_counts[strat_counts == 2].index.tolist()\n",
    "categories_eq1 = strat_counts[strat_counts == 1].index.tolist()\n",
    "\n",
    "# Get participant IDs for each group\n",
    "participants_gte3 = phq8_ground_truths[phq8_ground_truths['strat_var'].isin(categories_gte3)]\n",
    "participants_eq2 = phq8_ground_truths[phq8_ground_truths['strat_var'].isin(categories_eq2)]\n",
    "participants_eq1 = phq8_ground_truths[phq8_ground_truths['strat_var'].isin(categories_eq1)]\n",
    "\n",
    "train_ids = []\n",
    "val_ids = []\n",
    "test_ids = []\n",
    "\n",
    "# Process categories with >= 3 subjects using sklearn\n",
    "if len(participants_gte3) > 0:\n",
    "    # Group by category to handle small categories specially\n",
    "    for category in categories_gte3:\n",
    "        category_participants = participants_gte3[participants_gte3['strat_var'] == category]['Participant_ID'].tolist()\n",
    "        n = len(category_participants)\n",
    "        \n",
    "        # Calculate ideal splits\n",
    "        ideal_train = n * 0.40\n",
    "        ideal_val = n * 0.30\n",
    "        ideal_test = n * 0.30\n",
    "        \n",
    "        # Use consistent 40/30/30 split for all categories >= 3\n",
    "        actual_train = round(ideal_train)\n",
    "        actual_val = round(ideal_val) \n",
    "        actual_test = n - actual_train - actual_val\n",
    "\n",
    "        # Ensure no negative values\n",
    "        if actual_test < 0:\n",
    "            actual_train = max(1, actual_train - 1)\n",
    "            actual_test = n - actual_train - actual_val\n",
    "        \n",
    "        # Randomly assign participants\n",
    "        np.random.seed(42)\n",
    "        shuffled = category_participants.copy()\n",
    "        np.random.shuffle(shuffled)\n",
    "        \n",
    "        train_ids.extend(shuffled[:actual_train])\n",
    "        val_ids.extend(shuffled[actual_train:actual_train+actual_val])\n",
    "        test_ids.extend(shuffled[actual_train+actual_val:])\n",
    "        \n",
    "# Process categories with exactly 2 subjects (1 for train, 1 for test)\n",
    "if len(participants_eq2) > 0:\n",
    "    for category in categories_eq2:\n",
    "        category_participants = participants_eq2[participants_eq2['strat_var'] == category]['Participant_ID'].tolist()\n",
    "        # Randomly assign 1 to train and 1 to test\n",
    "        np.random.shuffle(category_participants)\n",
    "        train_ids.append(category_participants[0])\n",
    "        test_ids.append(category_participants[1])\n",
    "    \n",
    "    print(f\"\\nProcessed {len(participants_eq2)} participants from categories with 2 subjects\")\n",
    "    print(f\"  {len(categories_eq2)} categories: 1 to train, 1 to test each\")\n",
    "    \n",
    "    print(f\"\\nProcessed {len(participants_eq2)} participants from categories with 2 subjects\")\n",
    "    print(f\"  {len(categories_eq2)} categories: 1 to train, 1 to validation each\")\n",
    "\n",
    "# Process categories with exactly 1 subject (all go to train)\n",
    "if len(participants_eq1) > 0:\n",
    "    train_ids.extend(participants_eq1['Participant_ID'].tolist())\n",
    "    print(f\"\\nProcessed {len(participants_eq1)} participants from categories with 1 subject\")\n",
    "    print(f\"  All {len(participants_eq1)} assigned to train set\")\n",
    "\n",
    "# Post-Prossessing Override: Handle cases where PHQ8_Score has exactly 2 participants (regardless of gender) after previous stratification takes place\n",
    "phq8_score_counts = phq8_ground_truths['PHQ8_Score'].value_counts()\n",
    "scores_with_2 = phq8_score_counts[phq8_score_counts == 2].index.tolist()\n",
    "\n",
    "for score in scores_with_2:\n",
    "    score_participants = phq8_ground_truths[phq8_ground_truths['PHQ8_Score'] == score]['Participant_ID'].tolist()\n",
    "    \n",
    "    # Remove these participants from their current assignments\n",
    "    for pid in score_participants:\n",
    "        if pid in train_ids:\n",
    "            train_ids.remove(pid)\n",
    "        if pid in val_ids:\n",
    "            val_ids.remove(pid)\n",
    "        if pid in test_ids:\n",
    "            test_ids.remove(pid)\n",
    "    \n",
    "    # Reassign: 1 to validation, 1 to test\n",
    "    np.random.shuffle(score_participants)\n",
    "    val_ids.append(score_participants[0])\n",
    "    test_ids.append(score_participants[1])\n",
    "    \n",
    "    print(f\"Override applied for PHQ8_Score={score}: 1 to validation, 1 to test\")\n",
    "\n",
    "print(f\"\\nFinal split sizes:\")\n",
    "print(f\"Train: {len(train_ids)} ({len(train_ids)/len(phq8_ground_truths)*100:.1f}%)\")\n",
    "print(f\"Validation: {len(val_ids)} ({len(val_ids)/len(phq8_ground_truths)*100:.1f}%)\")\n",
    "print(f\"Test: {len(test_ids)} ({len(test_ids)/len(phq8_ground_truths)*100:.1f}%)\")\n",
    "print(f\"Total: {len(train_ids) + len(val_ids) + len(test_ids)}\")\n",
    "\n",
    "def check_balance(ids, df, split_name):\n",
    "    \"\"\"\n",
    "    Checks the stratification balance\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ids : list\n",
    "        list of participant ids\n",
    "    df : pandas dataframe\n",
    "        dataframe of all the participant ground truth info\n",
    "    split_name : string\n",
    "        name of the split\n",
    "\n",
    "    Prints\n",
    "    -------\n",
    "    string\n",
    "        Strings giving information about the balance of the data\n",
    "    \"\"\"\n",
    "    subset = df[df['Participant_ID'].isin(ids)]\n",
    "    print(f\"\\n{split_name} set balance:\")\n",
    "    print(f\"PHQ8_Binary: {subset['PHQ8_Binary'].value_counts().to_dict()}\")\n",
    "    print(f\"Gender: {subset['Gender'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Show PHQ8_Score distribution\n",
    "    score_dist = subset['PHQ8_Score'].value_counts().sort_index()\n",
    "    print(f\"PHQ8_Score distribution: {dict(score_dist)}\")\n",
    "    print(f\"Score stats: Mean={subset['PHQ8_Score'].mean():.1f}, Std={subset['PHQ8_Score'].std():.1f}\")\n",
    "    \n",
    "    # Show how many unique categories are in this split\n",
    "    unique_categories = subset['strat_var'].nunique()\n",
    "    print(f\"Unique categories (Gender_PHQ8Score): {unique_categories}\")\n",
    "\n",
    "check_balance(train_ids, phq8_ground_truths, \"Train\")\n",
    "check_balance(val_ids, phq8_ground_truths, \"Validation\")\n",
    "check_balance(test_ids, phq8_ground_truths, \"Test\")\n",
    "\n",
    "# Show detailed category distribution across splits\n",
    "print(\"\\nDetailed category distribution:\")\n",
    "for split_name, split_ids in [(\"Train\", train_ids), (\"Val\", val_ids), (\"Test\", test_ids)]:\n",
    "    subset = phq8_ground_truths[phq8_ground_truths['Participant_ID'].isin(split_ids)]\n",
    "    category_counts = subset['strat_var'].value_counts()\n",
    "    print(f\"\\n{split_name} categories with counts:\")\n",
    "    for cat in sorted(category_counts.index):\n",
    "        gender, score = cat.split('_')\n",
    "        print(f\"  Gender={gender}, PHQ8={score}: {category_counts[cat]} subjects\")\n",
    "\n",
    "# Output the three lists\n",
    "print(f\"\\nTrain IDs ({len(train_ids)}): {train_ids}\")\n",
    "print(f\"\\nValidation IDs ({len(val_ids)}): {val_ids}\")\n",
    "print(f\"\\nTest IDs ({len(test_ids)}): {test_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e19460",
   "metadata": {},
   "source": [
    "## Display figure for data distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 rows and 3 columns\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "gender_colors = {0: '#9b59b6', 1: \"#0091ff\"}  # Purple for female, orange for male\n",
    "\n",
    "# Create color gradient for PHQ-8 scores\n",
    "def get_phq8_color(score):\n",
    "    if score <= 4:\n",
    "        return '#2ecc71'  # Green\n",
    "    elif score <= 9:\n",
    "        return '#f1c40f'  # Yellow\n",
    "    elif score <= 14:\n",
    "        return '#ff8c00'  # Orange\n",
    "    elif score <= 19:\n",
    "        return '#e74c3c'  # Red\n",
    "    else:\n",
    "        return '#c0392b'  # Dark red\n",
    "\n",
    "phq8_colors = {i: get_phq8_color(i) for i in range(25)}\n",
    "\n",
    "# Dataframes for each split\n",
    "train_df = phq8_ground_truths[phq8_ground_truths['Participant_ID'].isin(train_ids)]\n",
    "val_df = phq8_ground_truths[phq8_ground_truths['Participant_ID'].isin(val_ids)]\n",
    "test_df = phq8_ground_truths[phq8_ground_truths['Participant_ID'].isin(test_ids)]\n",
    "\n",
    "# Split names and dataframes\n",
    "splits = ['Training (41%)', 'Validation (30%)', 'Test (29%)']\n",
    "split_dfs = [train_df, val_df, test_df]\n",
    "\n",
    "# Max values for consistent y-axis scale\n",
    "max_gender_count = 0\n",
    "max_phq8_count = 0\n",
    "\n",
    "# Get max gender and max PHQ8 scale\n",
    "for df in split_dfs:\n",
    "    gender_counts = df['Gender'].value_counts()\n",
    "    if len(gender_counts) > 0:\n",
    "        max_gender_count = max(max_gender_count, gender_counts.max())\n",
    "    \n",
    "    phq8_counts = df['PHQ8_Score'].value_counts()\n",
    "    if len(phq8_counts) > 0:\n",
    "        max_phq8_count = max(max_phq8_count, phq8_counts.max())\n",
    "\n",
    "# Set consistent max y scale with padding\n",
    "gender_ylim = max_gender_count * 1.15\n",
    "phq8_ylim = max_phq8_count * 1.15\n",
    "\n",
    "# Row 1: Gender Distribution\n",
    "for idx, (split_name, df) in enumerate(zip(splits, split_dfs)):\n",
    "    ax = axes[0, idx]\n",
    "    \n",
    "    # Count gender distribution\n",
    "    gender_counts = df['Gender'].value_counts().sort_index()\n",
    "    gender_counts = gender_counts.sort_index()\n",
    "    \n",
    "    # Create bar plot with different colors for each gender\n",
    "    bars = ax.bar(gender_counts.index, gender_counts.values, \n",
    "                   color=[gender_colors.get(g, '#cccccc') for g in gender_counts.index],\n",
    "                   edgecolor='black', linewidth=1.5, alpha=0.8, width=0.5)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, gender_counts.values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                f'{int(val)}', ha='center', va='bottom', fontsize=22, fontweight='bold')\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Gender', fontsize=20, fontweight='bold')\n",
    "    ax.set_ylabel('Subject Count', fontsize=20, fontweight='bold')\n",
    "    ax.set_title(f'{split_name}', fontsize=23, fontweight='bold')\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels(['Female', 'Male'])\n",
    "    ax.set_xlim(-0.5, 1.5)\n",
    "    ax.set_ylim(0, gender_ylim)  # Consistent y-axis for all gender plots\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    ax.tick_params(axis='x', labelsize=14)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Row 2: PHQ-8 Total Score Distribution\n",
    "for idx, (split_name, df) in enumerate(zip(splits, split_dfs)):\n",
    "    ax = axes[1, idx]\n",
    "    \n",
    "    # Get PHQ-8 score distribution\n",
    "    phq8_scores = df['PHQ8_Score'].value_counts().sort_index()\n",
    "\n",
    "    # Create bar plot with colors\n",
    "    bars = ax.bar(phq8_scores.index, phq8_scores.values,\n",
    "                    color=[phq8_colors.get(score, '#cccccc') for score in phq8_scores.index],\n",
    "                    edgecolor='black', linewidth=0.5, alpha=0.9)\n",
    "    \n",
    "    # Add PHQ8 severity range backgrounds with outlines\n",
    "    ax.axvspan(-0.5, 4.5, alpha=0.15, facecolor='green', linewidth=1.5, label='Minimal')\n",
    "    ax.axvspan(4.5, 9.5, alpha=0.15, facecolor='yellow', linewidth=1.5, label='Mild')\n",
    "    ax.axvspan(9.5, 14.5, alpha=0.15, facecolor='orange', linewidth=1.5, label='Moderate')\n",
    "    ax.axvspan(14.5, 19.5, alpha=0.15, facecolor='red', linewidth=1.5, label='Mod-Severe')\n",
    "    ax.axvspan(19.5, 24.5, alpha=0.15, facecolor='darkred', linewidth=1.5, label='Severe')\n",
    "    \n",
    "    # Add mean line\n",
    "    mean_score = df['PHQ8_Score'].mean()\n",
    "    ax.axvline(mean_score, color='black', linestyle='--', linewidth=2, alpha=0.7)\n",
    "    ax.text(mean_score, phq8_ylim * 0.9, f'Mean: {mean_score:.1f}', \n",
    "            ha='center', fontsize=15, color='black', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('PHQ-8 Total Score', fontsize=20, fontweight='bold')\n",
    "    ax.set_ylabel('Subject Count', fontsize=22, fontweight='bold')\n",
    "    ax.set_xlim(-0.5, 24.5)\n",
    "    ax.set_xticks(range(0, 25, 2))\n",
    "    ax.set_ylim(0, phq8_ylim)  # Consistent y-axis for all PHQ-8 plots\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    ax.tick_params(axis='x', labelsize=14)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Legend for PHQ-8 severity ranges with border\n",
    "legend_handles = [\n",
    "    Patch(facecolor='#2ecc71', label='Minimal'),\n",
    "    Patch(facecolor='#f1c40f', label='Mild'),\n",
    "    Patch(facecolor='#ff8c00', label='Moderate'),\n",
    "    Patch(facecolor='#e74c3c', label='Mod-Severe'),\n",
    "    Patch(facecolor='#c0392b', label='Severe')\n",
    "]\n",
    "\n",
    "axes[1, 2].legend(handles=legend_handles, loc='upper right', fontsize=10, title='Severity', \n",
    "                title_fontsize=11, framealpha=0.9, edgecolor='black', \n",
    "                fancybox=True, shadow=True)\n",
    "\n",
    "# Overall title\n",
    "fig.suptitle('Distribution of Training, Validation, and Test Sets', \n",
    "             fontsize=28, fontweight='bold', y=1.02)\n",
    "\n",
    "# Adjust layout spacing\n",
    "plt.tight_layout(pad=1.0, h_pad=1.5, w_pad=2.5)\n",
    "plt.savefig(r\"/data/users2/user/data_visual.pdf\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DISTRIBUTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for split_name, df, ids in zip(['Training', 'Validation', 'Test'], \n",
    "                                split_dfs, [train_ids, val_ids, test_ids]):\n",
    "    print(f\"\\n{split_name} Set (n={len(ids)}):\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Gender distribution\n",
    "    gender_dist = df['Gender'].value_counts().sort_index()\n",
    "    print(f\"Gender Distribution:\")\n",
    "    for gender_val in [0, 1]:\n",
    "        count = gender_dist.get(gender_val, 0)\n",
    "        gender_label = \"Female\" if gender_val == 0 else \"Male\"\n",
    "        print(f\"  {gender_label}: {count} ({count/len(df)*100:.1f}%)\" if len(df) > 0 else f\"  {gender_label}: 0 (0.0%)\")\n",
    "    \n",
    "    # PHQ-8 statistics\n",
    "    if len(df) > 0:\n",
    "        print(f\"\\nPHQ-8 Score Statistics:\")\n",
    "        print(f\"  Mean: {df['PHQ8_Score'].mean():.2f}\")\n",
    "        print(f\"  Std: {df['PHQ8_Score'].std():.2f}\")\n",
    "        print(f\"  Median: {df['PHQ8_Score'].median():.1f}\")\n",
    "        print(f\"  Range: {df['PHQ8_Score'].min()}-{df['PHQ8_Score'].max()}\")\n",
    "    else:\n",
    "        print(f\"\\nNo data in this split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0debc891",
   "metadata": {},
   "source": [
    "## Grab knowledgebase transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3069a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with participant transcripts\n",
    "participant_transcripts = {}\n",
    "\n",
    "for participant_id in train_ids:\n",
    "    try:\n",
    "        current_transcript = pd.read_csv(fr\"/data/users4/user/ai-psychiatrist/datasets/daic_woz_dataset/{participant_id}_P/{participant_id}_TRANSCRIPT.csv\", sep=\"\\t\")\n",
    "\n",
    "        # Handle missing values by converting to string and replacing NaN\n",
    "        current_transcript['speaker'] = current_transcript['speaker'].fillna('Unknown').astype(str)\n",
    "        current_transcript['value'] = current_transcript['value'].fillna('').astype(str)\n",
    "        \n",
    "        # Reformatting transcript data to be a string with speaker name + text\n",
    "        current_patient_transcript = '\\n'.join(current_transcript['speaker'] + ': ' + current_transcript['value'])\n",
    "        participant_transcripts[participant_id] = current_patient_transcript\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File for participant {participant_id} not found\")\n",
    "        participant_transcripts[participant_id] = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing participant {participant_id}: {e}\")\n",
    "        participant_transcripts[participant_id] = None\n",
    "\n",
    "print(len(participant_transcripts))\n",
    "print(participant_transcripts)\n",
    "print(f\"{'-'*30}\\n{len(train_ids)}\")\n",
    "print(train_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d43ae",
   "metadata": {},
   "source": [
    "## Embed knowledgebase transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_embeddings(pickle_file):\n",
    "    \"\"\"\n",
    "    Loads embedded reference transcripts from pickle file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pickle_file : string\n",
    "        path to the pickle file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dict of embedded chunks of the reference transcripts and their participant IDs\n",
    "    \"\"\"\n",
    "    if os.path.exists(pickle_file):\n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        except:\n",
    "            print(f\"Error loading {pickle_file}\")\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_embeddings(embeddings_dict, pickle_file):\n",
    "    \"\"\"\n",
    "    Saves an embedding to the pickle file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings_dict : dict\n",
    "        The dictionary with participant IDs as keys and (raw_text, embedding) as values for each transcript chunk\n",
    "\n",
    "    Writes\n",
    "    -------\n",
    "    dict\n",
    "        Adds the embeddings to the pickle file\n",
    "    \"\"\"\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(embeddings_dict, f)\n",
    "\n",
    "def get_embedding(text, model=\"dengcao/Qwen3-Embedding-8B:Q8_0\", dim=None):\n",
    "    \"\"\"\n",
    "    Creates embedding from given text input and model \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : string\n",
    "        The text to be embedded\n",
    "    model : string\n",
    "        The name of the ollama model to be used for embedding\n",
    "    dim : int, optional\n",
    "        If provided, truncate to this dimension and normalize (MRL support)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        The vector embedding of the text\n",
    "    \"\"\"\n",
    "    BASE_URL = f\"http://{OLLAMA_NODE}:11434/api/embeddings\"\n",
    "    \n",
    "    response = requests.post(\n",
    "        BASE_URL,\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"prompt\": text\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        embedding = response.json()[\"embedding\"]\n",
    "        \n",
    "        # Manually setting dimension because ollama doesn't natively support atm\n",
    "        if dim is not None:\n",
    "            # Truncate and normalize for MRL models\n",
    "            embedding = embedding[:dim]\n",
    "            norm = math.sqrt(sum(x * x for x in embedding))\n",
    "            if norm > 0:\n",
    "                embedding = [x / norm for x in embedding]\n",
    "        \n",
    "        return embedding\n",
    "    else:\n",
    "        raise Exception(f\"API call failed with status {response.status_code}: {response.text}\")\n",
    "\n",
    "def create_sliding_chunks(transcript_text, chunk_size=8, step_size=2):\n",
    "    \"\"\"\n",
    "    Splits the transcript into several chunks \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transcript_text : string\n",
    "        The transcript\n",
    "    chunk_size : int\n",
    "        The amount of newlines per chunk\n",
    "    step_size : int\n",
    "        The newline distance moved each time a chunk is created\n",
    "            -Ex. transcript_text = \"A\\nB\\nC\\nD\\nE\\nF\\nG\\nH\", chunk_size = 4, step_size = 2\n",
    "            -Chunk 1: \"A\\nB\\nC\\nD\"\n",
    "            -Chunk 2: \"C\\nD\\nE\\nF\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        The text chunk strings\n",
    "    \"\"\"\n",
    "    lines = transcript_text.split('\\n')\n",
    "    \n",
    "    # Remove any empty lines at the end\n",
    "    while lines and lines[-1] == '':\n",
    "        lines.pop()\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    # If fewer lines than chunk_size, just return the whole thing\n",
    "    if len(lines) <= chunk_size:\n",
    "        return ['\\n'.join(lines)]\n",
    "    \n",
    "    # Create sliding windows\n",
    "    for i in range(0, len(lines) - chunk_size + 1, step_size):\n",
    "        chunk = '\\n'.join(lines[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # If the last chunk doesn't include the final lines, add one more chunk\n",
    "    last_chunk_start = len(lines) - chunk_size\n",
    "    if last_chunk_start > 0 and (last_chunk_start % step_size) != 0:\n",
    "        final_chunk = '\\n'.join(lines[last_chunk_start:])\n",
    "        if final_chunk not in chunks:\n",
    "            chunks.append(final_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_transcripts(participant_transcripts, pickle_file, dim):\n",
    "    \"\"\"\n",
    "    Chunking and embedding the reference transcripts and saving them to the pickle file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    participant_transcripts : dict\n",
    "        A dict of participant transcripts with the key being participant id and value being the transcript\n",
    "    pickle_file : str\n",
    "        The path to the pickle file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The dictionary with participant IDs as keys and (raw_text, embedding) as values for each transcript chunk\n",
    "    \"\"\"\n",
    "    # Load existing embeddings\n",
    "    participant_embedded_transcripts = load_existing_embeddings(pickle_file)\n",
    "    \n",
    "    for participant_id, transcript in participant_transcripts.items():\n",
    "        # Skip if already processed\n",
    "        if participant_id in participant_embedded_transcripts:\n",
    "            print(f\"Skipping participant {participant_id} - already processed\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing participant {participant_id}...\")\n",
    "        \n",
    "        try:\n",
    "            # Create sliding window chunks\n",
    "            chunks = create_sliding_chunks(transcript)\n",
    "            \n",
    "            # Get embeddings for each chunk\n",
    "            embeddings_list = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                print(f\"  Processing chunk {i+1}/{len(chunks)}\")\n",
    "                embedding = get_embedding(chunk,dim=dim)\n",
    "                embeddings_list.append((chunk, embedding))\n",
    "            \n",
    "            # Convert to numpy array and store\n",
    "            participant_embedded_transcripts[participant_id] = np.array(embeddings_list, dtype=object)\n",
    "            \n",
    "            # Save after each participant\n",
    "            save_embeddings(participant_embedded_transcripts, pickle_file)\n",
    "            print(f\"Completed participant {participant_id} - saved to {pickle_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing participant {participant_id}: {e}\")\n",
    "            print(\"Stopping processing and saving current progress...\")\n",
    "            save_embeddings(participant_embedded_transcripts, pickle_file)\n",
    "            break\n",
    "    \n",
    "    return participant_embedded_transcripts\n",
    "\n",
    "pickle_file = \"/data/users2/user/ai-psychiatrist/chunk_8_step_2_dim_64_participant_embedded_transcripts.pkl\"\n",
    "participant_embedded_transcripts = process_transcripts(participant_transcripts, pickle_file, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85875b",
   "metadata": {},
   "source": [
    "## Grabbing evidence from transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a2b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_extraction_prompt = f\"\"\"Analyze the following therapy transcript and extract specific text chunks that provide evidence for each PHQ-8 domain. Based on the evidence found, determine the appropriate PHQ-8 score for each domain.\n",
    "\n",
    "PHQ-8 scoring reference:\n",
    "- 0 = not at all (0-1 days)\n",
    "- 1 = several days (2-6 days) \n",
    "- 2 = more than half the days (7-11 days)\n",
    "- 3 = nearly every day (12-14 days)\n",
    "\n",
    "For each domain below, extract direct quotes from the transcript that indicate the presence, absence, frequency, or severity of symptoms. Then determine the most appropriate score based on the evidence.\n",
    "\n",
    "PHQ-8 domains:\n",
    "- nointerest: little interest or pleasure in activities\n",
    "- depressed: feeling down, depressed, or hopeless\n",
    "- sleep: sleep problems (trouble falling/staying asleep or sleeping too much)\n",
    "- tired: feeling tired or having little energy\n",
    "- appetite: appetite changes (poor appetite or overeating)\n",
    "- failure: negative self-perception or feeling like a failure\n",
    "- concentrating: trouble concentrating on tasks\n",
    "- moving: psychomotor changes (moving/speaking slowly or restlessness)\n",
    "\n",
    "Return a JSON object with arrays of relevant transcript quotes for each domain. Each quote should be a direct excerpt from the transcript that supports your scoring decision. Include as many relevant quotes as you find - this could range from 0 quotes (if no evidence) to multiple quotes per domain.\n",
    "\n",
    "Therapy transcript:\n",
    "{current_patient_transcript}\n",
    "\n",
    "Respond with valid JSON matching this structure:\n",
    "{{\n",
    "    \"PHQ8_NoInterest\": [\"evidence_1\", \"evidence_2\", \"evidence_3\", \"evidence_4\"],\n",
    "    \"PHQ8_Depressed\": [\"evidence_1\"],\n",
    "    \"PHQ8_Sleep\": [\"evidence_1\", \"evidence_2\", \"evidence_3\"],\n",
    "    \"PHQ8_Tired\": [\"evidence_1\", \"evidence_2\"],\n",
    "    \"PHQ8_Appetite\": [],\n",
    "    \"PHQ8_Failure\": [\"evidence_1\", \"evidence_2\", \"evidence_3\", \"evidence_4\", \"evidence_5\"],\n",
    "    \"PHQ8_Concentrating\": [\"evidence_1\"],\n",
    "    \"PHQ8_Moving\": [\"evidence_1\", \"evidence_2\"]\n",
    "}}\n",
    "\n",
    "Important: Extract UNIQUE quotes only - do not repeat the same quote multiple times. Each quote should be different and provide distinct, related evidence. If no evidence exists for a domain, return an empty array for that domain. Also, do not format the evidence grabbed in any way, output it EXACTLY as it is in the transcript.\n",
    "\"\"\"\n",
    "\n",
    "response = requests.post(\n",
    "    BASE_URL,\n",
    "    json={\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": evidence_extraction_prompt}],\n",
    "        \"options\": {\n",
    "            # Fairly deterministic parameters\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_k\": 20,\n",
    "            \"top_p\": 0.8\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Get the full response directly\n",
    "response_json = response.json()\n",
    "full_response = response_json['message']['content']\n",
    "\n",
    "# Use the full_response for processing\n",
    "content = full_response.strip('```json\\n').strip('\\n```')\n",
    "\n",
    "try:\n",
    "    evidence_dict = json.loads(content)\n",
    "    \n",
    "    # Add scores to the output\n",
    "    scores = phq8_ground_truths.iloc[13]\n",
    "    evidence_dict['scores'] = {\n",
    "        'PHQ8_NoInterest': int(scores['PHQ8_NoInterest']),\n",
    "        'PHQ8_Depressed': int(scores['PHQ8_Depressed']),\n",
    "        'PHQ8_Sleep': int(scores['PHQ8_Sleep']),\n",
    "        'PHQ8_Tired': int(scores['PHQ8_Tired']),\n",
    "        'PHQ8_Appetite': int(scores['PHQ8_Appetite']),\n",
    "        'PHQ8_Failure': int(scores['PHQ8_Failure']),\n",
    "        'PHQ8_Concentrating': int(scores['PHQ8_Concentrating']),\n",
    "        'PHQ8_Moving': int(scores['PHQ8_Moving'])\n",
    "    }\n",
    "    \n",
    "    # Remove duplicate quotes in each evidence list\n",
    "    for key in evidence_dict:\n",
    "        if isinstance(evidence_dict[key], list):\n",
    "            evidence_dict[key] = list(dict.fromkeys(evidence_dict[key]))\n",
    "    \n",
    "    formatted_output = json.dumps(evidence_dict, indent=4)\n",
    "    print(formatted_output)\n",
    "    \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error parsing JSON: {e}\")\n",
    "    print(f\"Raw content: {content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e0195",
   "metadata": {},
   "source": [
    "## Embedding the evidence given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f51ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_chunks(evidence_text_embedding, participant_embedded_transcripts, top_k=3):\n",
    "    \"\"\"\n",
    "    Runs cosine similarity between the evidence and all of the reference transcript embedded chunks.\n",
    "    Then, grabs the top_k most similar ones.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    evidence_text_embedding : list\n",
    "        The embedding of the pulled evidence from the current transcript for the given PHQ8 question\n",
    "    participant_embedded_transcripts : dict\n",
    "        The dictionary with participant IDs as keys and (raw_text, embedding) as values for each transcript chunk\n",
    "    top_k : int\n",
    "        The number of most similar chunks that should be pulled (ex. top_k=3 means pull the 3 most similar chunks)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of dictionaries containing the most similar chunks, each with keys:\n",
    "        'participant_id', 'raw_text', 'similarity', and 'embedding'.\n",
    "        Sorted by similarity score in descending order.\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    \n",
    "    # Go through all participants and their embeddings\n",
    "    for participant_id, embeddings_array in participant_embedded_transcripts.items():\n",
    "        for i, (raw_text, embedding) in enumerate(embeddings_array):\n",
    "            # Calculate cosine similarity\n",
    "            similarity = cosine_similarity(\n",
    "                [evidence_text_embedding], \n",
    "                [embedding]\n",
    "            )[0][0]\n",
    "            \n",
    "            similarities.append({\n",
    "                'participant_id': participant_id,\n",
    "                'raw_text': raw_text,\n",
    "                'similarity': similarity,\n",
    "                'embedding': embedding\n",
    "            })\n",
    "    \n",
    "    # Sort by similarity and get top_k amount\n",
    "    similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "def process_evidence_for_references(evidence_dict, participant_embedded_transcripts, phq8_ground_truths, top_k):\n",
    "    \"\"\"\n",
    "    Grabs chunks from other transcripts that are similar to the evidence pulled for the current transcript.\n",
    "    Then, grabs those chunks ground truth scores and formats all that into a string for use in the prompt.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    evidence_dict : dict\n",
    "        Dictionary containing PHQ-8 domain keys mapped to lists of evidence quotes\n",
    "    \n",
    "    participant_embedded_transcripts : dict\n",
    "        The dictionary with participant IDs as keys and (raw_text, embedding) as values for each transcript chunk\n",
    "    \n",
    "    phq8_ground_truths : pandas dataframe\n",
    "        Dataframe containing ground truth PHQ-8 scores for participants\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string with all the reference transcripts chunks and their corresponding PHQ8 scores\n",
    "    \"\"\"\n",
    "    evidence_keys = [\n",
    "        'PHQ8_NoInterest', 'PHQ8_Depressed', 'PHQ8_Sleep', 'PHQ8_Tired',\n",
    "        'PHQ8_Appetite', 'PHQ8_Failure', 'PHQ8_Concentrating', 'PHQ8_Moving'\n",
    "    ]\n",
    "    \n",
    "    all_references = []\n",
    "    \n",
    "    for evidence_key in evidence_keys:\n",
    "        # Get evidence texts for this key\n",
    "        evidence_texts = evidence_dict.get(evidence_key, [])\n",
    "        \n",
    "        # Skip if empty\n",
    "        if not evidence_texts:\n",
    "            continue\n",
    "        \n",
    "        # Combine evidence texts into single string\n",
    "        combined_text = '\\n'.join(evidence_texts)\n",
    "        \n",
    "        # Skip if less than 15 characters\n",
    "        if len(combined_text) < 15:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing {evidence_key}...\")\n",
    "        \n",
    "        try:\n",
    "            # Get embedding for combined evidence text\n",
    "            evidence_embedding = get_embedding(combined_text)\n",
    "            \n",
    "            # Find top_k similar chunks\n",
    "            similar_chunks = find_similar_chunks(\n",
    "                evidence_embedding, \n",
    "                participant_embedded_transcripts, \n",
    "                top_k=top_k\n",
    "            )\n",
    "            \n",
    "            # Add each reference with its own header\n",
    "            for chunk_info in similar_chunks:\n",
    "                participant_id = chunk_info['participant_id']\n",
    "                raw_text = chunk_info['raw_text']\n",
    "                \n",
    "                # Get the ground truth score for this participant and evidence type\n",
    "                participant_data = phq8_ground_truths.loc[\n",
    "                    phq8_ground_truths['Participant_ID'] == participant_id\n",
    "                ]\n",
    "                \n",
    "                if not participant_data.empty:\n",
    "                    score = int(participant_data[evidence_key].values[0])\n",
    "                    reference_entry = f\"({evidence_key} Score: {score})\\n{raw_text}\"\n",
    "                    all_references.append(reference_entry)\n",
    "                else:\n",
    "                    print(f\"Warning: No ground truth data found for participant {participant_id}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {evidence_key}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all references into a string\n",
    "    if all_references:\n",
    "        reference_evidence = \"<Reference Examples>\\n\\n\" + \"\\n\\n\".join(all_references) + \"\\n\\n<Reference Examples>\"\n",
    "    else:\n",
    "        reference_evidence = \"<Reference Examples>\\nNo valid evidence found\\n<Reference Examples>\"\n",
    "    \n",
    "    return reference_evidence\n",
    "\n",
    "reference_evidence = process_evidence_for_references(\n",
    "    evidence_dict, \n",
    "    participant_embedded_transcripts, \n",
    "    phq8_ground_truths,\n",
    "    top_k=3,\n",
    ")\n",
    "print()\n",
    "print(reference_evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21bb5b",
   "metadata": {},
   "source": [
    "## Run Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PHQ8ScoreWithExplanation(BaseModel):\n",
    "    evidence: str  # Direct quotes or references from interview\n",
    "    reason: str  # Brief explanation for the score\n",
    "    score: Union[int, str]  # 0-3 or \"N/A\"\n",
    "\n",
    "class PHQ8ScoresWithExplanations(BaseModel):\n",
    "    PHQ8_NoInterest: PHQ8ScoreWithExplanation  # Little interest or pleasure in doing things\n",
    "    PHQ8_Depressed: PHQ8ScoreWithExplanation   # Feeling down, depressed, or hopeless\n",
    "    PHQ8_Sleep: PHQ8ScoreWithExplanation       # Trouble falling/staying asleep, or sleeping too much\n",
    "    PHQ8_Tired: PHQ8ScoreWithExplanation       # Feeling tired or having little energy\n",
    "    PHQ8_Appetite: PHQ8ScoreWithExplanation    # Poor appetite or overeating\n",
    "    PHQ8_Failure: PHQ8ScoreWithExplanation     # Feeling bad about self or like a failure\n",
    "    PHQ8_Concentrating: PHQ8ScoreWithExplanation # Trouble concentrating\n",
    "    PHQ8_Moving: PHQ8ScoreWithExplanation      # Moving/speaking slowly or being fidgety/restless\n",
    "\n",
    "system_prompt = \"\"\"You are a AI psychiatrist assessment tool specialized in analyzing interview transcripts to evaluate PHQ-8 (Patient Health Questionnaire-8) scores. The PHQ-8 is a validated depression screening instrument that assesses eight of the nine DSM-IV criteria for depressive disorders over the past 2 weeks.\n",
    "\n",
    "PHQ-8 Scoring System:\n",
    "- 0 = Not at all (0-1 days in the past 2 weeks)\n",
    "- 1 = Several days (2-6 days)\n",
    "- 2 = More than half the days (7-11 days)\n",
    "- 3 = Nearly every day (12-14 days)\n",
    "\n",
    "Key Principles:\n",
    "1. Base all assessments strictly on evidence from the transcript\n",
    "2. When no relevant evidence exists for a symptom, output \"N/A\" rather than assuming absence (score of 0)\n",
    "3. Consider both explicit statements and behavioral indicators in the interview\n",
    "4. Be conservative - only assign scores when there is clear evidence of frequency\n",
    "5. Remember that absence of mention does not necessarily mean absence of symptom\n",
    "6. When reference examples are provided, use them for cross-analysis to inform scoring decisions\n",
    "7. Weight reference examples based on their similarity to the current evidence - highly similar references should be considered more heavily\n",
    "8. Follow the format specified exactly\"\"\"\n",
    "\n",
    "def run_phq8_analysis(patient_transcript, ground_truth, reference_evidence=\"\"):\n",
    "    \"\"\"\n",
    "    Predicts PHQ8 values from a patients transcript, compares to ground truth, and calculates evaluation metrics\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    patient_transcript : string\n",
    "        The patients transcript\n",
    "    ground_truth : pandas series\n",
    "        The ground truth PHQ8 values for a given participant\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A 5-tuple containing:\n",
    "        - phq8_scores (PHQ8ScoresWithExplanations or None): Parsed PHQ8 scores with evidence and reasoning\n",
    "        - avg_difference (float or None): Average absolute difference between predicted and ground truth scores\n",
    "        - accuracy_on_available (float or None): Accuracy score on questions that weren't marked N/A (0-1 scale)\n",
    "        - num_questions_NA (int or None): Number of questions marked as N/A out of 8 total\n",
    "        - overall_accuracy (float or None): Overall accuracy adjusted for N/A responses (0-1 scale)\n",
    "        \n",
    "        Returns (None, None, None, None, None) if parsing fails.\n",
    "    \"\"\"\n",
    "    quantitative_prompt = f\"\"\"Analyze the following interview transcript and predict PHQ-8 scores for each symptom domain.\n",
    "\n",
    "Interview transcript:\n",
    "<transcript>\n",
    "{patient_transcript}\n",
    "</transcript>\n",
    "\n",
    "{reference_evidence}\n",
    "\n",
    "Analyze each symptom using the following approach in <thinking> tags:\n",
    "1. Search for direct quotes or behavioral evidence related to each PHQ-8 symptom\n",
    "2. When reference examples are provided, compare the current evidence with similar reference cases\n",
    "3. Evaluate the frequency/severity based on available evidence and reference comparisons\n",
    "4. Consider how similar the reference examples are to the current evidence - if highly similar, give more weight to the reference scores; if less similar, rely more on direct analysis\n",
    "5. If no relevant evidence exists, mark as \"N/A\" rather than assuming absence\n",
    "6. Only assign numeric scores (0-3) when evidence clearly indicates frequency\n",
    "\n",
    "After your analysis, provide your final assessment in <answer> tags as a JSON object.\n",
    "\n",
    "For each symptom, provide:\n",
    "1. \"evidence\": exact quotes from transcript (use \"No relevant evidence found\" if not discussed)\n",
    "2. \"reason\": explanation of scoring decision, including cross-reference analysis when applicable and why N/A if applicable\n",
    "3. \"score\": integer 0-3 based on evidence, or \"N/A\" if no relevant evidence\n",
    "\n",
    "Return ONLY a JSON object in <answer> tags with these exact keys:\n",
    "- \"PHQ8_NoInterest\": {{evidence, reason, score}} for little interest or pleasure in doing things (anhedonia)\n",
    "- \"PHQ8_Depressed\": {{evidence, reason, score}} for feeling down, depressed, or hopeless (depressed mood)\n",
    "- \"PHQ8_Sleep\": {{evidence, reason, score}} for trouble falling or staying asleep, or sleeping too much (sleep problems)\n",
    "- \"PHQ8_Tired\": {{evidence, reason, score}} for feeling tired or having little energy (fatigue)\n",
    "- \"PHQ8_Appetite\": {{evidence, reason, score}} for poor appetite or overeating (appetite/weight changes)\n",
    "- \"PHQ8_Failure\": {{evidence, reason, score}} for feeling bad about yourself or that you are a failure (negative self-perception)\n",
    "- \"PHQ8_Concentrating\": {{evidence, reason, score}} for trouble concentrating on things like reading or watching TV (concentration problems)\n",
    "- \"PHQ8_Moving\": {{evidence, reason, score}} for moving or speaking slowly, or being fidgety/restless (psychomotor changes)\"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        BASE_URL,\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": quantitative_prompt}],\n",
    "            \"options\": {\n",
    "                # Fairly deterministic parameters\n",
    "                \"temperature\": 0.2,\n",
    "                \"top_k\": 20,\n",
    "                \"top_p\": 0.8\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Get the full response directly\n",
    "    response_data = response.json()\n",
    "    full_response = response_data['message']['content']\n",
    "\n",
    "    # Parse and validate the response\n",
    "    try:\n",
    "        content = full_response\n",
    "        \n",
    "        # Extract content from <answer> tags if present\n",
    "        if '<answer>' in content and '</answer>' in content:\n",
    "            content = content.split('<answer>')[1].split('</answer>')[0].strip()\n",
    "        \n",
    "        # Remove markdown code blocks if present\n",
    "        if content.startswith('```json'):\n",
    "            content = content.split('```json')[1].split('```')[0].strip()\n",
    "        elif content.startswith('```'):\n",
    "            content = content.split('```')[1].split('```')[0].strip()\n",
    "        \n",
    "        # Parse the JSON response and validate with Pydantic\n",
    "        scores_dict = json.loads(content)\n",
    "        phq8_scores = PHQ8ScoresWithExplanations(**scores_dict)\n",
    "        \n",
    "        # Extract the 8 PHQ-8 score values\n",
    "        scores_list = [\n",
    "            phq8_scores.PHQ8_NoInterest.score,\n",
    "            phq8_scores.PHQ8_Depressed.score,\n",
    "            phq8_scores.PHQ8_Sleep.score,\n",
    "            phq8_scores.PHQ8_Tired.score,\n",
    "            phq8_scores.PHQ8_Appetite.score,\n",
    "            phq8_scores.PHQ8_Failure.score,\n",
    "            phq8_scores.PHQ8_Concentrating.score,\n",
    "            phq8_scores.PHQ8_Moving.score\n",
    "        ]\n",
    "        \n",
    "        print(\"Comparison of Predicted vs Ground Truth:\")\n",
    "        print(\"Metric\\t\\t\\tPredicted\\tGround Truth\\tDifference\")\n",
    "        print(\"-\" * 65)\n",
    "\n",
    "        differences = []\n",
    "        n_available = 0\n",
    "        num_questions_NA = 0\n",
    "        metrics = ['PHQ8_NoInterest', 'PHQ8_Depressed', 'PHQ8_Sleep', 'PHQ8_Tired', \n",
    "                'PHQ8_Appetite', 'PHQ8_Failure', 'PHQ8_Concentrating', 'PHQ8_Moving']\n",
    "        predicted_values = [phq8_scores.PHQ8_NoInterest.score, phq8_scores.PHQ8_Depressed.score, phq8_scores.PHQ8_Sleep.score, \n",
    "                        phq8_scores.PHQ8_Tired.score, phq8_scores.PHQ8_Appetite.score, phq8_scores.PHQ8_Failure.score,\n",
    "                        phq8_scores.PHQ8_Concentrating.score, phq8_scores.PHQ8_Moving.score]\n",
    "\n",
    "        for metric, pred_val in zip(metrics, predicted_values):\n",
    "            print(f\"DEBUG: Processing {metric}\")\n",
    "            print(f\"DEBUG: pred_val = {pred_val}, type = {type(pred_val)}\")\n",
    "            print(f\"DEBUG: ground_truth[metric] = {ground_truth[metric]}, type = {type(ground_truth[metric])}\")\n",
    "            \n",
    "            gt_val = int(ground_truth[metric].iloc[0] if hasattr(ground_truth[metric], 'iloc') else ground_truth[metric])\n",
    "            print(f\"DEBUG: gt_val = {gt_val}, type = {type(gt_val)}\")\n",
    "            \n",
    "            if pred_val == \"N/A\":\n",
    "                diff_str = \"N/A\"\n",
    "                num_questions_NA += 1\n",
    "                print(f\"DEBUG: pred_val is N/A\")\n",
    "            else:\n",
    "                # Convert pred_val to int if it's a string representation of a number\n",
    "                try:\n",
    "                    pred_val_int = int(pred_val)\n",
    "                    print(f\"DEBUG: converted pred_val to int: {pred_val_int}\")\n",
    "                except (ValueError, TypeError):\n",
    "                    print(f\"DEBUG: Could not convert pred_val '{pred_val}' to int, treating as N/A\")\n",
    "                    diff_str = \"N/A\"\n",
    "                    num_questions_NA += 1\n",
    "                    continue\n",
    "                    \n",
    "                diff = abs(pred_val_int - gt_val)\n",
    "                differences.append(diff)\n",
    "                diff_str = str(diff)\n",
    "                n_available += 1\n",
    "                print(f\"DEBUG: diff = {diff}\")\n",
    "            \n",
    "            print(f\"{metric:<23} {str(pred_val):<12} {gt_val:<15} {diff_str}\")\n",
    "            print(\"DEBUG: ---\")\n",
    "\n",
    "        # Calculate metrics\n",
    "        if n_available > 0:\n",
    "            avg_difference = sum(differences) / n_available\n",
    "            accuracy_on_available = 1 - (avg_difference / 3)\n",
    "        else:\n",
    "            avg_difference = float('inf')\n",
    "            accuracy_on_available = 0\n",
    "        \n",
    "        # Accuracy * % available questions\n",
    "        overall_accuracy = accuracy_on_available * (1 - (num_questions_NA / 8))\n",
    "        \n",
    "        print(\"-\" * 65)\n",
    "        if n_available > 0:\n",
    "            print(f\"Average Absolute Difference (on available): {avg_difference:.2f}\")\n",
    "            print(f\"Accuracy on available questions: {accuracy_on_available:.2%}\")\n",
    "        print(f\"Questions marked N/A: {num_questions_NA}/8\")\n",
    "        print(f\"Overall accuracy: {overall_accuracy:.2%}\")\n",
    "        \n",
    "        # Reasoning and evidence section\n",
    "        print(\"\\n\\nDetailed Reasoning for Each Score:\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        for key in metrics:\n",
    "            score_data = getattr(phq8_scores, key)\n",
    "            print(f\"\\n{key} (Score: {score_data.score})\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"Evidence: {score_data.evidence}\")\n",
    "            print(f\"Reason: {score_data.reason}\")\n",
    "\n",
    "        return phq8_scores, avg_difference, accuracy_on_available, num_questions_NA, overall_accuracy\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "        print(\"Raw response:\", response)\n",
    "        print(json.dumps(response.json(), indent=2))\n",
    "        return None, None, None, None, None\n",
    "\n",
    "run_phq8_analysis(current_patient_transcript, phq8_ground_truths, reference_evidence)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
