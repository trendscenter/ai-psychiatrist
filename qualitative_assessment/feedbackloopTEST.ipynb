{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85bbab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "\n",
    "def parse_score_and_explanation(response_text):\n",
    "    \"\"\"Extract score and explanation from model response\"\"\"\n",
    "    score_patterns = [\n",
    "        r'score[:\\s]*(\\d+)',\n",
    "        r'(\\d+)[/\\s]*(?:out of\\s*)?5',\n",
    "        r'(\\d+)[/\\s]*5',\n",
    "        r'rating[:\\s]*(\\d+)',\n",
    "        r'^(\\d+)',  # Number at start of line\n",
    "    ]\n",
    "    \n",
    "    score = None\n",
    "    for pattern in score_patterns:\n",
    "        match = re.search(pattern, response_text, re.IGNORECASE | re.MULTILINE)\n",
    "        if match:\n",
    "            potential_score = int(match.group(1))\n",
    "            if 1 <= potential_score <= 5:\n",
    "                score = potential_score\n",
    "                break\n",
    "    \n",
    "    return score, response_text.strip()\n",
    "\n",
    "def identify_low_scoring_participants(results_csv_path, threshold=3):\n",
    "    \"\"\"Identify participants who scored <= threshold in any category\"\"\"\n",
    "    df = pd.read_csv(results_csv_path)\n",
    "    \n",
    "    # Identify participants with low scores\n",
    "    low_scoring_mask = (\n",
    "        (df['coherence'] <= threshold) | \n",
    "        (df['completeness'] <= threshold) | \n",
    "        (df['specificity'] <= threshold) | \n",
    "        (df['accuracy'] <= threshold)\n",
    "    )\n",
    "    \n",
    "    low_scoring_participants = df[low_scoring_mask].copy()\n",
    "    \n",
    "    # Add columns to track which metrics were low\n",
    "    low_scoring_participants['low_coherence'] = df['coherence'] <= threshold\n",
    "    low_scoring_participants['low_completeness'] = df['completeness'] <= threshold\n",
    "    low_scoring_participants['low_specificity'] = df['specificity'] <= threshold\n",
    "    low_scoring_participants['low_accuracy'] = df['accuracy'] <= threshold\n",
    "    \n",
    "    print(f\"Found {len(low_scoring_participants)} participants with scores <= {threshold}\")\n",
    "    \n",
    "    return low_scoring_participants\n",
    "\n",
    "def generate_improved_prompt(metric, original_explanation, original_score):\n",
    "    \"\"\"Generate an improved prompt based on the feedback from the original assessment\"\"\"\n",
    "    \n",
    "    # Extract common issues from the explanation\n",
    "    feedback_analysis = f\"\"\"\n",
    "    Based on the previous assessment that scored {original_score}/5, here are the identified issues:\n",
    "    {original_explanation}\n",
    "    \n",
    "    Please pay special attention to avoiding these specific mistakes.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_prompts = {\n",
    "        'coherence': f\"\"\"Evaluate the following qualitative assessment output for COHERENCE only. Compare qualitative_assessment to the provided transcript.\n",
    "\n",
    "{feedback_analysis}\n",
    "\n",
    "Score from 1-5 based on these criteria:\n",
    "- Score of 5: 0 mistakes (completely logically consistent)\n",
    "- Score of 4: 1-2 mistakes (minor inconsistencies)\n",
    "- Score of 3: 3-4 mistakes (moderate inconsistencies)\n",
    "- Score of 2: 5-6 mistakes (significant inconsistencies)\n",
    "- Score of 1: 7 or more mistakes (major logical contradictions)\n",
    "\n",
    "Coherence focuses on logical consistency. Look for:\n",
    "- Internal contradictions within the assessment\n",
    "- Statements that conflict with each other\n",
    "- Logical flow and reasoning consistency\n",
    "- Whether conclusions follow from the evidence presented\n",
    "\n",
    "Pay extra attention to ensuring all statements are internally consistent and support each other logically.\n",
    "\n",
    "Format your response as:\n",
    "Score: [your score]\n",
    "Explanation: [detailed explanation focusing on logical consistency]\"\"\",\n",
    "\n",
    "        'completeness': f\"\"\"Evaluate the following qualitative assessment output for COMPLETENESS only. Compare qualitative_assessment to the provided transcript.\n",
    "\n",
    "{feedback_analysis}\n",
    "\n",
    "Score from 1-5 based on these criteria:\n",
    "- Score of 5: 0 mistakes (covers all relevant information)\n",
    "- Score of 4: 1-2 mistakes (minor omissions)\n",
    "- Score of 3: 3-4 mistakes (moderate gaps)\n",
    "- Score of 2: 5-6 mistakes (significant omissions)\n",
    "- Score of 1: 7 or more mistakes (major gaps in coverage)\n",
    "\n",
    "Completeness focuses on thoroughness. Look for:\n",
    "- Coverage of all relevant symptoms mentioned in transcript\n",
    "- Inclusion of severity indicators where discussed\n",
    "- Duration and frequency details when available\n",
    "- Social, biological, and risk factors as appropriate\n",
    "- Supporting quotes and evidence\n",
    "\n",
    "Ensure the assessment doesn't miss important information from the transcript.\n",
    "\n",
    "Format your response as:\n",
    "Score: [your score]\n",
    "Explanation: [detailed explanation focusing on completeness and thoroughness]\"\"\",\n",
    "\n",
    "        'specificity': f\"\"\"Evaluate the following qualitative assessment output for SPECIFICITY only. Compare qualitative_assessment to the provided transcript.\n",
    "\n",
    "{feedback_analysis}\n",
    "\n",
    "Score from 1-5 based on these criteria:\n",
    "- Score of 5: 0 mistakes (highly specific, no vague statements)\n",
    "- Score of 4: 1-2 mistakes (mostly specific with minor vagueness)\n",
    "- Score of 3: 3-4 mistakes (moderate use of vague language)\n",
    "- Score of 2: 5-6 mistakes (frequent vague statements)\n",
    "- Score of 1: 7 or more mistakes (predominantly vague and generic)\n",
    "\n",
    "Specificity focuses on precision and detail. Look for:\n",
    "- Use of specific examples and direct quotes\n",
    "- Avoidance of vague terms like \"seems depressed\" or \"appears sad\"\n",
    "- Concrete behavioral observations\n",
    "- Specific symptoms rather than general mood descriptions\n",
    "- Detailed supporting evidence\n",
    "\n",
    "Ensure the assessment uses precise, specific language backed by concrete evidence.\n",
    "\n",
    "Format your response as:\n",
    "Score: [your score]\n",
    "Explanation: [detailed explanation focusing on specificity and precision]\"\"\",\n",
    "\n",
    "        'accuracy': f\"\"\"Evaluate the following qualitative assessment output for ACCURACY only. Compare qualitative_assessment to the provided transcript.\n",
    "\n",
    "{feedback_analysis}\n",
    "\n",
    "Score from 1-5 based on these criteria:\n",
    "- Score of 5: 0 mistakes (completely accurate to DSM-5/PHQ-8)\n",
    "- Score of 4: 1-2 mistakes (minor inaccuracies)\n",
    "- Score of 3: 3-4 mistakes (moderate errors)\n",
    "- Score of 2: 5-6 mistakes (significant inaccuracies)\n",
    "- Score of 1: 7 or more mistakes (major errors in clinical accuracy)\n",
    "\n",
    "Accuracy focuses on clinical correctness. Look for:\n",
    "- Correct identification of symptoms per DSM-5 criteria\n",
    "- Accurate duration and frequency assessments\n",
    "- Proper use of clinical terminology\n",
    "- Alignment with established diagnostic criteria\n",
    "- Factual accuracy regarding what was actually said in transcript\n",
    "\n",
    "Ensure all clinical observations and interpretations are accurate and evidence-based.\n",
    "\n",
    "Format your response as:\n",
    "Score: [your score]\n",
    "Explanation: [detailed explanation focusing on clinical accuracy]\"\"\"\n",
    "    }\n",
    "    \n",
    "    return base_prompts.get(metric, \"\")\n",
    "\n",
    "def reassess_participant(participant_data, transcript, ollama_config):\n",
    "    \"\"\"Reassess a single participant with improved prompts for low-scoring metrics\"\"\"\n",
    "    \n",
    "    participant_id = participant_data['participant_id']\n",
    "    qualitative_assessment = participant_data['qualitative_assessment']\n",
    "    \n",
    "    print(f\"Reassessing participant {participant_id}\")\n",
    "    \n",
    "    results = {'participant_id': participant_id}\n",
    "    \n",
    "    # Define which metrics need reassessment\n",
    "    metrics_to_reassess = []\n",
    "    if participant_data['low_coherence']:\n",
    "        metrics_to_reassess.append('coherence')\n",
    "    if participant_data['low_completeness']:\n",
    "        metrics_to_reassess.append('completeness')\n",
    "    if participant_data['low_specificity']:\n",
    "        metrics_to_reassess.append('specificity')\n",
    "    if participant_data['low_accuracy']:\n",
    "        metrics_to_reassess.append('accuracy')\n",
    "    \n",
    "    print(f\"  Reassessing metrics: {metrics_to_reassess}\")\n",
    "    \n",
    "    # Reassess each low-scoring metric\n",
    "    for metric in metrics_to_reassess:\n",
    "        print(f\"  Reassessing {metric}...\")\n",
    "        \n",
    "        # Get original score and explanation\n",
    "        original_score = participant_data[metric]\n",
    "        original_explanation = participant_data[f'{metric}_explanation']\n",
    "        \n",
    "        # Generate improved prompt\n",
    "        improved_prompt = generate_improved_prompt(metric, original_explanation, original_score)\n",
    "        \n",
    "        # Add transcript and assessment to prompt\n",
    "        full_prompt = f\"\"\"{improved_prompt}\n",
    "\n",
    "---\n",
    "Here is the transcript: \n",
    "{transcript}\n",
    "\n",
    "Here is the assessment based on the transcript: \n",
    "{qualitative_assessment}\n",
    "---\"\"\"\n",
    "        \n",
    "        # Make API request\n",
    "        request_data = {\n",
    "            \"model\": ollama_config['model'],\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": full_prompt}],\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(ollama_config['base_url'], json=request_data, timeout=290)\n",
    "            if response.status_code == 200:\n",
    "                content = response.json()['message']['content']\n",
    "                score, _ = parse_score_and_explanation(content)\n",
    "                results[f'{metric}_reassess'] = score\n",
    "                results[f'{metric}_reassess_explanation'] = content\n",
    "                print(f\"    New {metric} score: {score} (was {original_score})\")\n",
    "            else:\n",
    "                results[f'{metric}_reassess'] = None\n",
    "                results[f'{metric}_reassess_explanation'] = f\"API Error: {response.status_code}\"\n",
    "                print(f\"    API error for {metric}: {response.status_code}\")\n",
    "            \n",
    "            time.sleep(2)  # Rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[f'{metric}_reassess'] = None\n",
    "            results[f'{metric}_reassess_explanation'] = f\"Error: {str(e)}\"\n",
    "            print(f\"    Error reassessing {metric}: {str(e)}\")\n",
    "    \n",
    "    # Copy over scores that weren't reassessed\n",
    "    all_metrics = ['coherence', 'completeness', 'specificity', 'accuracy']\n",
    "    for metric in all_metrics:\n",
    "        if metric not in metrics_to_reassess:\n",
    "            results[f'{metric}_reassess'] = participant_data[metric]\n",
    "            results[f'{metric}_reassess_explanation'] = participant_data[f'{metric}_explanation']\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_feedback_loop_reassessment(results_csv_path, qualitative_results_path, ollama_config, threshold=3):\n",
    "    \"\"\"Main function to run the feedback loop reassessment\"\"\"\n",
    "    \n",
    "    # Load original evaluation results\n",
    "    print(\"Loading original evaluation results...\")\n",
    "    eval_results = pd.read_csv(results_csv_path)\n",
    "    \n",
    "    # Load qualitative assessments\n",
    "    print(\"Loading qualitative assessments...\")\n",
    "    qual_results = pd.read_csv(qualitative_results_path)\n",
    "    \n",
    "    # Merge to get qualitative assessments for participants\n",
    "    merged_data = eval_results.merge(qual_results, on='participant_id', how='left')\n",
    "    \n",
    "    # Identify low-scoring participants\n",
    "    low_scoring_participants = identify_low_scoring_participants(results_csv_path, threshold)\n",
    "    \n",
    "    if len(low_scoring_participants) == 0:\n",
    "        print(\"No participants found with low scores. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Merge with qualitative data\n",
    "    reassess_data = low_scoring_participants.merge(qual_results, on='participant_id', how='left')\n",
    "    \n",
    "    print(f\"Starting reassessment of {len(reassess_data)} participants...\")\n",
    "    \n",
    "    reassessment_results = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    for index, row in reassess_data.iterrows():\n",
    "        participant_id = row['participant_id']\n",
    "        \n",
    "        print(f\"\\n--- Reassessing {index + 1}/{len(reassess_data)}: {participant_id} ---\")\n",
    "        \n",
    "        # Load transcript\n",
    "        id_transcript = os.path.join(\"/data/users4/xli/ai-psychiatrist/datasets/daic_woz_dataset/\", \n",
    "                                   f\"{participant_id}_P\", f\"{participant_id}_TRANSCRIPT.csv\")\n",
    "        \n",
    "        if not os.path.exists(id_transcript):\n",
    "            print(f\"Transcript not found for {participant_id}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            transcript_data = pd.read_csv(id_transcript, sep='\\t')\n",
    "            dialogue_txt = transcript_data.dropna(subset=['speaker', 'value'])\n",
    "            dialogue_txt['dialogue'] = dialogue_txt['speaker'] + \" : \" + dialogue_txt['value']\n",
    "            transcript = \"\\n\".join(dialogue_txt['dialogue'])\n",
    "            \n",
    "            # Reassess participant\n",
    "            result = reassess_participant(row, transcript, ollama_config)\n",
    "            reassessment_results.append(result)\n",
    "            processed_count += 1\n",
    "            \n",
    "            print(f\"Completed reassessment for {participant_id} ({processed_count} total)\")\n",
    "            \n",
    "            # Save progress every 5 participants\n",
    "            if len(reassessment_results) % 5 == 0:\n",
    "                temp_df = pd.DataFrame(reassessment_results)\n",
    "                temp_output = results_csv_path.replace('.csv', '_reassessment_temp.csv')\n",
    "                temp_df.to_csv(temp_output, index=False)\n",
    "                print(f\"Saved progress: {len(reassessment_results)} reassessments\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {participant_id}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        time.sleep(1)  # General rate limiting\n",
    "    \n",
    "    # Save final results\n",
    "    if reassessment_results:\n",
    "        output_path = results_csv_path.replace('.csv', '_reassessment_results.csv')\n",
    "        final_df = pd.DataFrame(reassessment_results)\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\n=== REASSESSMENT SUMMARY ===\")\n",
    "        print(f\"Total participants reassessed: {len(reassessment_results)}\")\n",
    "        print(f\"Results saved to: {output_path}\")\n",
    "        \n",
    "        # Calculate improvement statistics\n",
    "        improvement_stats = {}\n",
    "        for metric in ['coherence', 'completeness', 'specificity', 'accuracy']:\n",
    "            original_col = metric\n",
    "            reassess_col = f'{metric}_reassess'\n",
    "            \n",
    "            if original_col in merged_data.columns and reassess_col in final_df.columns:\n",
    "                # Get participants that had this metric reassessed\n",
    "                reassessed_participants = final_df[final_df[reassess_col].notna()]\n",
    "                original_scores = []\n",
    "                new_scores = []\n",
    "                \n",
    "                for _, row in reassessed_participants.iterrows():\n",
    "                    pid = row['participant_id']\n",
    "                    orig_data = merged_data[merged_data['participant_id'] == pid]\n",
    "                    if not orig_data.empty:\n",
    "                        orig_score = orig_data[original_col].iloc[0]\n",
    "                        new_score = row[reassess_col]\n",
    "                        if pd.notna(orig_score) and pd.notna(new_score):\n",
    "                            original_scores.append(orig_score)\n",
    "                            new_scores.append(new_score)\n",
    "                \n",
    "                if original_scores and new_scores:\n",
    "                    avg_original = sum(original_scores) / len(original_scores)\n",
    "                    avg_new = sum(new_scores) / len(new_scores)\n",
    "                    improvement = avg_new - avg_original\n",
    "                    improvement_stats[metric] = {\n",
    "                        'count': len(original_scores),\n",
    "                        'avg_original': avg_original,\n",
    "                        'avg_new': avg_new,\n",
    "                        'improvement': improvement\n",
    "                    }\n",
    "        \n",
    "        print(\"\\nImprovement Statistics:\")\n",
    "        for metric, stats in improvement_stats.items():\n",
    "            print(f\"{metric.capitalize()}: {stats['count']} reassessed, \"\n",
    "                  f\"avg score {stats['avg_original']:.2f} → {stats['avg_new']:.2f} \"\n",
    "                  f\"(+{stats['improvement']:.2f})\")\n",
    "    \n",
    "    else:\n",
    "        print(\"No successful reassessments completed.\")\n",
    "\n",
    "# Configuration\n",
    "OLLAMA_NODE = \"arctrddgxa002\"  # Change this to your Ollama node\n",
    "BASE_URL = f\"http://{OLLAMA_NODE}:11434/api/chat\"\n",
    "MODEL = \"gemma3-optimized:27b\"  # Change this to your model\n",
    "\n",
    "ollama_config = {\n",
    "    'base_url': BASE_URL,\n",
    "    'model': MODEL\n",
    "}\n",
    "\n",
    "# File paths - update these to match your files\n",
    "RESULTS_CSV_PATH = \"/data/users2/nblair7/analysis_results/eval_results_new.csv\"\n",
    "QUALITATIVE_RESULTS_PATH = \"/data/users2/nblair7/analysis_results/qual_resultsfin.csv\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the feedback loop reassessment\n",
    "    run_feedback_loop_reassessment(\n",
    "        results_csv_path=RESULTS_CSV_PATH,\n",
    "        qualitative_results_path=QUALITATIVE_RESULTS_PATH,\n",
    "        ollama_config=ollama_config,\n",
    "        threshold=3  # Reassess participants with scores <= 3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dd3b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENHANCED FEEDBACK LOOP RE-EVALUATION SYSTEM ===\n",
      "Input file: /data/users2/nblair7/analysis_results/qual_resultsfin.csv\n",
      "Feedback assessments file: /data/users2/nblair7/analysis_results/feedback_qualitative_assessments2.csv\n",
      "Feedback evaluations file: /data/users2/nblair7/analysis_results/feedback_evaluation_scores2.csv\n",
      "Loading CSV file...\n",
      "Loaded 142 participants\n",
      "\n",
      "--- Processing 1/142: 303 ---\n",
      "Looking for transcript at: /data/users4/xli/ai-psychiatrist/datasets/daic_woz_dataset/303_P/303_TRANSCRIPT.csv\n",
      "Transcript found, loading data...\n",
      "Dialogue length: 14183 characters\n",
      "  Getting initial coherence response...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 241\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# coherence\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  Getting initial coherence response...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m coherence_response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoherence_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m coherence_response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m    243\u001b[39m     coherence_content = coherence_response.json()[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/http/client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1394\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1397\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    288\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "\n",
    "\n",
    "\n",
    "def parse_score_and_explanation(response_text):\n",
    "    \"\"\"Extract score and explanation from model response\"\"\"\n",
    "    score_patterns = [\n",
    "        r'score[:\\s]*(\\d+)',\n",
    "        r'(\\d+)[/\\s]*(?:out of\\s*)?5',\n",
    "        r'(\\d+)[/\\s]*5',\n",
    "        r'rating[:\\s]*(\\d+)',\n",
    "        r'^(\\d+)',  # Number at start of line\n",
    "    ]\n",
    "    \n",
    "    score = None\n",
    "    for pattern in score_patterns:\n",
    "        match = re.search(pattern, response_text, re.IGNORECASE | re.MULTILINE)\n",
    "        if match:\n",
    "            potential_score = int(match.group(1))\n",
    "            if 1 <= potential_score <= 5:\n",
    "                score = potential_score\n",
    "                break\n",
    "    \n",
    "    return score, response_text.strip()\n",
    "\n",
    "# Configuration\n",
    "OLLAMA_NODE = \"arctrddgxa004\" # TODO: Change this variable to the node where Ollama is running\n",
    "BASE_URL = f\"http://{OLLAMA_NODE}:11434/api/chat\"\n",
    "model = \"gemma3-optimized:27b\" # TODO: Change this variable to the model you want to use\n",
    "\n",
    "# Input file \n",
    "input_csv_path = \"/data/users2/nblair7/analysis_results/qual_resultsfin.csv\"  \n",
    "\n",
    "#Output files\n",
    "feedback_assessments_csv = \"/data/users2/nblair7/analysis_results/feedback_qualitative_assessments2.csv\"  # re-evaluated qualitative assessments\n",
    "feedback_evaluations_csv = \"/data/users2/nblair7/analysis_results/feedback_evaluation_scores2.csv\"  # re-evaluated evaluation scores\n",
    "\n",
    "print(\"=== ENHANCED FEEDBACK LOOP RE-EVALUATION SYSTEM ===\")\n",
    "print(f\"Input file: {input_csv_path}\")\n",
    "print(f\"Feedback assessments file: {feedback_assessments_csv}\")\n",
    "print(f\"Feedback evaluations file: {feedback_evaluations_csv}\")\n",
    "\n",
    "# Load the CSV file\n",
    "print(\"Loading CSV file...\")\n",
    "df = pd.read_csv(input_csv_path)\n",
    "print(f\"Loaded {len(df)} participants\")\n",
    "\n",
    "feedback_assessments = []  # Store re-evaluated qualitative assessments\n",
    "feedback_evaluations = []  # Store re-evaluated evaluation scores\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "feedback_count = 0\n",
    "failed_evaluations = []\n",
    "\n",
    "# Check for existing feedback files to resume processing\n",
    "completed_subjects = set()\n",
    "if os.path.exists(feedback_assessments_csv):\n",
    "    existing_feedback_assessments = pd.read_csv(feedback_assessments_csv)\n",
    "    completed_subjects.update(existing_feedback_assessments['participant_id'].tolist())\n",
    "    feedback_assessments = existing_feedback_assessments.to_dict('records')\n",
    "    print(f\"Found existing feedback assessments: {len(feedback_assessments)} records\")\n",
    "\n",
    "if os.path.exists(feedback_evaluations_csv):\n",
    "    existing_feedback_evaluations = pd.read_csv(feedback_evaluations_csv)\n",
    "    completed_subjects.update(existing_feedback_evaluations['participant_id'].tolist())\n",
    "    feedback_evaluations = existing_feedback_evaluations.to_dict('records')\n",
    "    print(f\"Found existing feedback evaluations: {len(feedback_evaluations)} records\")\n",
    "\n",
    "if completed_subjects:\n",
    "    print(f\"Already completed {len(completed_subjects)} subjects\")\n",
    "    df = df[~df['participant_id'].isin(completed_subjects)]\n",
    "    print(f\"Remaining subjects to process: {len(df)}\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    participant_id = row['participant_id']\n",
    "    qualitative_assessment = row['qualitative_assessment']\n",
    "    \n",
    "    print(f\"\\n--- Processing {index + 1}/{len(df)}: {participant_id} ---\")\n",
    "    \n",
    "    # Load transcript for this participant\n",
    "    id_transcript = os.path.join(\"/data/users4/xli/ai-psychiatrist/datasets/daic_woz_dataset/\", f\"{participant_id}_P\", f\"{participant_id}_TRANSCRIPT.csv\")\n",
    "    print(f\"Looking for transcript at: {id_transcript}\")\n",
    "    \n",
    "    if not os.path.exists(id_transcript):\n",
    "        print(f\"Transcript not found for {participant_id}\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    print(f\"Transcript found, loading data...\")\n",
    "    \n",
    "    try:\n",
    "        transcript_data = pd.read_csv(id_transcript, sep='\\t')\n",
    "        dialogue_txt = transcript_data.dropna(subset=['speaker', 'value'])\n",
    "        dialogue_txt['dialogue'] = dialogue_txt['speaker'] + \" : \" + dialogue_txt['value']\n",
    "        transcript = \"\\n\".join(dialogue_txt['dialogue'])\n",
    "        \n",
    "        print(f\"Dialogue length: {len(transcript)} characters\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading transcript for {participant_id}: {e}\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Original evaluation prompts (for initial assessment to determine if feedback is needed)\n",
    "    coherence_prompt = f\"\"\"Evaluate the following qualitative assessment output for COHERENCE only. Compare qualitative_assessment to the provided transcript. Consider the qualitative assessment (qualitative_assessment) of the transcript (transcript) and compare qualitative_assessment to the provided transcript.\n",
    "Score from 1-5 solely based on the following criteria: \n",
    " - score of 5 : 0 mistakes\n",
    " - score of 4 : 1-2 mistakes\n",
    " - score of 3 : 3-4 mistakes\n",
    " - score of 2 : 5-6 mistakes\n",
    " - score of 1 : 7 or more mistakes\n",
    "\n",
    "Coherence (1–5): Is the response logically consistent? Mistakes are logically inconsistent statements or contradictions within the assessment.\n",
    "\n",
    "Format your response as:\n",
    "Score: [your score]\n",
    "Explanation: [explanation]\n",
    "\n",
    "---\n",
    "Here is the transcript: \n",
    "{transcript}\n",
    "\n",
    "Here is the assessment based on the transcript: \n",
    "{qualitative_assessment}\n",
    "---\"\"\"\n",
    "\n",
    "    completeness_prompt = f\"\"\"Evaluate the following qualitative assessment output for COMPLETENESS only. Compare qualitative_assessment to the provided transcript. Consider the qualitative assessment (qualitative_assessment) of the transcript (transcript) and compare qualitative_assessment to the provided transcript.\n",
    "\n",
    "Score from 1-5 solely based on the following criteria: \n",
    " - score of 5 : 0 mistakes\n",
    " - score of 4 : 1-2 mistakes\n",
    " - score of 3 : 3-4 mistakes\n",
    " - score of 2 : 5-6 mistakes\n",
    " - score of 1 : 7 or more mistakes\n",
    "\n",
    "Completeness (1–5): Does the assessment cover all relevant symptoms, severities, duration/frequency? Mistakes have missed PHQ-8 symptoms, or duration/frequency details.\n",
    "\n",
    "Format your response as:\n",
    "Score: [your score]\n",
    "Explanation: [explanation]\n",
    "\n",
    "Here is the transcript: \n",
    "{transcript}\n",
    "\n",
    "Here is the assessment based on the transcript: \n",
    "{qualitative_assessment}\n",
    "---\"\"\"\n",
    "\n",
    "    specificity_prompt = f\"\"\"Evaluate the following qualitative assessment output for SPECIFICITY only. Consider the qualitative assessment (qualitative_assessment) of the transcript (transcript) and compare qualitative_assessment to the provided transcript.\n",
    "\n",
    "Score from 1-5 solely based on the following criteria: \n",
    " - score of 5 : 0 mistakes\n",
    " - score of 4 : 1-2 mistakes\n",
    " - score of 3 : 3-4 mistakes\n",
    " - score of 2 : 5-6 mistakes\n",
    " - score of 1 : 7 or more mistakes\n",
    "\n",
    "specificity (1–5): Is the assessment specific? Mistakes include using vague/generic statements like 'the patient seems depressed'.\n",
    "\n",
    "Format your response as:\n",
    "Score: [your score]\n",
    "Explanation: [explanation]\n",
    "\n",
    "---\n",
    "Here is the transcript: \n",
    "{transcript}\n",
    "\n",
    "Here is the assessment based on the transcript: \n",
    "{qualitative_assessment}\n",
    "---\"\"\"\n",
    "\n",
    "    accuracy_prompt = f\"\"\"Evaluate the following qualitative assessment output for ACCURACY only. Consider the qualitative assessment (qualitative_assessment) of the transcript (transcript) and compare qualitative_assessment to the provided transcript.\n",
    "\n",
    "Score from 1-5 solely based on the following criteria: \n",
    " - score of 5 : 0 mistakes\n",
    " - score of 4 : 1-2 mistakes\n",
    " - score of 3 : 3-4 mistakes\n",
    " - score of 2 : 5-6 mistakes\n",
    " - score of 1 : 7 or more mistakes\n",
    "\n",
    "Accuracy (1–5): Are the signs/symptoms aligned with DSM-5 or PHQ-8? Mistakes are incorrect symptoms or incorrect duration/frequecy. \n",
    "\n",
    "Format your response as:\n",
    "Score: [your score]\n",
    "Explanation: [explanation]\n",
    "\n",
    "---\n",
    "Here is the transcript: \n",
    "{transcript}\n",
    "\n",
    "Here is the assessment based on the transcript: \n",
    "{qualitative_assessment}\n",
    "---\"\"\"\n",
    "\n",
    "    # Initial evaluation requests\n",
    "    coherence_request = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": coherence_prompt}],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "    }\n",
    "    \n",
    "    completeness_request = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": completeness_prompt}],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "    }\n",
    "    \n",
    "    specificity_request = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": specificity_prompt}],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "    }\n",
    "    \n",
    "    accuracy_request = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": accuracy_prompt}],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "    }\n",
    "    \n",
    "    timeout = 300  \n",
    "    \n",
    "    try:\n",
    "        # Initial evaluation to check if feedback is needed\n",
    "        initial_scores = {}\n",
    "        initial_explanations = {}\n",
    "        \n",
    "        # coherence\n",
    "        print(\"  Getting initial coherence response...\")\n",
    "        coherence_response = requests.post(BASE_URL, json=coherence_request, timeout=timeout-10)\n",
    "        if coherence_response.status_code == 200:\n",
    "            coherence_content = coherence_response.json()['message']['content']\n",
    "            coherence_score, _ = parse_score_and_explanation(coherence_content)\n",
    "            initial_scores['coherence'] = coherence_score\n",
    "            initial_explanations['coherence'] = coherence_content\n",
    "            print(f\"  Initial coherence score: {coherence_score}\")\n",
    "        else:\n",
    "            initial_scores['coherence'] = None\n",
    "            initial_explanations['coherence'] = None\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        # completeness\n",
    "        print(\"  Getting initial completeness response...\")\n",
    "        completeness_response = requests.post(BASE_URL, json=completeness_request, timeout=timeout-10)\n",
    "        if completeness_response.status_code == 200:\n",
    "            completeness_content = completeness_response.json()['message']['content']\n",
    "            completeness_score, _ = parse_score_and_explanation(completeness_content)\n",
    "            initial_scores['completeness'] = completeness_score\n",
    "            initial_explanations['completeness'] = completeness_content\n",
    "            print(f\"  Initial completeness score: {completeness_score}\")\n",
    "        else:\n",
    "            initial_scores['completeness'] = None\n",
    "            initial_explanations['completeness'] = None\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        # specificity\n",
    "        print(\"  Getting initial specificity response...\")\n",
    "        specificity_response = requests.post(BASE_URL, json=specificity_request, timeout=timeout-10)\n",
    "        if specificity_response.status_code == 200:\n",
    "            specificity_content = specificity_response.json()['message']['content']\n",
    "            specificity_score, _ = parse_score_and_explanation(specificity_content)\n",
    "            initial_scores['specificity'] = specificity_score\n",
    "            initial_explanations['specificity'] = specificity_content\n",
    "            print(f\"  Initial specificity score: {specificity_score}\")\n",
    "        else:\n",
    "            initial_scores['specificity'] = None\n",
    "            initial_explanations['specificity'] = None\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        # accuracy\n",
    "        print(\"  Getting initial accuracy response...\")\n",
    "        accuracy_response = requests.post(BASE_URL, json=accuracy_request, timeout=timeout-10)\n",
    "        if accuracy_response.status_code == 200:\n",
    "            accuracy_content = accuracy_response.json()['message']['content']\n",
    "            accuracy_score, _ = parse_score_and_explanation(accuracy_content)\n",
    "            initial_scores['accuracy'] = accuracy_score\n",
    "            initial_explanations['accuracy'] = accuracy_content\n",
    "            print(f\"  Initial accuracy score: {accuracy_score}\")\n",
    "        else:\n",
    "            initial_scores['accuracy'] = None\n",
    "            initial_explanations['accuracy'] = None\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Check if any score is <= 3 to trigger feedback loop\n",
    "        low_scores = []\n",
    "        for metric in ['coherence', 'completeness', 'specificity', 'accuracy']:\n",
    "            if initial_scores[metric] and initial_scores[metric] <= 3:\n",
    "                low_scores.append(metric)\n",
    "        \n",
    "        if low_scores:\n",
    "            print(f\"  LOW SCORES DETECTED: {low_scores} - Starting enhanced feedback loop...\")\n",
    "            feedback_count += 1\n",
    "            \n",
    "            # Keep track of all attempts for this participant\n",
    "            iteration = 0\n",
    "            max_iterations = 10  # Prevent infinite loops\n",
    "            current_assessment = qualitative_assessment\n",
    "            current_scores = initial_scores.copy()\n",
    "            current_explanations = initial_explanations.copy()\n",
    "            all_low_scores = []  # Track which scores were low across iterations\n",
    "            \n",
    "            # Feedback loop - keep trying until all scores are 4 or 5\n",
    "            while low_scores and iteration < max_iterations:\n",
    "                iteration += 1\n",
    "                print(f\"    Iteration {iteration}: Improving assessment for low scores: {low_scores}\")\n",
    "                all_low_scores.extend(low_scores)\n",
    "                \n",
    "                # Build feedback message for low scoring metrics\n",
    "                feedback_details = []\n",
    "                for metric in low_scores:\n",
    "                    if current_scores[metric] and current_explanations[metric]:\n",
    "                        metric_description = {\n",
    "                            'coherence': 'logical consistency (avoiding contradictions within the assessment)',\n",
    "                            'completeness': 'comprehensive coverage (including all relevant PHQ-8 symptoms, severities, and duration/frequency details)',\n",
    "                            'specificity': 'specific details (avoiding vague/generic statements like \"the patient seems depressed\")',\n",
    "                            'accuracy': 'accuracy according to DSM-5 or PHQ-8 criteria (correct symptoms and duration/frequency)'\n",
    "                        }\n",
    "                        feedback_details.append(f\"\"\"\n",
    "- **{metric.upper()}** scored {current_scores[metric]}/5 because: {current_explanations[metric]}\n",
    "  Please specifically improve the {metric_description[metric]} in your revised assessment.\"\"\")\n",
    "                \n",
    "                # Enhanced regeneration prompt with feedback context\n",
    "                system_prompt = \"You are a psychiatrist. Your job is to assess and diagnose patients for depression. Be as objective, concise, and accurate as possible.\"\n",
    "                \n",
    "                feedback_prompt = f\"\"\"The following is an interview transcript from a psychiatric assessment of a subject who is being screened for depression. Please note that 'Ellie' is the interviewer, and 'Participant' is the subject being assessed.\n",
    "\n",
    "TRANSCRIPT:\n",
    "{transcript}\n",
    "\n",
    "ORIGINAL ASSESSMENT:\n",
    "{qualitative_assessment}\n",
    "\n",
    "EVALUATION FEEDBACK:\n",
    "The original assessment has been evaluated and needs improvement in the following areas:\n",
    "{chr(10).join(feedback_details)}\n",
    "\n",
    "TASK:\n",
    "Please reassess the transcript and provide an improved qualitative assessment that addresses the specific feedback above. Build upon the original assessment but focus on improving the identified weaknesses.\n",
    "\n",
    "Please:\n",
    "1. Provide an overall qualitative assessment of the subject's mental health. \n",
    "2. Summarize social aspects that may influence the subject's mental health. (ex. familial relationships, friendship dynamics, work environment, etc. that are relevant to the subject's mental health)\n",
    "3. Summarize biological aspects that may influence the subject's mental health. (ex. familial history of mental health issues, previous or pre-existing mental health issues, stress levels, etc. that are relevant to the subject's mental health)\n",
    "4. Identify potential risk factors the subject may be experiencing.\n",
    "5. Use exact quotes from the transcript to support your assessment for each tag.\n",
    "\n",
    "Output should be formatted as bullet points with headings for each section using stars. Example: **Tiredness** <explanation of tiredness>. Do not include any additional text outside the bullet points.\n",
    "\n",
    "Please answer in this XML format with each tag on a new line, properly indented. Use straight quotes instead of curly quotes, and do not include any additional text outside the XML tags:\n",
    "\n",
    "<assessment>\n",
    "  <!-- Summary of participant's overall mental health -->\n",
    "  <exact_quotes>\n",
    "    <!-- Quotes from the transcript that support the assessment -->\n",
    "  </exact_quotes>\n",
    "</assessment>\n",
    "\n",
    "<social_factors>\n",
    "  <!-- Summary of social influences on patient's health -->\n",
    "  <exact_quotes>\n",
    "    <!-- Quotes from the transcript that support the assessment -->\n",
    "  </exact_quotes>\n",
    "</social_factors>\n",
    "\n",
    "<biological_factors>\n",
    "  <!-- Summary of biological influences on patient's health -->\n",
    "  <exact_quotes>\n",
    "    <!-- Quotes from the transcript that support the assessment -->\n",
    "  </exact_quotes>\n",
    "</biological_factors>\n",
    "\n",
    "<risk_factors>\n",
    "  <!-- Summary of potential risk factors -->\n",
    "  <exact_quotes>\n",
    "    <!-- Quotes from the transcript that support the assessment -->\n",
    "  </exact_quotes>\n",
    "</risk_factors>\n",
    "\"\"\"\n",
    "                \n",
    "                feedback_request = {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                               {\"role\": \"user\", \"content\": feedback_prompt}],\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "                }\n",
    "                \n",
    "                feedback_response = requests.post(BASE_URL, json=feedback_request, timeout=timeout)\n",
    "                if feedback_response.status_code != 200:\n",
    "                    print(f\"    Failed to regenerate assessment: {feedback_response.status_code}\")\n",
    "                    failed_evaluations.append(participant_id)\n",
    "                    break\n",
    "                \n",
    "                current_assessment = feedback_response.json()['message']['content']\n",
    "                print(f\"    New assessment generated with targeted improvements, re-evaluating...\")\n",
    "                \n",
    "                # Re-evaluate with new assessment\n",
    "                new_coherence_prompt = coherence_prompt.replace(qualitative_assessment, current_assessment)\n",
    "                new_completeness_prompt = completeness_prompt.replace(qualitative_assessment, current_assessment)\n",
    "                new_specificity_prompt = specificity_prompt.replace(qualitative_assessment, current_assessment)\n",
    "                new_accuracy_prompt = accuracy_prompt.replace(qualitative_assessment, current_assessment)\n",
    "                \n",
    "                # Store new scores and explanations\n",
    "                new_scores = {}\n",
    "                new_explanations = {}\n",
    "                \n",
    "                # Re-evaluate coherence\n",
    "                time.sleep(2)\n",
    "                new_coherence_request = {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": new_coherence_prompt}],\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "                }\n",
    "                new_coherence_response = requests.post(BASE_URL, json=new_coherence_request, timeout=timeout-10)\n",
    "                if new_coherence_response.status_code == 200:\n",
    "                    new_coherence_content = new_coherence_response.json()['message']['content']\n",
    "                    new_coherence_score, _ = parse_score_and_explanation(new_coherence_content)\n",
    "                    new_scores['coherence'] = new_coherence_score\n",
    "                    new_explanations['coherence'] = new_coherence_content\n",
    "                \n",
    "                # Re-evaluate completeness\n",
    "                time.sleep(2)\n",
    "                new_completeness_request = {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": new_completeness_prompt}],\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "                }\n",
    "                new_completeness_response = requests.post(BASE_URL, json=new_completeness_request, timeout=timeout-10)\n",
    "                if new_completeness_response.status_code == 200:\n",
    "                    new_completeness_content = new_completeness_response.json()['message']['content']\n",
    "                    new_completeness_score, _ = parse_score_and_explanation(new_completeness_content)\n",
    "                    new_scores['completeness'] = new_completeness_score\n",
    "                    new_explanations['completeness'] = new_completeness_content\n",
    "                \n",
    "                # Re-evaluate specificity\n",
    "                time.sleep(2)\n",
    "                new_specificity_request = {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": new_specificity_prompt}],\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "                }\n",
    "                new_specificity_response = requests.post(BASE_URL, json=new_specificity_request, timeout=timeout-10)\n",
    "                if new_specificity_response.status_code == 200:\n",
    "                    new_specificity_content = new_specificity_response.json()['message']['content']\n",
    "                    new_specificity_score, _ = parse_score_and_explanation(new_specificity_content)\n",
    "                    new_scores['specificity'] = new_specificity_score\n",
    "                    new_explanations['specificity'] = new_specificity_content\n",
    "                \n",
    "                # Re-evaluate accuracy\n",
    "                time.sleep(2)\n",
    "                new_accuracy_request = {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": new_accuracy_prompt}],\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "                }\n",
    "                new_accuracy_response = requests.post(BASE_URL, json=new_accuracy_request, timeout=timeout-10)\n",
    "                if new_accuracy_response.status_code == 200:\n",
    "                    new_accuracy_content = new_accuracy_response.json()['message']['content']\n",
    "                    new_accuracy_score, _ = parse_score_and_explanation(new_accuracy_content)\n",
    "                    new_scores['accuracy'] = new_accuracy_score\n",
    "                    new_explanations['accuracy'] = new_accuracy_content\n",
    "                \n",
    "                # Update current scores and explanations for next iteration\n",
    "                current_scores.update(new_scores)\n",
    "                current_explanations.update(new_explanations)\n",
    "                \n",
    "                # Check which scores are still low\n",
    "                low_scores = []\n",
    "                for metric in ['coherence', 'completeness', 'specificity', 'accuracy']:\n",
    "                    if metric in new_scores and new_scores[metric] and new_scores[metric] <= 3:\n",
    "                        low_scores.append(metric)\n",
    "                \n",
    "                # Print current scores\n",
    "                print(f\"    Iteration {iteration} scores: \" + \n",
    "                      f\"Coherence={current_scores.get('coherence', 'N/A')}, \" +\n",
    "                      f\"Completeness={current_scores.get('completeness', 'N/A')}, \" +\n",
    "                      f\"Specificity={current_scores.get('specificity', 'N/A')}, \" +\n",
    "                      f\"Accuracy={current_scores.get('accuracy', 'N/A')}\")\n",
    "                \n",
    "                if low_scores:\n",
    "                    print(f\"    Still have low scores: {low_scores}, continuing with targeted feedback...\")\n",
    "                else:\n",
    "                    print(f\"    All scores now 4 or 5! Enhanced feedback loop complete after {iteration} iterations.\")\n",
    "            \n",
    "            # Save final results after feedback loop completes\n",
    "            if iteration >= max_iterations:\n",
    "                print(f\"    Reached max iterations ({max_iterations}), stopping feedback loop\")\n",
    "            \n",
    "            # Save the final qualitative assessment\n",
    "            feedback_assessment_record = {\n",
    "                'participant_id': participant_id,\n",
    "                'original_qualitative_assessment': qualitative_assessment,\n",
    "                'feedback_qualitative_assessment': current_assessment,\n",
    "                'iterations_required': iteration,\n",
    "                'low_scores_detected': ', '.join(sorted(set(all_low_scores))),\n",
    "                'initial_coherence_score': initial_scores.get('coherence'),\n",
    "                'initial_completeness_score': initial_scores.get('completeness'),\n",
    "                'initial_specificity_score': initial_scores.get('specificity'),\n",
    "                'initial_accuracy_score': initial_scores.get('accuracy'),\n",
    "                'final_coherence_score': current_scores.get('coherence'),\n",
    "                'final_completeness_score': current_scores.get('completeness'),\n",
    "                'final_specificity_score': current_scores.get('specificity'),\n",
    "                'final_accuracy_score': current_scores.get('accuracy')\n",
    "            }\n",
    "            feedback_assessments.append(feedback_assessment_record)\n",
    "            \n",
    "            # Save the final evaluation scores\n",
    "            feedback_eval_record = {\n",
    "                'participant_id': participant_id,\n",
    "                'iterations_required': iteration,\n",
    "                'low_scores_detected': ', '.join(sorted(set(all_low_scores)))\n",
    "            }\n",
    "            \n",
    "            # Add final scores and explanations to record\n",
    "            for metric in ['coherence', 'completeness', 'specificity', 'accuracy']:\n",
    "                if metric in current_scores:\n",
    "                    feedback_eval_record[f'final_{metric}_score'] = current_scores[metric]\n",
    "                    feedback_eval_record[f'final_{metric}_explanation'] = current_explanations[metric]\n",
    "                if metric in initial_scores:\n",
    "                    feedback_eval_record[f'initial_{metric}_score'] = initial_scores[metric]\n",
    "                    feedback_eval_record[f'initial_{metric}_explanation'] = initial_explanations[metric]\n",
    "            \n",
    "            feedback_evaluations.append(feedback_eval_record)\n",
    "            processed_count += 1\n",
    "        else:\n",
    "            print(f\"  No low scores detected - skipping feedback loop\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Completed participant {participant_id} in {elapsed_time:.1f}s ({processed_count} with feedback applied)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing participant {participant_id}: {e}\")\n",
    "        failed_evaluations.append(participant_id)\n",
    "    \n",
    "    # Save progress every 10 participants\n",
    "    if (len(feedback_assessments) % 10 == 0 and len(feedback_assessments) > 0) or len(feedback_assessments) == 1:\n",
    "        # Save feedback assessments\n",
    "        if feedback_assessments:\n",
    "            feedback_assessments_df = pd.DataFrame(feedback_assessments)\n",
    "            feedback_assessments_df.to_csv(feedback_assessments_csv, index=False)\n",
    "            print(f\"Saved feedback assessments: {len(feedback_assessments)} records to {feedback_assessments_csv}\")\n",
    "        \n",
    "        # Save feedback evaluations\n",
    "        if feedback_evaluations:\n",
    "            feedback_evaluations_df = pd.DataFrame(feedback_evaluations)\n",
    "            feedback_evaluations_df.to_csv(feedback_evaluations_csv, index=False)\n",
    "            print(f\"Saved feedback evaluations: {len(feedback_evaluations)} records to {feedback_evaluations_csv}\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n=== PROCESSING SUMMARY ===\")\n",
    "print(f\"Total subjects processed: {len(df)}\")\n",
    "print(f\"Skipped (no transcript): {skipped_count}\")\n",
    "print(f\"Feedback loops applied: {feedback_count}\")\n",
    "print(f\"Successfully processed with feedback: {processed_count}\")\n",
    "print(f\"Failed: {len(failed_evaluations)}\")\n",
    "print(f\"Feedback assessments created: {len(feedback_assessments)}\")\n",
    "print(f\"Feedback evaluations created: {len(feedback_evaluations)}\")\n",
    "\n",
    "if failed_evaluations:\n",
    "    print(f\"Failed participant IDs: {failed_evaluations}\")\n",
    "\n",
    "# Save final feedback files\n",
    "if feedback_assessments:\n",
    "    feedback_assessments_df = pd.DataFrame(feedback_assessments)\n",
    "    feedback_assessments_df.to_csv(feedback_assessments_csv, index=False)\n",
    "    print(f\"Final feedback assessments saved: {feedback_assessments_csv}\")\n",
    "    print(f\"Enhanced feedback assessments CSV columns:\")\n",
    "    print(f\"- participant_id\")\n",
    "    print(f\"- original_qualitative_assessment\")\n",
    "    print(f\"- feedback_qualitative_assessment\")\n",
    "    print(f\"- iterations_required\")\n",
    "    print(f\"- low_scores_detected\")\n",
    "    print(f\"- initial_[metric]_score and final_[metric]_score for comparison\")\n",
    "\n",
    "if feedback_evaluations:\n",
    "    feedback_evaluations_df = pd.DataFrame(feedback_evaluations)\n",
    "    feedback_evaluations_df.to_csv(feedback_evaluations_csv, index=False)\n",
    "    print(f\"Final feedback evaluations saved: {feedback_evaluations_csv}\")\n",
    "    print(f\"Enhanced feedback evaluations CSV columns:\")\n",
    "    print(f\"- participant_id\")\n",
    "    print(f\"- iterations_required\")\n",
    "    print(f\"- initial_[metric]_score / final_[metric]_score\")\n",
    "    print(f\"- initial_[metric]_explanation / final_[metric]_explanation\")\n",
    "    print(f\"- low_scores_detected\")\n",
    "\n",
    "if not feedback_assessments and not feedback_evaluations:\n",
    "    print(\"No participants required feedback - no CSV files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f3af89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENHANCED FEEDBACK LOOP RE-EVALUATION SYSTEM (FAILED IDs ONLY) ===\n",
      "Input file: /data/users2/nblair7/analysis_results/qual_resultsfin.csv\n",
      "Failed IDs to process: [380, 383, 385, 386, 391, 392, 393, 397, 400, 401, 402, 409, 412, 414, 415, 416, 419, 423, 425, 426, 427, 428, 429, 430, 433, 434, 437, 441, 443, 444, 445, 446, 447, 448, 449, 454, 455, 456, 457, 459, 463, 464, 468, 471, 473, 474, 475, 478, 479, 485, 486, 487, 488, 491, 302, 307, 331, 335, 346, 367, 377, 381, 382, 388, 389, 390, 395, 403, 404, 406, 413, 417, 418, 420, 422, 436, 439, 440, 451, 458, 472, 476, 477, 482, 483, 484, 489, 490, 492]\n",
      "Feedback assessments file: /data/users2/nblair7/analysis_results/ASSESSMENTT.csv\n",
      "Feedback evaluations file: /data/users2/nblair7/analysis_results/SCOREST.csv\n",
      "Loading CSV file...\n",
      "Loaded 142 participants\n",
      "Filtered to 89 failed participants to reprocess\n",
      "\n",
      "--- Processing 54/89: 380 ---\n",
      "Looking for transcript at: /data/users4/xli/ai-psychiatrist/datasets/daic_woz_dataset/380_P/380_TRANSCRIPT.csv\n",
      "Transcript found, loading data...\n",
      "Dialogue length: 28949 characters\n",
      "  Getting initial coherence response...\n",
      "  Initial coherence score: 5\n",
      "  Getting initial completeness response...\n",
      "  Initial completeness score: 3\n",
      "  Getting initial specificity response...\n",
      "  Initial specificity score: 5\n",
      "  Getting initial accuracy response...\n",
      "  Initial accuracy score: 5\n",
      "  LOW SCORES DETECTED: ['completeness'] - Starting enhanced feedback loop...\n",
      "    Iteration 1: Improving assessment for low scores: ['completeness']\n",
      "    New assessment generated with targeted improvements, re-evaluating...\n",
      "    Iteration 1 scores: Coherence=5, Completeness=3, Specificity=5, Accuracy=5\n",
      "    Still have low scores: ['completeness'], continuing with targeted feedback...\n",
      "    Iteration 2: Improving assessment for low scores: ['completeness']\n",
      "    New assessment generated with targeted improvements, re-evaluating...\n",
      "    Iteration 2 scores: Coherence=5, Completeness=3, Specificity=5, Accuracy=4\n",
      "    Still have low scores: ['completeness'], continuing with targeted feedback...\n",
      "    Iteration 3: Improving assessment for low scores: ['completeness']\n",
      "    New assessment generated with targeted improvements, re-evaluating...\n",
      "    Iteration 3 scores: Coherence=5, Completeness=3, Specificity=5, Accuracy=5\n",
      "    Still have low scores: ['completeness'], continuing with targeted feedback...\n",
      "    Iteration 4: Improving assessment for low scores: ['completeness']\n",
      "    New assessment generated with targeted improvements, re-evaluating...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "\n",
    "\n",
    "\n",
    "def parse_score_and_explanation(response_text):\n",
    "    \"\"\"Extract score and explanation from model response\"\"\"\n",
    "    score_patterns = [\n",
    "        r'score[:\\s]*(\\d+)',\n",
    "        r'(\\d+)[/\\s]*(?:out of\\s*)?5',\n",
    "        r'(\\d+)[/\\s]*5',\n",
    "        r'rating[:\\s]*(\\d+)',\n",
    "        r'^(\\d+)',  # Number at start of line\n",
    "    ]\n",
    "    \n",
    "    score = None\n",
    "    for pattern in score_patterns:\n",
    "        match = re.search(pattern, response_text, re.IGNORECASE | re.MULTILINE)\n",
    "        if match:\n",
    "            potential_score = int(match.group(1))\n",
    "            if 1 <= potential_score <= 5:\n",
    "                score = potential_score\n",
    "                break\n",
    "    \n",
    "    return score, response_text.strip()\n",
    "\n",
    "# Configuration\n",
    "OLLAMA_NODE = \"arctrddgxa004\" # TODO: Change this variable to the node where Ollama is running\n",
    "BASE_URL = f\"http://{OLLAMA_NODE}:11434/api/chat\"\n",
    "model = \"gemma3-optimized:27b\" # TODO: Change this variable to the model you want to use\n",
    "\n",
    "# FAILED IDs TO PROCESS - Add your failed participant IDs here\n",
    "FAILED_IDS = [\n",
    "    380, 383, 385, 386, 391, 392, 393, 397, 400, 401, 402, 409, 412, 414, 415, 416, 419, 423, 425, 426, 427, 428, 429, 430, 433, 434, 437, 441, 443, 444, 445, 446, 447, 448, 449, 454, 455, 456, 457, 459, 463, 464, 468, 471, 473, 474, 475, 478, 479, 485, 486, 487, 488, 491, 302, 307, 331, 335, 346, 367, 377, 381, 382, 388, 389, 390, 395, 403, 404, 406, 413, 417, 418, 420, 422, 436, 439, 440, 451, 458, 472, 476, 477, 482, 483, 484, 489, 490, 492\n",
    "]\n",
    "\n",
    "# Input file \n",
    "input_csv_path = \"/data/users2/nblair7/analysis_results/qual_resultsfin.csv\"  \n",
    "\n",
    "#Output files\n",
    "feedback_assessments_csv = \"/data/users2/nblair7/analysis_results/ASSESSMENTT.csv\"  # re-evaluated qualitative assessments\n",
    "feedback_evaluations_csv = \"/data/users2/nblair7/analysis_results/SCOREST.csv\"  # re-evaluated evaluation scores\n",
    "\n",
    "print(\"=== ENHANCED FEEDBACK LOOP RE-EVALUATION SYSTEM (FAILED IDs ONLY) ===\")\n",
    "print(f\"Input file: {input_csv_path}\")\n",
    "print(f\"Failed IDs to process: {FAILED_IDS}\")\n",
    "print(f\"Feedback assessments file: {feedback_assessments_csv}\")\n",
    "print(f\"Feedback evaluations file: {feedback_evaluations_csv}\")\n",
    "\n",
    "# Load the CSV file\n",
    "print(\"Loading CSV file...\")\n",
    "df = pd.read_csv(input_csv_path)\n",
    "print(f\"Loaded {len(df)} participants\")\n",
    "\n",
    "# Filter to only process failed IDs\n",
    "if FAILED_IDS:\n",
    "    df = df[df['participant_id'].astype(str).isin([str(pid) for pid in FAILED_IDS])]\n",
    "    print(f\"Filtered to {len(df)} failed participants to reprocess\")\n",
    "else:\n",
    "    print(\"WARNING: No failed IDs specified in FAILED_IDS list. Please add them to the FAILED_IDS variable.\")\n",
    "    exit(1)\n",
    "\n",
    "feedback_assessments = []  # Store re-evaluated qualitative assessments\n",
    "feedback_evaluations = []  # Store re-evaluated evaluation scores\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "feedback_count = 0\n",
    "failed_evaluations = []\n",
    "\n",
    "# Check for existing feedback files to resume processing\n",
    "completed_subjects = set()\n",
    "if os.path.exists(feedback_assessments_csv):\n",
    "    existing_feedback_assessments = pd.read_csv(feedback_assessments_csv)\n",
    "    completed_subjects.update(existing_feedback_assessments['participant_id'].tolist())\n",
    "    feedback_assessments = existing_feedback_assessments.to_dict('records')\n",
    "    print(f\"Found existing feedback assessments: {len(feedback_assessments)} records\")\n",
    "\n",
    "if os.path.exists(feedback_evaluations_csv):\n",
    "    existing_feedback_evaluations = pd.read_csv(feedback_evaluations_csv)\n",
    "    completed_subjects.update(existing_feedback_evaluations['participant_id'].tolist())\n",
    "    feedback_evaluations = existing_feedback_evaluations.to_dict('records')\n",
    "    print(f\"Found existing feedback evaluations: {len(feedback_evaluations)} records\")\n",
    "\n",
    "if completed_subjects:\n",
    "    print(f\"Already completed {len(completed_subjects)} subjects\")\n",
    "    df = df[~df['participant_id'].isin(completed_subjects)]\n",
    "    print(f\"Remaining subjects to process: {len(df)}\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    participant_id = row['participant_id']\n",
    "    qualitative_assessment = row['qualitative_assessment']\n",
    "    \n",
    "    print(f\"\\n--- Processing {index + 1}/{len(df)}: {participant_id} ---\")\n",
    "    \n",
    "    # Load transcript for this participant\n",
    "    id_transcript = os.path.join(\"/data/users4/xli/ai-psychiatrist/datasets/daic_woz_dataset/\", f\"{participant_id}_P\", f\"{participant_id}_TRANSCRIPT.csv\")\n",
    "    print(f\"Looking for transcript at: {id_transcript}\")\n",
    "    \n",
    "    if not os.path.exists(id_transcript):\n",
    "        print(f\"Transcript not found for {participant_id}\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    print(f\"Transcript found, loading data...\")\n",
    "    \n",
    "    try:\n",
    "        transcript_data = pd.read_csv(id_transcript, sep='\\t')\n",
    "        dialogue_txt = transcript_data.dropna(subset=['speaker', 'value'])\n",
    "        dialogue_txt['dialogue'] = dialogue_txt['speaker'] + \" : \" + dialogue_txt['value']\n",
    "        transcript = \"\\n\".join(dialogue_txt['dialogue'])\n",
    "        \n",
    "        print(f\"Dialogue length: {len(transcript)} characters\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading transcript for {participant_id}: {e}\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Original evaluation prompts (for initial assessment to determine if feedback is needed)\n",
    "    coherence_prompt = f\"\"\"Evaluate the following qualitative assessment output for COHERENCE only. Compare qualitative_assessment to the provided transcript. Consider the qualitative assessment (qualitative_assessment) of the transcript (transcript) and compare qualitative_assessment to the provided transcript.\n",
    "Score from 1-5 solely based on the following criteria: \n",
    " - score of 5 : 0 mistakes\n",
    " - score of 4 : 1-2 mistakes\n",
    " - score of 3 : 3-4 mistakes\n",
    " - score of 2 : 5-6 mistakes\n",
    " - score of 1 : 7 or more mistakes\n",
    "\n",
    "Coherence (1–5): Is the response logically consistent? Mistakes are logically inconsistent statements or contradictions within the assessment.\n",
    "\n",
    "Format your response as:\n",
    "Score: [your score]\n",
    "Explanation: [explanation]\n",
    "\n",
    "---\n",
    "Here is the transcript: \n",
    "{transcript}\n",
    "\n",
    "Here is the assessment based on the transcript: \n",
    "{qualitative_assessment}\n",
    "---\"\"\"\n",
    "\n",
    "    completeness_prompt = f\"\"\"Evaluate the following qualitative assessment output for COMPLETENESS only. Compare qualitative_assessment to the provided transcript. Consider the qualitative assessment (qualitative_assessment) of the transcript (transcript) and compare qualitative_assessment to the provided transcript.\n",
    "\n",
    "Score from 1-5 solely based on the following criteria: \n",
    " - score of 5 : 0 mistakes\n",
    " - score of 4 : 1-2 mistakes\n",
    " - score of 3 : 3-4 mistakes\n",
    " - score of 2 : 5-6 mistakes\n",
    " - score of 1 : 7 or more mistakes\n",
    "\n",
    "Completeness (1–5): Does the assessment cover all relevant symptoms, severities, duration/frequency? Mistakes have missed PHQ-8 symptoms, or duration/frequency details.\n",
    "\n",
    "Format your response as:\n",
    "Score: [your score]\n",
    "Explanation: [explanation]\n",
    "\n",
    "Here is the transcript: \n",
    "{transcript}\n",
    "\n",
    "Here is the assessment based on the transcript: \n",
    "{qualitative_assessment}\n",
    "---\"\"\"\n",
    "\n",
    "    specificity_prompt = f\"\"\"Evaluate the following qualitative assessment output for SPECIFICITY only. Consider the qualitative assessment (qualitative_assessment) of the transcript (transcript) and compare qualitative_assessment to the provided transcript.\n",
    "\n",
    "Score from 1-5 solely based on the following criteria: \n",
    " - score of 5 : 0 mistakes\n",
    " - score of 4 : 1-2 mistakes\n",
    " - score of 3 : 3-4 mistakes\n",
    " - score of 2 : 5-6 mistakes\n",
    " - score of 1 : 7 or more mistakes\n",
    "\n",
    "specificity (1–5): Is the assessment specific? Mistakes include using vague/generic statements like 'the patient seems depressed'.\n",
    "\n",
    "Format your response as:\n",
    "Score: [your score]\n",
    "Explanation: [explanation]\n",
    "\n",
    "---\n",
    "Here is the transcript: \n",
    "{transcript}\n",
    "\n",
    "Here is the assessment based on the transcript: \n",
    "{qualitative_assessment}\n",
    "---\"\"\"\n",
    "\n",
    "    accuracy_prompt = f\"\"\"Evaluate the following qualitative assessment output for ACCURACY only. Consider the qualitative assessment (qualitative_assessment) of the transcript (transcript) and compare qualitative_assessment to the provided transcript.\n",
    "\n",
    "Score from 1-5 solely based on the following criteria: \n",
    " - score of 5 : 0 mistakes\n",
    " - score of 4 : 1-2 mistakes\n",
    " - score of 3 : 3-4 mistakes\n",
    " - score of 2 : 5-6 mistakes\n",
    " - score of 1 : 7 or more mistakes\n",
    "\n",
    "Accuracy (1–5): Are the signs/symptoms aligned with DSM-5 or PHQ-8? Mistakes are incorrect symptoms or incorrect duration/frequecy. \n",
    "\n",
    "Format your response as:\n",
    "Score: [your score]\n",
    "Explanation: [explanation]\n",
    "\n",
    "---\n",
    "Here is the transcript: \n",
    "{transcript}\n",
    "\n",
    "Here is the assessment based on the transcript: \n",
    "{qualitative_assessment}\n",
    "---\"\"\"\n",
    "\n",
    "    # Initial evaluation requests\n",
    "    coherence_request = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": coherence_prompt}],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "    }\n",
    "    \n",
    "    completeness_request = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": completeness_prompt}],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "    }\n",
    "    \n",
    "    specificity_request = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": specificity_prompt}],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "    }\n",
    "    \n",
    "    accuracy_request = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": accuracy_prompt}],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "    }\n",
    "    \n",
    "    timeout = 300  \n",
    "    \n",
    "    try:\n",
    "        # Initial evaluation to check if feedback is needed\n",
    "        initial_scores = {}\n",
    "        initial_explanations = {}\n",
    "        \n",
    "        # coherence\n",
    "        print(\"  Getting initial coherence response...\")\n",
    "        coherence_response = requests.post(BASE_URL, json=coherence_request, timeout=timeout-10)\n",
    "        if coherence_response.status_code == 200:\n",
    "            coherence_content = coherence_response.json()['message']['content']\n",
    "            coherence_score, _ = parse_score_and_explanation(coherence_content)\n",
    "            initial_scores['coherence'] = coherence_score\n",
    "            initial_explanations['coherence'] = coherence_content\n",
    "            print(f\"  Initial coherence score: {coherence_score}\")\n",
    "        else:\n",
    "            initial_scores['coherence'] = None\n",
    "            initial_explanations['coherence'] = None\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        # completeness\n",
    "        print(\"  Getting initial completeness response...\")\n",
    "        completeness_response = requests.post(BASE_URL, json=completeness_request, timeout=timeout-10)\n",
    "        if completeness_response.status_code == 200:\n",
    "            completeness_content = completeness_response.json()['message']['content']\n",
    "            completeness_score, _ = parse_score_and_explanation(completeness_content)\n",
    "            initial_scores['completeness'] = completeness_score\n",
    "            initial_explanations['completeness'] = completeness_content\n",
    "            print(f\"  Initial completeness score: {completeness_score}\")\n",
    "        else:\n",
    "            initial_scores['completeness'] = None\n",
    "            initial_explanations['completeness'] = None\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        # specificity\n",
    "        print(\"  Getting initial specificity response...\")\n",
    "        specificity_response = requests.post(BASE_URL, json=specificity_request, timeout=timeout-10)\n",
    "        if specificity_response.status_code == 200:\n",
    "            specificity_content = specificity_response.json()['message']['content']\n",
    "            specificity_score, _ = parse_score_and_explanation(specificity_content)\n",
    "            initial_scores['specificity'] = specificity_score\n",
    "            initial_explanations['specificity'] = specificity_content\n",
    "            print(f\"  Initial specificity score: {specificity_score}\")\n",
    "        else:\n",
    "            initial_scores['specificity'] = None\n",
    "            initial_explanations['specificity'] = None\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        # accuracy\n",
    "        print(\"  Getting initial accuracy response...\")\n",
    "        accuracy_response = requests.post(BASE_URL, json=accuracy_request, timeout=timeout-10)\n",
    "        if accuracy_response.status_code == 200:\n",
    "            accuracy_content = accuracy_response.json()['message']['content']\n",
    "            accuracy_score, _ = parse_score_and_explanation(accuracy_content)\n",
    "            initial_scores['accuracy'] = accuracy_score\n",
    "            initial_explanations['accuracy'] = accuracy_content\n",
    "            print(f\"  Initial accuracy score: {accuracy_score}\")\n",
    "        else:\n",
    "            initial_scores['accuracy'] = None\n",
    "            initial_explanations['accuracy'] = None\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Check if any score is <= 3 to trigger feedback loop\n",
    "        low_scores = []\n",
    "        for metric in ['coherence', 'completeness', 'specificity', 'accuracy']:\n",
    "            if initial_scores[metric] and initial_scores[metric] <= 3:\n",
    "                low_scores.append(metric)\n",
    "        \n",
    "        if low_scores:\n",
    "            print(f\"  LOW SCORES DETECTED: {low_scores} - Starting enhanced feedback loop...\")\n",
    "            feedback_count += 1\n",
    "            \n",
    "            # Keep track of all attempts for this participant\n",
    "            iteration = 0\n",
    "            max_iterations = 10  # Prevent infinite loops\n",
    "            current_assessment = qualitative_assessment\n",
    "            current_scores = initial_scores.copy()\n",
    "            current_explanations = initial_explanations.copy()\n",
    "            all_low_scores = []  # Track which scores were low across iterations\n",
    "            \n",
    "            # Feedback loop - keep trying until all scores are 4 or 5\n",
    "            while low_scores and iteration < max_iterations:\n",
    "                iteration += 1\n",
    "                print(f\"    Iteration {iteration}: Improving assessment for low scores: {low_scores}\")\n",
    "                all_low_scores.extend(low_scores)\n",
    "                \n",
    "                # Build feedback message for low scoring metrics\n",
    "                feedback_details = []\n",
    "                for metric in low_scores:\n",
    "                    if current_scores[metric] and current_explanations[metric]:\n",
    "                        metric_description = {\n",
    "                            'coherence': 'logical consistency (avoiding contradictions within the assessment)',\n",
    "                            'completeness': 'comprehensive coverage (including all relevant PHQ-8 symptoms, severities, and duration/frequency details)',\n",
    "                            'specificity': 'specific details (avoiding vague/generic statements like \"the patient seems depressed\")',\n",
    "                            'accuracy': 'accuracy according to DSM-5 or PHQ-8 criteria (correct symptoms and duration/frequency)'\n",
    "                        }\n",
    "                        feedback_details.append(f\"\"\"\n",
    "- **{metric.upper()}** scored {current_scores[metric]}/5 because: {current_explanations[metric]}\n",
    "  Please specifically improve the {metric_description[metric]} in your revised assessment.\"\"\")\n",
    "                \n",
    "                # Enhanced regeneration prompt with feedback context\n",
    "                system_prompt = \"You are a psychiatrist. Your job is to assess and diagnose patients for depression. Be as objective, concise, and accurate as possible.\"\n",
    "                \n",
    "                feedback_prompt = f\"\"\"The following is an interview transcript from a psychiatric assessment of a subject who is being screened for depression. Please note that 'Ellie' is the interviewer, and 'Participant' is the subject being assessed.\n",
    "\n",
    "TRANSCRIPT:\n",
    "{transcript}\n",
    "\n",
    "ORIGINAL ASSESSMENT:\n",
    "{qualitative_assessment}\n",
    "\n",
    "EVALUATION FEEDBACK:\n",
    "The original assessment has been evaluated and needs improvement in the following areas:\n",
    "{chr(10).join(feedback_details)}\n",
    "\n",
    "TASK:\n",
    "Please reassess the transcript and provide an improved qualitative assessment that addresses the specific feedback above. Build upon the original assessment but focus on improving the identified weaknesses.\n",
    "\n",
    "Please:\n",
    "1. Provide an overall qualitative assessment of the subject's mental health. \n",
    "2. Summarize social aspects that may influence the subject's mental health. (ex. familial relationships, friendship dynamics, work environment, etc. that are relevant to the subject's mental health)\n",
    "3. Summarize biological aspects that may influence the subject's mental health. (ex. familial history of mental health issues, previous or pre-existing mental health issues, stress levels, etc. that are relevant to the subject's mental health)\n",
    "4. Identify potential risk factors the subject may be experiencing.\n",
    "5. Use exact quotes from the transcript to support your assessment for each tag.\n",
    "\n",
    "Output should be formatted as bullet points with headings for each section using stars. Example: **Tiredness** <explanation of tiredness>. Do not include any additional text outside the bullet points.\n",
    "\n",
    "Please answer in this XML format with each tag on a new line, properly indented. Use straight quotes instead of curly quotes, and do not include any additional text outside the XML tags:\n",
    "\n",
    "<assessment>\n",
    "  <!-- Summary of participant's overall mental health -->\n",
    "  <exact_quotes>\n",
    "    <!-- Quotes from the transcript that support the assessment -->\n",
    "  </exact_quotes>\n",
    "</assessment>\n",
    "\n",
    "<social_factors>\n",
    "  <!-- Summary of social influences on patient's health -->\n",
    "  <exact_quotes>\n",
    "    <!-- Quotes from the transcript that support the assessment -->\n",
    "  </exact_quotes>\n",
    "</social_factors>\n",
    "\n",
    "<biological_factors>\n",
    "  <!-- Summary of biological influences on patient's health -->\n",
    "  <exact_quotes>\n",
    "    <!-- Quotes from the transcript that support the assessment -->\n",
    "  </exact_quotes>\n",
    "</biological_factors>\n",
    "\n",
    "<risk_factors>\n",
    "  <!-- Summary of potential risk factors -->\n",
    "  <exact_quotes>\n",
    "    <!-- Quotes from the transcript that support the assessment -->\n",
    "  </exact_quotes>\n",
    "</risk_factors>\n",
    "\"\"\"\n",
    "                \n",
    "                feedback_request = {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                               {\"role\": \"user\", \"content\": feedback_prompt}],\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "                }\n",
    "                \n",
    "                feedback_response = requests.post(BASE_URL, json=feedback_request, timeout=timeout)\n",
    "                if feedback_response.status_code != 200:\n",
    "                    print(f\"    Failed to regenerate assessment: {feedback_response.status_code}\")\n",
    "                    failed_evaluations.append(participant_id)\n",
    "                    break\n",
    "                \n",
    "                current_assessment = feedback_response.json()['message']['content']\n",
    "                print(f\"    New assessment generated with targeted improvements, re-evaluating...\")\n",
    "                \n",
    "                # Re-evaluate with new assessment\n",
    "                new_coherence_prompt = coherence_prompt.replace(qualitative_assessment, current_assessment)\n",
    "                new_completeness_prompt = completeness_prompt.replace(qualitative_assessment, current_assessment)\n",
    "                new_specificity_prompt = specificity_prompt.replace(qualitative_assessment, current_assessment)\n",
    "                new_accuracy_prompt = accuracy_prompt.replace(qualitative_assessment, current_assessment)\n",
    "                \n",
    "                # Store new scores and explanations\n",
    "                new_scores = {}\n",
    "                new_explanations = {}\n",
    "                \n",
    "                # Re-evaluate coherence\n",
    "                time.sleep(2)\n",
    "                new_coherence_request = {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": new_coherence_prompt}],\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "                }\n",
    "                new_coherence_response = requests.post(BASE_URL, json=new_coherence_request, timeout=timeout-10)\n",
    "                if new_coherence_response.status_code == 200:\n",
    "                    new_coherence_content = new_coherence_response.json()['message']['content']\n",
    "                    new_coherence_score, _ = parse_score_and_explanation(new_coherence_content)\n",
    "                    new_scores['coherence'] = new_coherence_score\n",
    "                    new_explanations['coherence'] = new_coherence_content\n",
    "                \n",
    "                # Re-evaluate completeness\n",
    "                time.sleep(2)\n",
    "                new_completeness_request = {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": new_completeness_prompt}],\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "                }\n",
    "                new_completeness_response = requests.post(BASE_URL, json=new_completeness_request, timeout=timeout-10)\n",
    "                if new_completeness_response.status_code == 200:\n",
    "                    new_completeness_content = new_completeness_response.json()['message']['content']\n",
    "                    new_completeness_score, _ = parse_score_and_explanation(new_completeness_content)\n",
    "                    new_scores['completeness'] = new_completeness_score\n",
    "                    new_explanations['completeness'] = new_completeness_content\n",
    "                \n",
    "                # Re-evaluate specificity\n",
    "                time.sleep(2)\n",
    "                new_specificity_request = {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": new_specificity_prompt}],\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "                }\n",
    "                new_specificity_response = requests.post(BASE_URL, json=new_specificity_request, timeout=timeout-10)\n",
    "                if new_specificity_response.status_code == 200:\n",
    "                    new_specificity_content = new_specificity_response.json()['message']['content']\n",
    "                    new_specificity_score, _ = parse_score_and_explanation(new_specificity_content)\n",
    "                    new_scores['specificity'] = new_specificity_score\n",
    "                    new_explanations['specificity'] = new_specificity_content\n",
    "                \n",
    "                # Re-evaluate accuracy\n",
    "                time.sleep(2)\n",
    "                new_accuracy_request = {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": new_accuracy_prompt}],\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "                }\n",
    "                new_accuracy_response = requests.post(BASE_URL, json=new_accuracy_request, timeout=timeout-10)\n",
    "                if new_accuracy_response.status_code == 200:\n",
    "                    new_accuracy_content = new_accuracy_response.json()['message']['content']\n",
    "                    new_accuracy_score, _ = parse_score_and_explanation(new_accuracy_content)\n",
    "                    new_scores['accuracy'] = new_accuracy_score\n",
    "                    new_explanations['accuracy'] = new_accuracy_content\n",
    "                \n",
    "                # Update current scores and explanations for next iteration\n",
    "                current_scores.update(new_scores)\n",
    "                current_explanations.update(new_explanations)\n",
    "                \n",
    "                # Check which scores are still low\n",
    "                low_scores = []\n",
    "                for metric in ['coherence', 'completeness', 'specificity', 'accuracy']:\n",
    "                    if metric in new_scores and new_scores[metric] and new_scores[metric] <= 3:\n",
    "                        low_scores.append(metric)\n",
    "                \n",
    "                # Print current scores\n",
    "                print(f\"    Iteration {iteration} scores: \" + \n",
    "                      f\"Coherence={current_scores.get('coherence', 'N/A')}, \" +\n",
    "                      f\"Completeness={current_scores.get('completeness', 'N/A')}, \" +\n",
    "                      f\"Specificity={current_scores.get('specificity', 'N/A')}, \" +\n",
    "                      f\"Accuracy={current_scores.get('accuracy', 'N/A')}\")\n",
    "                \n",
    "                if low_scores:\n",
    "                    print(f\"    Still have low scores: {low_scores}, continuing with targeted feedback...\")\n",
    "                else:\n",
    "                    print(f\"    All scores now 4 or 5! Enhanced feedback loop complete after {iteration} iterations.\")\n",
    "            \n",
    "            # Save final results after feedback loop completes\n",
    "            if iteration >= max_iterations:\n",
    "                print(f\"    Reached max iterations ({max_iterations}), stopping feedback loop\")\n",
    "            \n",
    "            # Save the final qualitative assessment\n",
    "            feedback_assessment_record = {\n",
    "                'participant_id': participant_id,\n",
    "                'original_qualitative_assessment': qualitative_assessment,\n",
    "                'feedback_qualitative_assessment': current_assessment,\n",
    "                'iterations_required': iteration,\n",
    "                'low_scores_detected': ', '.join(sorted(set(all_low_scores))),\n",
    "                'initial_coherence_score': initial_scores.get('coherence'),\n",
    "                'initial_completeness_score': initial_scores.get('completeness'),\n",
    "                'initial_specificity_score': initial_scores.get('specificity'),\n",
    "                'initial_accuracy_score': initial_scores.get('accuracy'),\n",
    "                'final_coherence_score': current_scores.get('coherence'),\n",
    "                'final_completeness_score': current_scores.get('completeness'),\n",
    "                'final_specificity_score': current_scores.get('specificity'),\n",
    "                'final_accuracy_score': current_scores.get('accuracy')\n",
    "            }\n",
    "            feedback_assessments.append(feedback_assessment_record)\n",
    "            \n",
    "            # Save the final evaluation scores\n",
    "            feedback_eval_record = {\n",
    "                'participant_id': participant_id,\n",
    "                'iterations_required': iteration,\n",
    "                'low_scores_detected': ', '.join(sorted(set(all_low_scores)))\n",
    "            }\n",
    "            \n",
    "            # Add final scores and explanations to record\n",
    "            for metric in ['coherence', 'completeness', 'specificity', 'accuracy']:\n",
    "                if metric in current_scores:\n",
    "                    feedback_eval_record[f'final_{metric}_score'] = current_scores[metric]\n",
    "                    feedback_eval_record[f'final_{metric}_explanation'] = current_explanations[metric]\n",
    "                if metric in initial_scores:\n",
    "                    feedback_eval_record[f'initial_{metric}_score'] = initial_scores[metric]\n",
    "                    feedback_eval_record[f'initial_{metric}_explanation'] = initial_explanations[metric]\n",
    "            \n",
    "            feedback_evaluations.append(feedback_eval_record)\n",
    "            processed_count += 1\n",
    "        else:\n",
    "            print(f\"  No low scores detected - skipping feedback loop\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Completed participant {participant_id} in {elapsed_time:.1f}s ({processed_count} with feedback applied)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing participant {participant_id}: {e}\")\n",
    "        failed_evaluations.append(participant_id)\n",
    "    \n",
    "    # Save progress every 10 participants\n",
    "    if (len(feedback_assessments) % 10 == 0 and len(feedback_assessments) > 0) or len(feedback_assessments) == 1:\n",
    "        # Save feedback assessments\n",
    "        if feedback_assessments:\n",
    "            feedback_assessments_df = pd.DataFrame(feedback_assessments)\n",
    "            feedback_assessments_df.to_csv(feedback_assessments_csv, index=False)\n",
    "            print(f\"Saved feedback assessments: {len(feedback_assessments)} records to {feedback_assessments_csv}\")\n",
    "        \n",
    "        # Save feedback evaluations\n",
    "        if feedback_evaluations:\n",
    "            feedback_evaluations_df = pd.DataFrame(feedback_evaluations)\n",
    "            feedback_evaluations_df.to_csv(feedback_evaluations_csv, index=False)\n",
    "            print(f\"Saved feedback evaluations: {len(feedback_evaluations)} records to {feedback_evaluations_csv}\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n=== PROCESSING SUMMARY ===\")\n",
    "print(f\"Total subjects processed: {len(df)}\")\n",
    "print(f\"Skipped (no transcript): {skipped_count}\")\n",
    "print(f\"Feedback loops applied: {feedback_count}\")\n",
    "print(f\"Successfully processed with feedback: {processed_count}\")\n",
    "print(f\"Failed: {len(failed_evaluations)}\")\n",
    "print(f\"Feedback assessments created: {len(feedback_assessments)}\")\n",
    "print(f\"Feedback evaluations created: {len(feedback_evaluations)}\")\n",
    "\n",
    "if failed_evaluations:\n",
    "    print(f\"Failed participant IDs: {failed_evaluations}\")\n",
    "\n",
    "# Save final feedback files\n",
    "if feedback_assessments:\n",
    "    feedback_assessments_df = pd.DataFrame(feedback_assessments)\n",
    "    feedback_assessments_df.to_csv(feedback_assessments_csv, index=False)\n",
    "    print(f\"Final feedback assessments saved: {feedback_assessments_csv}\")\n",
    "    print(f\"Enhanced feedback assessments CSV columns:\")\n",
    "    print(f\"- participant_id\")\n",
    "    print(f\"- original_qualitative_assessment\")\n",
    "    print(f\"- feedback_qualitative_assessment\")\n",
    "    print(f\"- iterations_required\")\n",
    "    print(f\"- low_scores_detected\")\n",
    "    print(f\"- initial_[metric]_score and final_[metric]_score for comparison\")\n",
    "\n",
    "if feedback_evaluations:\n",
    "    feedback_evaluations_df = pd.DataFrame(feedback_evaluations)\n",
    "    feedback_evaluations_df.to_csv(feedback_evaluations_csv, index=False)\n",
    "    print(f\"Final feedback evaluations saved: {feedback_evaluations_csv}\")\n",
    "    print(f\"Enhanced feedback evaluations CSV columns:\")\n",
    "    print(f\"- participant_id\")\n",
    "    print(f\"- iterations_required\")\n",
    "    print(f\"- initial_[metric]_score / final_[metric]_score\")\n",
    "    print(f\"- initial_[metric]_explanation / final_[metric]_explanation\")\n",
    "    print(f\"- low_scores_detected\")\n",
    "\n",
    "if not feedback_assessments and not feedback_evaluations:\n",
    "    print(\"No participants required feedback - no CSV files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba97652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:24:52] Testing API connection...\n",
      "✓ API connection successful\n",
      "=== FEEDBACK LOOP RE-EVALUATION (PERSISTENT) ===\n",
      "Processing 89 failed IDs\n",
      "Will continue until all scores reach 4 or 5\n",
      "[14:24:52] Loading data...\n",
      "Found 89 participants to process\n",
      "Skipping 3 already completed. Remaining: 86\n",
      "\n",
      "[14:24:53] Processing 386 (1/86)\n",
      "  Loading transcript... ✓ Loaded (17714 chars)\n",
      "  Initial evaluation:\n",
      "    [14:24:53] Starting coherence evaluation... ✓ Score: 5 (61.2s)\n",
      "    [14:25:56] Starting completeness evaluation... ✓ Score: 1 (29.9s)\n",
      "    [14:26:28] Starting specificity evaluation... ✓ Score: 2 (30.1s)\n",
      "    [14:27:00] Starting accuracy evaluation... ✓ Score: 1 (34.7s)\n",
      "  Scores below 4 detected: ['completeness', 'specificity', 'accuracy']\n",
      "  Starting persistent feedback loop...\n",
      "    Feedback iteration 1:\n",
      "    [14:27:36] Starting improvement-1 evaluation... ✓ Score: 4 (137.2s)\n",
      "    Re-evaluating improved assessment:\n",
      "    [14:29:54] Starting completeness-reeval-1 evaluation... ✓ Score: 1 (30.4s)\n",
      "    [14:30:26] Starting specificity-reeval-1 evaluation... ✓ Score: 4 (23.2s)\n",
      "    [14:30:51] Starting accuracy-reeval-1 evaluation... ✓ Score: 1 (31.1s)\n",
      "    Scores still below 4: [('completeness', 1), ('accuracy', 1)]\n",
      "    Feedback iteration 2:\n",
      "    [14:31:24] Starting improvement-1 evaluation..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 268\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m improved_assessment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m improvement_attempts < \u001b[32m10\u001b[39m:\n\u001b[32m    267\u001b[39m     improvement_attempts += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     score, improved_assessment = \u001b[43mmake_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeedback_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimprovement-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimprovement_attempts\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparticipant_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m improved_assessment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    270\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    Retrying improvement generation (attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimprovement_attempts\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mmake_api_call\u001b[39m\u001b[34m(prompt, metric_name, participant_id, max_retries)\u001b[39m\n\u001b[32m     45\u001b[39m start_time = time.time()\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Increased timeout to 300 seconds (5 minutes)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m elapsed = time.time() - start_time\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/http/client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1394\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1397\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    288\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/users2/nblair7/envs/aipsy/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_score_and_explanation(response_text):\n",
    "    \"\"\"Extract score and explanation from model response\"\"\"\n",
    "    score_patterns = [\n",
    "        r'score[:\\s]*(\\d+)',\n",
    "        r'(\\d+)[/\\s]*(?:out of\\s*)?5',\n",
    "        r'(\\d+)[/\\s]*5',\n",
    "        r'rating[:\\s]*(\\d+)',\n",
    "        r'^(\\d+)',\n",
    "    ]\n",
    "    \n",
    "    score = None\n",
    "    for pattern in score_patterns:\n",
    "        match = re.search(pattern, response_text, re.IGNORECASE | re.MULTILINE)\n",
    "        if match:\n",
    "            potential_score = int(match.group(1))\n",
    "            if 1 <= potential_score <= 5:\n",
    "                score = potential_score\n",
    "                break\n",
    "    \n",
    "    return score, response_text.strip()\n",
    "\n",
    "def make_api_call(prompt, metric_name, participant_id, max_retries=10):\n",
    "    \"\"\"Make API call with unlimited retries and extended timeout\"\"\"\n",
    "    print(f\"    [{datetime.now().strftime('%H:%M:%S')}] Starting {metric_name} evaluation...\", end=\"\", flush=True)\n",
    "    \n",
    "    retry_count = 0\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            request = {\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"stream\": False,\n",
    "                \"options\": {\"temperature\": 0, \"top_k\": 20, \"top_p\": 0.9}\n",
    "            }\n",
    "            \n",
    "            start_time = time.time()\n",
    "            # Increased timeout to 300 seconds (5 minutes)\n",
    "            response = requests.post(BASE_URL, json=request, timeout=300)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                content = response.json()['message']['content']\n",
    "                score, explanation = parse_score_and_explanation(content)\n",
    "                print(f\" ✓ Score: {score} ({elapsed:.1f}s)\")\n",
    "                return score, explanation\n",
    "            else:\n",
    "                print(f\" ⚠ HTTP {response.status_code}, retrying... (attempt {retry_count + 1})\")\n",
    "                \n",
    "        except Timeout:\n",
    "            retry_count += 1\n",
    "            print(f\" ⚠ TIMEOUT, retrying... (attempt {retry_count})\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "            continue\n",
    "        except RequestException as e:\n",
    "            retry_count += 1\n",
    "            print(f\" ⚠ REQUEST ERROR, retrying... (attempt {retry_count})\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            retry_count += 1\n",
    "            print(f\" ⚠ ERROR, retrying... (attempt {retry_count})\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        \n",
    "        retry_count += 1\n",
    "        time.sleep(2)  # Brief pause between retries\n",
    "    \n",
    "    # If all retries failed\n",
    "    print(f\" ✗ Failed after {max_retries} attempts\")\n",
    "    return None, f\"Failed after {max_retries} attempts\"\n",
    "\n",
    "# Configuration\n",
    "OLLAMA_NODE = \"arctrddgxa004\"\n",
    "BASE_URL = f\"http://{OLLAMA_NODE}:11434/api/chat\"\n",
    "model = \"gemma3-optimized:27b\"\n",
    "\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Testing API connection...\")\n",
    "try:\n",
    "    test_response = requests.get(f\"http://{OLLAMA_NODE}:11434/api/tags\", timeout=30)\n",
    "    if test_response.status_code == 200:\n",
    "        print(\"✓ API connection successful\")\n",
    "    else:\n",
    "        print(f\"⚠ API returned status {test_response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ API connection failed: {e}\")\n",
    "    print(\"Script may hang on API calls!\")\n",
    "\n",
    "# FAILED IDs TO PROCESS\n",
    "FAILED_IDS = [\n",
    "    380, 383, 385, 386, 391, 392, 393, 397, 400, 401, 402, 409, 412, 414, 415, 416, 419, 423, 425, 426, 427, 428, 429, 430, 433, 434, 437, 441, 443, 444, 445, 446, 447, 448, 449, 454, 455, 456, 457, 459, 463, 464, 468, 471, 473, 474, 475, 478, 479, 485, 486, 487, 488, 491, 302, 307, 331, 335, 346, 367, 377, 381, 382, 388, 389, 390, 395, 403, 404, 406, 413, 417, 418, 420, 422, 436, 439, 440, 451, 458, 472, 476, 477, 482, 483, 484, 489, 490, 492\n",
    "]\n",
    "\n",
    "# Input/Output files\n",
    "input_csv_path = \"/data/users2/nblair7/analysis_results/qual_resultsfin.csv\"\n",
    "feedback_assessments_csv = \"/data/users2/nblair7/analysis_results/ASSESSMENT.csv\"\n",
    "feedback_evaluations_csv = \"/data/users2/nblair7/analysis_results/SCORES.csv\"\n",
    "\n",
    "print(\"=== FEEDBACK LOOP RE-EVALUATION (PERSISTENT) ===\")\n",
    "print(f\"Processing {len(FAILED_IDS)} failed IDs\")\n",
    "print(\"Will continue until all scores reach 4 or 5\")\n",
    "\n",
    "# Load data\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loading data...\")\n",
    "df = pd.read_csv(input_csv_path)\n",
    "df = df[df['participant_id'].astype(str).isin([str(pid) for pid in FAILED_IDS])]\n",
    "print(f\"Found {len(df)} participants to process\")\n",
    "\n",
    "# Check existing files\n",
    "completed_subjects = set()\n",
    "if os.path.exists(feedback_assessments_csv):\n",
    "    existing = pd.read_csv(feedback_assessments_csv)\n",
    "    completed_subjects.update(existing['participant_id'].tolist())\n",
    "\n",
    "if os.path.exists(feedback_evaluations_csv):\n",
    "    existing = pd.read_csv(feedback_evaluations_csv)\n",
    "    completed_subjects.update(existing['participant_id'].tolist())\n",
    "\n",
    "if completed_subjects:\n",
    "    df = df[~df['participant_id'].isin(completed_subjects)]\n",
    "    print(f\"Skipping {len(completed_subjects)} already completed. Remaining: {len(df)}\")\n",
    "\n",
    "assessments = []\n",
    "scores = []\n",
    "processed = 0\n",
    "failed = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    participant_id = row['participant_id']\n",
    "    qualitative_assessment = row['qualitative_assessment']\n",
    "    \n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Processing {participant_id} ({processed + 1}/{len(df)})\")\n",
    "    \n",
    "    # Load transcript\n",
    "    print(\"  Loading transcript...\", end=\"\", flush=True)\n",
    "    transcript_path = os.path.join(\"/data/users4/xli/ai-psychiatrist/datasets/daic_woz_dataset/\", f\"{participant_id}_P\", f\"{participant_id}_TRANSCRIPT.csv\")\n",
    "    \n",
    "    if not os.path.exists(transcript_path):\n",
    "        print(f\" ✗ No transcript found\")\n",
    "        failed.append(participant_id)\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        transcript_data = pd.read_csv(transcript_path, sep='\\t')\n",
    "        dialogue_txt = transcript_data.dropna(subset=['speaker', 'value'])\n",
    "        dialogue_txt['dialogue'] = dialogue_txt['speaker'] + \" : \" + dialogue_txt['value']\n",
    "        transcript = \"\\n\".join(dialogue_txt['dialogue'])\n",
    "        print(f\" ✓ Loaded ({len(transcript)} chars)\")\n",
    "    except Exception as e:\n",
    "        print(f\" ✗ Error: {e}\")\n",
    "        failed.append(participant_id)\n",
    "        continue\n",
    "    \n",
    "    # Evaluation prompts\n",
    "    prompts = {\n",
    "        'coherence': f\"\"\"Evaluate for COHERENCE (1-5): Is the response logically consistent? \n",
    "Score: 5=0 mistakes, 4=1-2 mistakes, 3=3-4 mistakes, 2=5-6 mistakes, 1=7+ mistakes\n",
    "Mistakes = contradictions within assessment.\n",
    "\n",
    "Format: Score: [number]\\nExplanation: [brief explanation]\n",
    "\n",
    "Transcript: {transcript[:2000]}...\n",
    "Assessment: {qualitative_assessment[:1000]}...\"\"\",\n",
    "\n",
    "        'completeness': f\"\"\"Evaluate for COMPLETENESS (1-5): Does assessment cover all relevant symptoms, severities, duration/frequency?\n",
    "Score: 5=0 mistakes, 4=1-2 mistakes, 3=3-4 mistakes, 2=5-6 mistakes, 1=7+ mistakes\n",
    "Mistakes = missed PHQ-8 symptoms or duration/frequency details.\n",
    "\n",
    "Format: Score: [number]\\nExplanation: [brief explanation]\n",
    "\n",
    "Transcript: {transcript[:2000]}...\n",
    "Assessment: {qualitative_assessment[:1000]}...\"\"\",\n",
    "\n",
    "        'specificity': f\"\"\"Evaluate for SPECIFICITY (1-5): Is the assessment specific vs vague?\n",
    "Score: 5=0 mistakes, 4=1-2 mistakes, 3=3-4 mistakes, 2=5-6 mistakes, 1=7+ mistakes\n",
    "Mistakes = vague statements like \"patient seems depressed\".\n",
    "\n",
    "Format: Score: [number]\\nExplanation: [brief explanation]\n",
    "\n",
    "Transcript: {transcript[:2000]}...\n",
    "Assessment: {qualitative_assessment[:1000]}...\"\"\",\n",
    "\n",
    "        'accuracy': f\"\"\"Evaluate for ACCURACY (1-5): Signs/symptoms aligned with DSM-5/PHQ-8?\n",
    "Score: 5=0 mistakes, 4=1-2 mistakes, 3=3-4 mistakes, 2=5-6 mistakes, 1=7+ mistakes\n",
    "Mistakes = incorrect symptoms or duration/frequency.\n",
    "\n",
    "Format: Score: [number]\\nExplanation: [brief explanation]\n",
    "\n",
    "Transcript: {transcript[:2000]}...\n",
    "Assessment: {qualitative_assessment[:1000]}...\"\"\"\n",
    "    }\n",
    "    \n",
    "    # Get initial scores\n",
    "    print(\"  Initial evaluation:\")\n",
    "    current_scores = {}\n",
    "    current_explanations = {}\n",
    "    \n",
    "    for metric, prompt in prompts.items():\n",
    "        score, explanation = make_api_call(prompt, metric, participant_id)\n",
    "        if score is not None:\n",
    "            current_scores[metric] = score\n",
    "            current_explanations[metric] = explanation[:200] + \"...\" if len(explanation) > 200 else explanation\n",
    "        else:\n",
    "            # If API call completely fails, mark as failed participant\n",
    "            print(f\"  ✗ Failed to get initial score for {metric}\")\n",
    "            failed.append(participant_id)\n",
    "            break\n",
    "        \n",
    "        time.sleep(2)  # Brief pause between API calls\n",
    "    \n",
    "    # Skip if we couldn't get initial scores\n",
    "    if participant_id in failed:\n",
    "        continue\n",
    "    \n",
    "    # Check if feedback needed - continue until ALL scores are 4 or 5\n",
    "    low_scores = [metric for metric, score in current_scores.items() if score and score < 4]\n",
    "    \n",
    "    current_assessment = qualitative_assessment\n",
    "    \n",
    "    if low_scores:\n",
    "        print(f\"  Scores below 4 detected: {low_scores}\")\n",
    "        print(\"  Starting persistent feedback loop...\")\n",
    "        iteration = 0\n",
    "        max_iterations = 50  # Increased limit, but will continue until target reached\n",
    "        \n",
    "        while low_scores and iteration < max_iterations:\n",
    "            iteration += 1\n",
    "            print(f\"    Feedback iteration {iteration}:\")\n",
    "            \n",
    "            # Build feedback prompt\n",
    "            feedback_details = []\n",
    "            for metric in low_scores:\n",
    "                if metric in current_scores and metric in current_explanations:\n",
    "                    feedback_details.append(f\"- {metric.upper()}: scored {current_scores[metric]}/5. Must reach 4 or 5.\")\n",
    "            \n",
    "            feedback_prompt = f\"\"\"Improve this psychiatric assessment based on feedback:\n",
    "\n",
    "TRANSCRIPT: {transcript[:1500]}...\n",
    "\n",
    "CURRENT ASSESSMENT: {current_assessment[:800]}...\n",
    "\n",
    "FEEDBACK: The assessment MUST be improved in these areas to reach score 4 or 5:\n",
    "{chr(10).join(feedback_details)}\n",
    "\n",
    "Requirements for each score:\n",
    "- Score 4: Maximum 1-2 mistakes/issues\n",
    "- Score 5: No mistakes/issues\n",
    "\n",
    "Provide significantly improved assessment in XML format:\n",
    "<assessment><!-- detailed, comprehensive improved assessment --></assessment>\n",
    "<social_factors><!-- social influences --></social_factors> \n",
    "<biological_factors><!-- biological influences --></biological_factors>\n",
    "<risk_factors><!-- risk factors --></risk_factors>\"\"\"\n",
    "            \n",
    "            # Generate improved assessment - keep trying until successful\n",
    "            improved_assessment = None\n",
    "            improvement_attempts = 0\n",
    "            while improved_assessment is None and improvement_attempts < 10:\n",
    "                improvement_attempts += 1\n",
    "                score, improved_assessment = make_api_call(feedback_prompt, f\"improvement-{improvement_attempts}\", participant_id)\n",
    "                if improved_assessment is None:\n",
    "                    print(f\"    Retrying improvement generation (attempt {improvement_attempts})...\")\n",
    "                    time.sleep(5)\n",
    "            \n",
    "            if improved_assessment is None:\n",
    "                print(f\"    ✗ Could not generate improvement after {improvement_attempts} attempts\")\n",
    "                break\n",
    "            \n",
    "            current_assessment = improved_assessment\n",
    "            \n",
    "            # Re-evaluate with new assessment - only the low scoring metrics\n",
    "            print(f\"    Re-evaluating improved assessment:\")\n",
    "            for metric, base_prompt in prompts.items():\n",
    "                if metric in low_scores:\n",
    "                    new_prompt = base_prompt.replace(qualitative_assessment[:1000], current_assessment[:1000])\n",
    "                    \n",
    "                    # Keep trying until we get a score\n",
    "                    eval_attempts = 0\n",
    "                    score = None\n",
    "                    while score is None and eval_attempts < 5:\n",
    "                        eval_attempts += 1\n",
    "                        score, explanation = make_api_call(new_prompt, f\"{metric}-reeval-{eval_attempts}\", participant_id)\n",
    "                        if score is None:\n",
    "                            time.sleep(3)\n",
    "                    \n",
    "                    if score is not None:\n",
    "                        current_scores[metric] = score\n",
    "                        current_explanations[metric] = explanation[:200] + \"...\" if len(explanation) > 200 else explanation\n",
    "                    else:\n",
    "                        print(f\"    ⚠ Could not re-evaluate {metric} after {eval_attempts} attempts\")\n",
    "                    \n",
    "                    time.sleep(2)\n",
    "            \n",
    "            # Check remaining low scores - must be 4 or 5\n",
    "            low_scores = [metric for metric, score in current_scores.items() if score and score < 4]\n",
    "            \n",
    "            if not low_scores:\n",
    "                print(f\"    ✓ All scores reached 4 or 5!\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"    Scores still below 4: {[(m, current_scores[m]) for m in low_scores]}\")\n",
    "        \n",
    "        if iteration >= max_iterations:\n",
    "            print(f\"  ⚠ Reached maximum iterations ({max_iterations}) without achieving target scores\")\n",
    "        else:\n",
    "            print(f\"  ✓ Feedback completed successfully after {iteration} iterations\")\n",
    "    else:\n",
    "        print(\"  ✓ All scores already 4 or 5, no feedback needed\")\n",
    "    \n",
    "    # Store results\n",
    "    assessments.append({\n",
    "        'participant_id': participant_id,\n",
    "        'dataset_type': 'feedback_improved',\n",
    "        'qualitative_assessment': current_assessment\n",
    "    })\n",
    "    \n",
    "    scores.append({\n",
    "        'participant_id': participant_id,\n",
    "        'coherence': current_scores.get('coherence'),\n",
    "        'coherence_explanation': current_explanations.get('coherence', ''),\n",
    "        'completeness': current_scores.get('completeness'),\n",
    "        'completeness_explanation': current_explanations.get('completeness', ''),\n",
    "        'specificity': current_scores.get('specificity'),\n",
    "        'specificity_explanation': current_explanations.get('specificity', ''),\n",
    "        'accuracy': current_scores.get('accuracy'),\n",
    "        'accuracy_explanation': current_explanations.get('accuracy', '')\n",
    "    })\n",
    "    \n",
    "    processed += 1\n",
    "    \n",
    "    # Save progress every participant (more frequent saves for long-running process)\n",
    "    try:\n",
    "        pd.DataFrame(assessments).to_csv(feedback_assessments_csv, index=False)\n",
    "        pd.DataFrame(scores).to_csv(feedback_evaluations_csv, index=False)\n",
    "        print(f\"  ✓ Progress saved: {processed} completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ Save failed: {e}\")\n",
    "\n",
    "# Final save\n",
    "try:\n",
    "    if assessments:\n",
    "        pd.DataFrame(assessments).to_csv(feedback_assessments_csv, index=False)\n",
    "        print(f\"✓ Final assessments saved: {len(assessments)} records\")\n",
    "\n",
    "    if scores:\n",
    "        pd.DataFrame(scores).to_csv(feedback_evaluations_csv, index=False)\n",
    "        print(f\"✓ Final scores saved: {len(scores)} records\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Final save failed: {e}\")\n",
    "\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "print(f\"Completed: {processed} participants\")\n",
    "print(f\"Failed: {len(failed)} participants\")\n",
    "if failed:\n",
    "    print(f\"Failed IDs: {failed}\")\n",
    "print(f\"Finished at: {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipsy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
