{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc6f95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV File Merger and Sorter\n",
      "========================================\n",
      "Input files: ['/data/users2/nblair7/analysis_results/feedback_evaluation_scores1.csv', '/data/users2/nblair7/analysis_results/SCOREST.csv', '/data/users2/nblair7/analysis_results/SCORES1.csv', '/data/users2/nblair7/analysis_results/SCORES.csv']\n",
      "Output file: /data/users2/nblair7/analysis_results/merged_sorted_data.csv\n",
      "Sort column: participant_id\n",
      "Duplicate handling: keep_last\n",
      "========================================\n",
      "File 1: Loaded 20 rows from /data/users2/nblair7/analysis_results/feedback_evaluation_scores1.csv\n",
      "File 2: Loaded 10 rows from /data/users2/nblair7/analysis_results/SCOREST.csv\n",
      "File 3: Loaded 6 rows from /data/users2/nblair7/analysis_results/SCORES1.csv\n",
      "File 4: Loaded 27 rows from /data/users2/nblair7/analysis_results/SCORES.csv\n",
      "\n",
      "Merged data: 63 total rows\n",
      "\n",
      "Warning: Found 22 rows with duplicate participant_id values\n",
      "Duplicate participant_id values: [np.int64(380), np.int64(391), np.int64(393), np.int64(397), np.int64(402), np.int64(414), np.int64(415), np.int64(419), np.int64(425), np.int64(433)]\n",
      "Kept last occurrence of each duplicate. Removed 12 rows.\n",
      "Data sorted by participant_id\n",
      "Saved merged and sorted data to: /data/users2/nblair7/analysis_results/merged_sorted_data.csv\n",
      "\n",
      "Preview of merged data (first 5 rows):\n",
      "========================================\n",
      "    participant_id  iterations_required                  low_scores_detected  \\\n",
      "0              303                  8.0  accuracy, completeness, specificity   \n",
      "1              304                  1.0                             accuracy   \n",
      "52             307                  NaN                                  NaN   \n",
      "2              310                 10.0  accuracy, completeness, specificity   \n",
      "3              312                  1.0                         completeness   \n",
      "\n",
      "    final_coherence_score                        final_coherence_explanation  \\\n",
      "0                     5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "1                     5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "52                    NaN                                                NaN   \n",
      "2                     5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "3                     5.0  Score: 5\\nExplanation: The assessment is inter...   \n",
      "\n",
      "    initial_coherence_score  \\\n",
      "0                       5.0   \n",
      "1                       5.0   \n",
      "52                      NaN   \n",
      "2                       5.0   \n",
      "3                       5.0   \n",
      "\n",
      "                        initial_coherence_explanation  \\\n",
      "0   Score: 5\\nExplanation: The assessment is remar...   \n",
      "1   Score: 5\\nExplanation: The assessment is remar...   \n",
      "52                                                NaN   \n",
      "2   Score: 5\\nExplanation: The assessment is remar...   \n",
      "3   Score: 5\\nExplanation: The assessment is remar...   \n",
      "\n",
      "    final_completeness_score  \\\n",
      "0                        4.0   \n",
      "1                        5.0   \n",
      "52                       NaN   \n",
      "2                        3.0   \n",
      "3                        4.0   \n",
      "\n",
      "                       final_completeness_explanation  \\\n",
      "0   Score: 4\\nExplanation: The assessment is gener...   \n",
      "1   Score: 5\\nExplanation: The assessment comprehe...   \n",
      "52                                                NaN   \n",
      "2   Score: 3\\nExplanation: The assessment is a goo...   \n",
      "3   Score: 4\\nExplanation: The assessment is quite...   \n",
      "\n",
      "    initial_completeness_score  ... initial_accuracy_score  \\\n",
      "0                          3.0  ...                    5.0   \n",
      "1                          4.0  ...                    3.0   \n",
      "52                         NaN  ...                    NaN   \n",
      "2                          3.0  ...                    3.0   \n",
      "3                          3.0  ...                    4.0   \n",
      "\n",
      "                         initial_accuracy_explanation coherence  \\\n",
      "0   Score: 5\\nExplanation: The assessment accurate...       NaN   \n",
      "1   Score: 3\\nExplanation: The assessment accurate...       NaN   \n",
      "52                                                NaN       5.0   \n",
      "2   Score: 3\\nExplanation: The assessment accurate...       NaN   \n",
      "3   Score: 4\\nExplanation: The assessment accurate...       NaN   \n",
      "\n",
      "                                coherence_explanation completeness  \\\n",
      "0                                                 NaN          NaN   \n",
      "1                                                 NaN          NaN   \n",
      "52  Score: 5\\nExplanation: The assessment is remar...          2.0   \n",
      "2                                                 NaN          NaN   \n",
      "3                                                 NaN          NaN   \n",
      "\n",
      "                             completeness_explanation specificity  \\\n",
      "0                                                 NaN         NaN   \n",
      "1                                                 NaN         NaN   \n",
      "52  Score: 2\\nExplanation: The assessment is a det...         2.0   \n",
      "2                                                 NaN         NaN   \n",
      "3                                                 NaN         NaN   \n",
      "\n",
      "                              specificity_explanation accuracy  \\\n",
      "0                                                 NaN      NaN   \n",
      "1                                                 NaN      NaN   \n",
      "52  Score: 2\\nExplanation: The assessment, while d...      4.0   \n",
      "2                                                 NaN      NaN   \n",
      "3                                                 NaN      NaN   \n",
      "\n",
      "                                 accuracy_explanation  \n",
      "0                                                 NaN  \n",
      "1                                                 NaN  \n",
      "52  Score: 4\\nExplanation: The assessment accurate...  \n",
      "2                                                 NaN  \n",
      "3                                                 NaN  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "\n",
      "Total rows: 51\n",
      "Columns: ['participant_id', 'iterations_required', 'low_scores_detected', 'final_coherence_score', 'final_coherence_explanation', 'initial_coherence_score', 'initial_coherence_explanation', 'final_completeness_score', 'final_completeness_explanation', 'initial_completeness_score', 'initial_completeness_explanation', 'final_specificity_score', 'final_specificity_explanation', 'initial_specificity_score', 'initial_specificity_explanation', 'final_accuracy_score', 'final_accuracy_explanation', 'initial_accuracy_score', 'initial_accuracy_explanation', 'coherence', 'coherence_explanation', 'completeness', 'completeness_explanation', 'specificity', 'specificity_explanation', 'accuracy', 'accuracy_explanation']\n",
      "participant_id range: 303 to 491\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def merge_and_sort_csvs(file_paths, output_filename=None, sort_column=None, duplicate_handling=None):\n",
    "    \"\"\"\n",
    "    Merge multiple CSV files and sort by participant_id in ascending order.\n",
    "    \n",
    "    Parameters:\n",
    "    file_paths (list): List of file paths to CSV files\n",
    "    output_filename (str): Name for the output file\n",
    "    sort_column (str): Column name to sort by\n",
    "    duplicate_handling (str): How to handle duplicates\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The merged and sorted dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use global defaults if not provided\n",
    "    if output_filename is None:\n",
    "        output_filename = OUTPUT_FILE\n",
    "    if sort_column is None:\n",
    "        sort_column = SORT_COLUMN\n",
    "    if duplicate_handling is None:\n",
    "        duplicate_handling = DUPLICATE_HANDLING\n",
    "    \n",
    "    # List to store dataframes\n",
    "    dataframes = []\n",
    "    \n",
    "    # Read each CSV file\n",
    "    for i, file_path in enumerate(file_paths, 1):\n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Error: File {file_path} not found\")\n",
    "                continue\n",
    "                \n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"File {i}: Loaded {len(df)} rows from {file_path}\")\n",
    "            \n",
    "            # Strip whitespace from headers\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            dataframes.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not dataframes:\n",
    "        print(\"No valid CSV files were loaded\")\n",
    "        return None\n",
    "    \n",
    "    # Merge all dataframes\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"\\nMerged data: {len(merged_df)} total rows\")\n",
    "    \n",
    "    # Check for duplicates before processing\n",
    "    if sort_column in merged_df.columns:\n",
    "        duplicates = merged_df[merged_df.duplicated(subset=[sort_column], keep=False)]\n",
    "        if len(duplicates) > 0:\n",
    "            unique_duplicate_ids = duplicates[sort_column].unique()\n",
    "            print(f\"\\nWarning: Found {len(duplicates)} rows with duplicate {sort_column} values\")\n",
    "            print(f\"Duplicate {sort_column} values: {list(unique_duplicate_ids)}\")\n",
    "            \n",
    "            # Handle duplicates based on configuration\n",
    "            if duplicate_handling == 'keep_first':\n",
    "                original_len = len(merged_df)\n",
    "                merged_df = merged_df.drop_duplicates(subset=[sort_column], keep='first')\n",
    "                print(f\"Kept first occurrence of each duplicate. Removed {original_len - len(merged_df)} rows.\")\n",
    "            elif duplicate_handling == 'keep_last':\n",
    "                original_len = len(merged_df)\n",
    "                merged_df = merged_df.drop_duplicates(subset=[sort_column], keep='last')\n",
    "                print(f\"Kept last occurrence of each duplicate. Removed {original_len - len(merged_df)} rows.\")\n",
    "            elif duplicate_handling == 'keep_all':\n",
    "                print(\"Keeping all duplicate rows as configured.\")\n",
    "            else:\n",
    "                print(f\"Unknown duplicate handling option: {duplicate_handling}. Keeping all rows.\")\n",
    "        else:\n",
    "            print(f\"No duplicate {sort_column} values found.\")\n",
    "    \n",
    "    # Check if sort column exists\n",
    "    if sort_column not in merged_df.columns:\n",
    "        print(f\"Warning: '{sort_column}' column not found in the data\")\n",
    "        print(f\"Available columns: {list(merged_df.columns)}\")\n",
    "        return merged_df\n",
    "    \n",
    "    # Sort by sort_column in ascending order\n",
    "    # Handle both numeric and string participant IDs\n",
    "    try:\n",
    "        # Try to convert to numeric first\n",
    "        merged_df[f'{sort_column}_numeric'] = pd.to_numeric(merged_df[sort_column], errors='coerce')\n",
    "        \n",
    "        # If we have numeric values, sort by those, otherwise sort as strings\n",
    "        if not merged_df[f'{sort_column}_numeric'].isna().all():\n",
    "            merged_df = merged_df.sort_values(f'{sort_column}_numeric', na_position='last')\n",
    "        else:\n",
    "            merged_df = merged_df.sort_values(sort_column, key=lambda x: x.astype(str))\n",
    "        \n",
    "        # Drop the temporary numeric column\n",
    "        if f'{sort_column}_numeric' in merged_df.columns:\n",
    "            merged_df = merged_df.drop(f'{sort_column}_numeric', axis=1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error sorting by {sort_column}: {str(e)}\")\n",
    "        print(\"Sorting as strings instead...\")\n",
    "        merged_df = merged_df.sort_values(sort_column, key=lambda x: x.astype(str))\n",
    "    \n",
    "    print(f\"Data sorted by {sort_column}\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    merged_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Saved merged and sorted data to: {output_filename}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# =====================================================\n",
    "# CONFIGURATION VARIABLES - EDIT THESE\n",
    "# =====================================================\n",
    "\n",
    "# File paths for your CSV files\n",
    "FILE1 = \"/data/users2/nblair7/analysis_results/feedback_evaluation_scores1.csv\"\n",
    "FILE2 = \"/data/users2/nblair7/analysis_results/SCOREST.csv\" \n",
    "FILE3 = \"/data/users2/nblair7/analysis_results/SCORES1.csv\"\n",
    "FILE4 = \"/data/users2/nblair7/analysis_results/SCORES.csv\"\n",
    "\n",
    "# Output filename for merged data\n",
    "OUTPUT_FILE = \"/data/users2/nblair7/analysis_results/merged_sorted_data.csv\"\n",
    "\n",
    "# Sort column (should be 'participant_id' based on your headers)\n",
    "SORT_COLUMN = \"participant_id\"\n",
    "\n",
    "# Handle duplicate participant_ids\n",
    "# Options: 'keep_all', 'keep_first', 'keep_last'\n",
    "DUPLICATE_HANDLING = \"keep_last\"\n",
    "\n",
    "# =====================================================\n",
    "# END CONFIGURATION - Don't edit below unless needed\n",
    "# =====================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the configuration variables\n",
    "    file_paths = [FILE1, FILE2, FILE3, FILE4]\n",
    "    \n",
    "    print(\"CSV File Merger and Sorter\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Input files: {file_paths}\")\n",
    "    print(f\"Output file: {OUTPUT_FILE}\")\n",
    "    print(f\"Sort column: {SORT_COLUMN}\")\n",
    "    print(f\"Duplicate handling: {DUPLICATE_HANDLING}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Check if files exist\n",
    "    existing_files = [f for f in file_paths if os.path.exists(f)]\n",
    "    \n",
    "    if len(existing_files) == 0:\n",
    "        print(\"No CSV files found. Please check your file paths:\")\n",
    "        for f in file_paths:\n",
    "            print(f\"  - {f} {'✓' if os.path.exists(f) else '✗ (not found)'}\")\n",
    "        return\n",
    "    \n",
    "    if len(existing_files) < len(file_paths):\n",
    "        print(\"Some files not found:\")\n",
    "        for f in file_paths:\n",
    "            print(f\"  - {f} {'✓' if os.path.exists(f) else '✗ (not found)'}\")\n",
    "        print(f\"\\nProceeding with {len(existing_files)} available files...\")\n",
    "    \n",
    "    # Merge and sort the files\n",
    "    result_df = merge_and_sort_csvs(existing_files)\n",
    "    \n",
    "    if result_df is not None:\n",
    "        print(f\"\\nPreview of merged data (first 5 rows):\")\n",
    "        print(\"=\" * 40)\n",
    "        print(result_df.head())\n",
    "        print(f\"\\nTotal rows: {len(result_df)}\")\n",
    "        print(f\"Columns: {list(result_df.columns)}\")\n",
    "        \n",
    "        # Show sort column range - FIXED: Use SORT_COLUMN instead of sort_column\n",
    "        if SORT_COLUMN in result_df.columns:\n",
    "            first_id = result_df[SORT_COLUMN].iloc[0]\n",
    "            last_id = result_df[SORT_COLUMN].iloc[-1]\n",
    "            print(f\"{SORT_COLUMN} range: {first_id} to {last_id}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7bdec13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Analyzing missing data patterns...\n",
      "\n",
      "PREVIEW: Missing data analysis for /data/users2/nblair7/analysis_results/merged_sorted_data.csv\n",
      "------------------------------------------------------------\n",
      "File: /data/users2/nblair7/analysis_results/merged_sorted_data.csv\n",
      "Rows: 51, Columns: 27\n",
      "\n",
      "Missing data by column:\n",
      "  iterations_required: 31 missing (60.8%)\n",
      "  low_scores_detected: 31 missing (60.8%)\n",
      "  final_coherence_score: 31 missing (60.8%)\n",
      "  final_coherence_explanation: 31 missing (60.8%)\n",
      "  initial_coherence_score: 31 missing (60.8%)\n",
      "  initial_coherence_explanation: 31 missing (60.8%)\n",
      "  final_completeness_score: 31 missing (60.8%)\n",
      "  final_completeness_explanation: 31 missing (60.8%)\n",
      "  initial_completeness_score: 31 missing (60.8%)\n",
      "  initial_completeness_explanation: 31 missing (60.8%)\n",
      "  final_specificity_score: 31 missing (60.8%)\n",
      "  final_specificity_explanation: 31 missing (60.8%)\n",
      "  initial_specificity_score: 31 missing (60.8%)\n",
      "  initial_specificity_explanation: 31 missing (60.8%)\n",
      "  final_accuracy_score: 31 missing (60.8%)\n",
      "  final_accuracy_explanation: 31 missing (60.8%)\n",
      "  initial_accuracy_score: 31 missing (60.8%)\n",
      "  initial_accuracy_explanation: 31 missing (60.8%)\n",
      "  coherence: 20 missing (39.2%)\n",
      "  coherence_explanation: 20 missing (39.2%)\n",
      "  completeness: 20 missing (39.2%)\n",
      "  completeness_explanation: 20 missing (39.2%)\n",
      "  specificity: 20 missing (39.2%)\n",
      "  specificity_explanation: 20 missing (39.2%)\n",
      "  accuracy: 20 missing (39.2%)\n",
      "  accuracy_explanation: 20 missing (39.2%)\n",
      "\n",
      "PREVIEW: Missing data analysis for //data/users2/nblair7/analysis_results/eval_results_new.csv\n",
      "------------------------------------------------------------\n",
      "File: //data/users2/nblair7/analysis_results/eval_results_new.csv\n",
      "Rows: 142, Columns: 9\n",
      "\n",
      "Missing data by column:\n",
      "\n",
      "Columns with complete data: ['coherence', 'coherence_explanation', 'completeness', 'completeness_explanation', 'specificity', 'specificity_explanation', 'accuracy', 'accuracy_explanation']\n",
      "\n",
      "============================================================\n",
      "STEP 2: Performing smart merge...\n",
      "Smart CSV Merger - Fill Missing Values Only\n",
      "==================================================\n",
      "Reading primary file: /data/users2/nblair7/analysis_results/merged_sorted_data.csv\n",
      "  - Loaded 51 rows, 27 columns\n",
      "Reading secondary file: //data/users2/nblair7/analysis_results/eval_results_new.csv\n",
      "  - Loaded 142 rows, 9 columns\n",
      "\n",
      "Common columns found: ['specificity_explanation', 'completeness', 'accuracy', 'completeness_explanation', 'specificity', 'coherence', 'coherence_explanation', 'accuracy_explanation']\n",
      "Will attempt to fill missing values in all common columns\n",
      "Columns to fill: ['specificity_explanation', 'completeness', 'accuracy', 'completeness_explanation', 'specificity', 'coherence', 'coherence_explanation', 'accuracy_explanation']\n",
      "\n",
      "Processing column 'specificity_explanation':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "Processing column 'completeness':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "Processing column 'accuracy':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "Processing column 'completeness_explanation':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "Processing column 'specificity':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "Processing column 'coherence':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "Processing column 'coherence_explanation':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "Processing column 'accuracy_explanation':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "==================================================\n",
      "SUMMARY OF CHANGES:\n",
      "Total values filled: 160\n",
      "  - specificity_explanation: 20 values filled\n",
      "  - completeness: 20 values filled\n",
      "  - accuracy: 20 values filled\n",
      "  - completeness_explanation: 20 values filled\n",
      "  - specificity: 20 values filled\n",
      "  - coherence: 20 values filled\n",
      "  - coherence_explanation: 20 values filled\n",
      "  - accuracy_explanation: 20 values filled\n",
      "\n",
      "Saved merged data to: /data/users2/nblair7/analysis_results/SCORES_FIXEDMERGE.csv\n",
      "Final dataset: 51 rows, 27 columns\n",
      "\n",
      "============================================================\n",
      "STEP 3: Final preview\n",
      "First 5 rows of merged data:\n",
      "   participant_id  iterations_required                  low_scores_detected  \\\n",
      "0             303                  8.0  accuracy, completeness, specificity   \n",
      "1             304                  1.0                             accuracy   \n",
      "2             307                  NaN                                  NaN   \n",
      "3             310                 10.0  accuracy, completeness, specificity   \n",
      "4             312                  1.0                         completeness   \n",
      "\n",
      "   final_coherence_score                        final_coherence_explanation  \\\n",
      "0                    5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "1                    5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "2                    NaN                                                NaN   \n",
      "3                    5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "4                    5.0  Score: 5\\nExplanation: The assessment is inter...   \n",
      "\n",
      "   initial_coherence_score                      initial_coherence_explanation  \\\n",
      "0                      5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "1                      5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "2                      NaN                                                NaN   \n",
      "3                      5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "4                      5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "\n",
      "   final_completeness_score  \\\n",
      "0                       4.0   \n",
      "1                       5.0   \n",
      "2                       NaN   \n",
      "3                       3.0   \n",
      "4                       4.0   \n",
      "\n",
      "                      final_completeness_explanation  \\\n",
      "0  Score: 4\\nExplanation: The assessment is gener...   \n",
      "1  Score: 5\\nExplanation: The assessment comprehe...   \n",
      "2                                                NaN   \n",
      "3  Score: 3\\nExplanation: The assessment is a goo...   \n",
      "4  Score: 4\\nExplanation: The assessment is quite...   \n",
      "\n",
      "   initial_completeness_score  ... initial_accuracy_score  \\\n",
      "0                         3.0  ...                    5.0   \n",
      "1                         4.0  ...                    3.0   \n",
      "2                         NaN  ...                    NaN   \n",
      "3                         3.0  ...                    3.0   \n",
      "4                         3.0  ...                    4.0   \n",
      "\n",
      "                        initial_accuracy_explanation coherence  \\\n",
      "0  Score: 5\\nExplanation: The assessment accurate...       5.0   \n",
      "1  Score: 3\\nExplanation: The assessment accurate...       5.0   \n",
      "2                                                NaN       5.0   \n",
      "3  Score: 3\\nExplanation: The assessment accurate...       5.0   \n",
      "4  Score: 4\\nExplanation: The assessment accurate...       5.0   \n",
      "\n",
      "                               coherence_explanation completeness  \\\n",
      "0  Score: 5\\nExplanation: The assessment is remar...          3.0   \n",
      "1  Score: 5\\nExplanation: The assessment is remar...          4.0   \n",
      "2  Score: 5\\nExplanation: The assessment is remar...          2.0   \n",
      "3  Score: 5\\nExplanation: The assessment is remar...          3.0   \n",
      "4  Score: 5\\nExplanation: The assessment is remar...          3.0   \n",
      "\n",
      "                            completeness_explanation specificity  \\\n",
      "0  Score: 3\\nExplanation: The assessment provides...         4.0   \n",
      "1  Score: 4\\nExplanation: The assessment is gener...         3.0   \n",
      "2  Score: 2\\nExplanation: The assessment is a det...         2.0   \n",
      "3  Score: 3\\nExplanation: The assessment provides...         4.0   \n",
      "4  Score: 3\\nExplanation: The assessment provides...         4.0   \n",
      "\n",
      "                             specificity_explanation accuracy  \\\n",
      "0  Score: 4\\nExplanation: The assessment is gener...      5.0   \n",
      "1  Score: 3\\nExplanation: The assessment demonstr...      3.0   \n",
      "2  Score: 2\\nExplanation: The assessment, while d...      4.0   \n",
      "3  Score: 4\\nExplanation: The assessment is gener...      3.0   \n",
      "4  Score: 4\\nExplanation: While the assessment is...      4.0   \n",
      "\n",
      "                                accuracy_explanation  \n",
      "0  Score: 5\\nExplanation: The assessment accurate...  \n",
      "1  Score: 3\\nExplanation: The assessment accurate...  \n",
      "2  Score: 4\\nExplanation: The assessment accurate...  \n",
      "3  Score: 3\\nExplanation: The assessment accurate...  \n",
      "4  Score: 4\\nExplanation: The assessment accurate...  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "\n",
      "Columns in final dataset: ['participant_id', 'iterations_required', 'low_scores_detected', 'final_coherence_score', 'final_coherence_explanation', 'initial_coherence_score', 'initial_coherence_explanation', 'final_completeness_score', 'final_completeness_explanation', 'initial_completeness_score', 'initial_completeness_explanation', 'final_specificity_score', 'final_specificity_explanation', 'initial_specificity_score', 'initial_specificity_explanation', 'final_accuracy_score', 'final_accuracy_explanation', 'initial_accuracy_score', 'initial_accuracy_explanation', 'coherence', 'coherence_explanation', 'completeness', 'completeness_explanation', 'specificity', 'specificity_explanation', 'accuracy', 'accuracy_explanation']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def smart_merge_csvs(primary_file, secondary_file, join_column=\"participant_id\", \n",
    "                     columns_to_fill=None, output_file=\"smart_merged_data.csv\"):\n",
    "    \"\"\"\n",
    "    Smart merge that only fills missing values in specified columns from secondary file.\n",
    "    \n",
    "    Parameters:\n",
    "    primary_file (str): Path to the main CSV file (this takes priority)\n",
    "    secondary_file (str): Path to the file with data to fill missing values\n",
    "    join_column (str): Column to join on (default: 'participant_id')\n",
    "    columns_to_fill (list): List of columns to fill missing values for. If None, fills all common columns\n",
    "    output_file (str): Output filename\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The merged dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Smart CSV Merger - Fill Missing Values Only\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Read the files\n",
    "    try:\n",
    "        print(f\"Reading primary file: {primary_file}\")\n",
    "        df_primary = pd.read_csv(primary_file)\n",
    "        df_primary.columns = df_primary.columns.str.strip()\n",
    "        print(f\"  - Loaded {len(df_primary)} rows, {len(df_primary.columns)} columns\")\n",
    "        \n",
    "        print(f\"Reading secondary file: {secondary_file}\")\n",
    "        df_secondary = pd.read_csv(secondary_file)\n",
    "        df_secondary.columns = df_secondary.columns.str.strip()\n",
    "        print(f\"  - Loaded {len(df_secondary)} rows, {len(df_secondary.columns)} columns\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading files: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Check if join column exists in both files\n",
    "    if join_column not in df_primary.columns:\n",
    "        print(f\"Error: '{join_column}' not found in primary file\")\n",
    "        print(f\"Available columns in primary: {list(df_primary.columns)}\")\n",
    "        return None\n",
    "    \n",
    "    if join_column not in df_secondary.columns:\n",
    "        print(f\"Error: '{join_column}' not found in secondary file\")\n",
    "        print(f\"Available columns in secondary: {list(df_secondary.columns)}\")\n",
    "        return None\n",
    "    \n",
    "    # Find common columns (excluding join column)\n",
    "    common_columns = list(set(df_primary.columns) & set(df_secondary.columns))\n",
    "    common_columns = [col for col in common_columns if col != join_column]\n",
    "    \n",
    "    print(f\"\\nCommon columns found: {common_columns}\")\n",
    "    \n",
    "    # Determine which columns to fill\n",
    "    if columns_to_fill is None:\n",
    "        columns_to_fill = common_columns\n",
    "        print(f\"Will attempt to fill missing values in all common columns\")\n",
    "    else:\n",
    "        # Validate that specified columns exist in both dataframes\n",
    "        missing_in_primary = [col for col in columns_to_fill if col not in df_primary.columns]\n",
    "        missing_in_secondary = [col for col in columns_to_fill if col not in df_secondary.columns]\n",
    "        \n",
    "        if missing_in_primary:\n",
    "            print(f\"Warning: These columns not found in primary file: {missing_in_primary}\")\n",
    "        if missing_in_secondary:\n",
    "            print(f\"Warning: These columns not found in secondary file: {missing_in_secondary}\")\n",
    "        \n",
    "        # Keep only columns that exist in both\n",
    "        columns_to_fill = [col for col in columns_to_fill \n",
    "                          if col in df_primary.columns and col in df_secondary.columns]\n",
    "    \n",
    "    print(f\"Columns to fill: {columns_to_fill}\")\n",
    "    \n",
    "    # Start with primary dataframe\n",
    "    result_df = df_primary.copy()\n",
    "    \n",
    "    # Track changes for reporting\n",
    "    changes_made = {}\n",
    "    \n",
    "    # For each column to fill\n",
    "    for col in columns_to_fill:\n",
    "        print(f\"\\nProcessing column '{col}':\")\n",
    "        \n",
    "        # Count missing values in primary\n",
    "        missing_mask = result_df[col].isna() | (result_df[col] == '') | (result_df[col] == 'NaN')\n",
    "        missing_count = missing_mask.sum()\n",
    "        print(f\"  - Found {missing_count} missing values in primary file\")\n",
    "        \n",
    "        if missing_count == 0:\n",
    "            print(f\"  - No missing values to fill in '{col}'\")\n",
    "            continue\n",
    "        \n",
    "        # Create a mapping from secondary file\n",
    "        secondary_mapping = df_secondary.set_index(join_column)[col].to_dict()\n",
    "        \n",
    "        # Track successful fills\n",
    "        fills_made = 0\n",
    "        \n",
    "        # Fill missing values\n",
    "        for idx in result_df.index:\n",
    "            if missing_mask.iloc[idx]:  # If this row has missing value\n",
    "                participant_id = result_df.loc[idx, join_column]\n",
    "                \n",
    "                if participant_id in secondary_mapping:\n",
    "                    secondary_value = secondary_mapping[participant_id]\n",
    "                    \n",
    "                    # Only fill if secondary value is not also missing\n",
    "                    if pd.notna(secondary_value) and secondary_value != '' and secondary_value != 'NaN':\n",
    "                        result_df.loc[idx, col] = secondary_value\n",
    "                        fills_made += 1\n",
    "        \n",
    "        print(f\"  - Successfully filled {fills_made} values\")\n",
    "        changes_made[col] = fills_made\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(\"SUMMARY OF CHANGES:\")\n",
    "    total_fills = sum(changes_made.values())\n",
    "    print(f\"Total values filled: {total_fills}\")\n",
    "    \n",
    "    for col, count in changes_made.items():\n",
    "        if count > 0:\n",
    "            print(f\"  - {col}: {count} values filled\")\n",
    "    \n",
    "    # Save result\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSaved merged data to: {output_file}\")\n",
    "    print(f\"Final dataset: {len(result_df)} rows, {len(result_df.columns)} columns\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def preview_missing_data(file_path, join_column=\"participant_id\"):\n",
    "    \"\"\"\n",
    "    Preview which columns have missing data and how much\n",
    "    \"\"\"\n",
    "    print(f\"\\nPREVIEW: Missing data analysis for {file_path}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        print(f\"File: {file_path}\")\n",
    "        print(f\"Rows: {len(df)}, Columns: {len(df.columns)}\")\n",
    "        \n",
    "        # Check for missing data in each column\n",
    "        missing_info = []\n",
    "        for col in df.columns:\n",
    "            if col != join_column:  # Skip the join column\n",
    "                missing_count = (df[col].isna() | (df[col] == '') | (df[col] == 'NaN')).sum()\n",
    "                missing_pct = (missing_count / len(df)) * 100\n",
    "                missing_info.append((col, missing_count, missing_pct))\n",
    "        \n",
    "        # Sort by missing count (descending)\n",
    "        missing_info.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nMissing data by column:\")\n",
    "        for col, count, pct in missing_info:\n",
    "            if count > 0:\n",
    "                print(f\"  {col}: {count} missing ({pct:.1f}%)\")\n",
    "        \n",
    "        # Show columns with no missing data\n",
    "        complete_columns = [col for col, count, pct in missing_info if count == 0]\n",
    "        if complete_columns:\n",
    "            print(f\"\\nColumns with complete data: {complete_columns}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing file: {e}\")\n",
    "\n",
    "# =====================================================\n",
    "# CONFIGURATION - EDIT THESE SETTINGS\n",
    "# =====================================================\n",
    "\n",
    "# File paths\n",
    "PRIMARY_FILE = \"/data/users2/nblair7/analysis_results/merged_sorted_data.csv\"  # This file takes priority\n",
    "SECONDARY_FILE = \"//data/users2/nblair7/analysis_results/eval_results_new.csv\"  # Fill missing values from here\n",
    "\n",
    "# Join column (usually participant_id)\n",
    "JOIN_COLUMN = \"participant_id\"\n",
    "\n",
    "# Specific columns to fill (set to None to fill all common columns)\n",
    "# Example: COLUMNS_TO_FILL = [\"score1\", \"score2\", \"feedback_rating\"]\n",
    "COLUMNS_TO_FILL = None  # Will fill all common columns\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = \"/data/users2/nblair7/analysis_results/SCORES_FIXEDMERGE.csv\"\n",
    "\n",
    "# =====================================================\n",
    "# MAIN EXECUTION\n",
    "# =====================================================\n",
    "\n",
    "def main():\n",
    "    # First, let's preview the missing data in both files\n",
    "    print(\"STEP 1: Analyzing missing data patterns...\")\n",
    "    preview_missing_data(PRIMARY_FILE, JOIN_COLUMN)\n",
    "    preview_missing_data(SECONDARY_FILE, JOIN_COLUMN)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 2: Performing smart merge...\")\n",
    "    \n",
    "    # Perform the smart merge\n",
    "    result = smart_merge_csvs(\n",
    "        primary_file=PRIMARY_FILE,\n",
    "        secondary_file=SECONDARY_FILE,\n",
    "        join_column=JOIN_COLUMN,\n",
    "        columns_to_fill=COLUMNS_TO_FILL,\n",
    "        output_file=OUTPUT_FILE\n",
    "    )\n",
    "    \n",
    "    if result is not None:\n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 3: Final preview\")\n",
    "        print(f\"First 5 rows of merged data:\")\n",
    "        print(result.head())\n",
    "        \n",
    "        print(f\"\\nColumns in final dataset: {list(result.columns)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f7c8a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Analyzing missing data patterns...\n",
      "\n",
      "PREVIEW: Missing data analysis for /data/users2/nblair7/analysis_results/merged_sorted_data.csv\n",
      "------------------------------------------------------------\n",
      "File: /data/users2/nblair7/analysis_results/merged_sorted_data.csv\n",
      "Rows: 51, Columns: 27\n",
      "\n",
      "Missing data by column:\n",
      "  iterations_required: 31 missing (60.8%)\n",
      "  low_scores_detected: 31 missing (60.8%)\n",
      "  final_coherence_score: 31 missing (60.8%)\n",
      "  final_coherence_explanation: 31 missing (60.8%)\n",
      "  initial_coherence_score: 31 missing (60.8%)\n",
      "  initial_coherence_explanation: 31 missing (60.8%)\n",
      "  final_completeness_score: 31 missing (60.8%)\n",
      "  final_completeness_explanation: 31 missing (60.8%)\n",
      "  initial_completeness_score: 31 missing (60.8%)\n",
      "  initial_completeness_explanation: 31 missing (60.8%)\n",
      "  final_specificity_score: 31 missing (60.8%)\n",
      "  final_specificity_explanation: 31 missing (60.8%)\n",
      "  initial_specificity_score: 31 missing (60.8%)\n",
      "  initial_specificity_explanation: 31 missing (60.8%)\n",
      "  final_accuracy_score: 31 missing (60.8%)\n",
      "  final_accuracy_explanation: 31 missing (60.8%)\n",
      "  initial_accuracy_score: 31 missing (60.8%)\n",
      "  initial_accuracy_explanation: 31 missing (60.8%)\n",
      "  coherence: 20 missing (39.2%)\n",
      "  coherence_explanation: 20 missing (39.2%)\n",
      "  completeness: 20 missing (39.2%)\n",
      "  completeness_explanation: 20 missing (39.2%)\n",
      "  specificity: 20 missing (39.2%)\n",
      "  specificity_explanation: 20 missing (39.2%)\n",
      "  accuracy: 20 missing (39.2%)\n",
      "  accuracy_explanation: 20 missing (39.2%)\n",
      "\n",
      "PREVIEW: Missing data analysis for /data/users2/nblair7/analysis_results/eval_results_new.csv\n",
      "------------------------------------------------------------\n",
      "File: /data/users2/nblair7/analysis_results/eval_results_new.csv\n",
      "Rows: 142, Columns: 9\n",
      "\n",
      "Missing data by column:\n",
      "\n",
      "Columns with complete data: ['coherence', 'coherence_explanation', 'completeness', 'completeness_explanation', 'specificity', 'specificity_explanation', 'accuracy', 'accuracy_explanation']\n",
      "\n",
      "STEP 1.5: Debugging specific participant...\n",
      "\n",
      "DEBUG: Analyzing participant 402\n",
      "--------------------------------------------------\n",
      "Common columns: ['specificity_explanation', 'completeness', 'accuracy', 'completeness_explanation', 'specificity', 'coherence', 'coherence_explanation', 'accuracy_explanation']\n",
      "\n",
      "Column-by-column comparison:\n",
      "  specificity_explanation:\n",
      "    Primary: 'Score: 4\n",
      "Explanation: The assessment is generally specific, drawing connections between statements and identifying potential risk/protective factors. However, there are a few instances where the assessment relies on interpretation without strong textual support. For example, stating the participant's avoidance is \"unsustainable if stressors increase\" is a prediction, not a direct observation from the transcript. Similarly, labeling the fiancé as a \"primary source of social support\" is a reasonable inference, but could be more nuanced given the limited discussion of other relationships. These are minor issues, but enough to prevent a perfect score. The assessment does a good job of backing up its claims with \"exact_quotes\" from the transcript.' (missing: False)\n",
      "    Secondary: 'Score: 4\n",
      "Explanation: While the assessment is generally good, there are a few instances where it relies on interpretation rather than directly referencing the transcript. For example, stating the participant has a \"subtle undercurrent of vulnerability\" or \"unresolved emotional issues\" is an interpretation. The assessment does a good job of pulling direct quotes, but occasionally infers meaning beyond what is explicitly stated. There are 1-2 instances of this interpretive language. The rest of the assessment is well-supported by the transcript.'\n",
      "    Would fill: False\n",
      "\n",
      "  completeness:\n",
      "    Primary: '4.0' (missing: False)\n",
      "    Secondary: '3'\n",
      "    Would fill: False\n",
      "\n",
      "  accuracy:\n",
      "    Primary: '4.0' (missing: False)\n",
      "    Secondary: '4'\n",
      "    Would fill: False\n",
      "\n",
      "  completeness_explanation:\n",
      "    Primary: 'Score: 4\n",
      "Explanation: The assessment is quite thorough and captures many of the key themes and details from the transcript. It accurately identifies the participant's strengths (resilience, proactive self-care, supportive relationship) and risk factors (avoidance, history of depression/insomnia). \n",
      "\n",
      "However, it *misses* a direct mention of the participant's suicidal ideation (stated they were feeling suicidal when they first sought help). While it acknowledges a history of depression, the severity of that history isn't fully conveyed by omitting the suicidal thoughts. Additionally, while it notes the participant's positive response to NLP and getting off antidepressants, it doesn't fully explore the potential implications of *stopping* medication – is this a stable situation, or could it be a risk factor if support were to decrease? These are minor omissions, but enough to bring the score down from a 5.' (missing: False)\n",
      "    Secondary: 'Score: 3\n",
      "Explanation: The assessment provides a good overview of the participant's mental health, covering biological, social, and risk factors. However, it misses key details regarding the *severity* and *duration* of the participant’s depression and insomnia. While it acknowledges the history, it doesn't quantify how long they've been dealing with these issues beyond stating \"a history of.\" Additionally, the assessment doesn't explicitly mention the participant's suicidal ideation (stated as \"i was feeling suicidal\") which is a significant symptom. These omissions constitute 3-4 mistakes, leading to a score of 3. The assessment is generally well-written and insightful, but lacks some crucial details for a complete picture of the participant's mental health.'\n",
      "    Would fill: False\n",
      "\n",
      "  specificity:\n",
      "    Primary: '4.0' (missing: False)\n",
      "    Secondary: '4'\n",
      "    Would fill: False\n",
      "\n",
      "  coherence:\n",
      "    Primary: '5.0' (missing: False)\n",
      "    Secondary: '5'\n",
      "    Would fill: False\n",
      "\n",
      "  coherence_explanation:\n",
      "    Primary: 'Score: 5\n",
      "Explanation: The assessment is remarkably coherent. It consistently and logically connects observations from the transcript to broader themes of mental health (biological, social, risk factors). The use of direct quotes to support each point is excellent and strengthens the assessment's validity. There are no logical inconsistencies or contradictions within the assessment. The summary statements accurately reflect the participant's statements and experiences as presented in the transcript.' (missing: False)\n",
      "    Secondary: 'Score: 5\n",
      "Explanation: The assessment is remarkably coherent. It consistently and logically connects observations from the transcript to broader themes of mental health (biological, social, risk factors). The summaries within each section are well-supported by the provided \"exact_quotes\" and demonstrate a clear understanding of the participant's statements. There are no contradictions or inconsistencies within the assessment itself, and it accurately reflects the nuances of the transcript. The assessment doesn't introduce any information *not* present in the transcript, and it maintains a consistent tone and perspective throughout.'\n",
      "    Would fill: False\n",
      "\n",
      "  accuracy_explanation:\n",
      "    Primary: 'Score: 4\n",
      "Explanation: The assessment accurately identifies the participant's history of depression, insomnia, and suicidal ideation. It also correctly notes the participant's reliance on self-management and avoidance of certain topics. However, there's a slight overemphasis on \"risk factors\" – while avoidance *is* a coping mechanism, framing it as a significant risk feels a bit strong given the participant's overall stable presentation and proactive approach to well-being. The assessment is thorough and well-supported by quotes from the transcript, but the minor overstatement of risk prevents a perfect score. There is one minor mistake: the assessment states the participant reports \"suicidal ideation\" but the transcript only states they *felt* suicidal in the past.' (missing: False)\n",
      "    Secondary: 'Score: 4\n",
      "Explanation: The assessment accurately identifies key themes from the transcript – the participant’s history of depression and insomnia, positive relationship with their fiancé, and tendency to avoid difficult topics. It correctly pulls relevant quotes to support its claims. However, there's a slight over-interpretation in framing the avoidance as necessarily indicative of *unresolved* issues – it could simply be a coping mechanism. Also, while the assessment notes the positive impact of NLP, it doesn't explicitly link this to the DSM-5 or PHQ-8 criteria. There's one minor misstep in assuming the avoidance *always* indicates unresolved issues, but overall the assessment is very strong and captures the nuances of the transcript.'\n",
      "    Would fill: False\n",
      "\n",
      "\n",
      "============================================================\n",
      "STEP 2: Performing smart merge...\n",
      "Smart CSV Merger - Fill Missing Values Only\n",
      "==================================================\n",
      "Reading primary file: /data/users2/nblair7/analysis_results/merged_sorted_data.csv\n",
      "  - Loaded 51 rows, 27 columns\n",
      "Reading secondary file: /data/users2/nblair7/analysis_results/eval_results_new.csv\n",
      "  - Loaded 142 rows, 9 columns\n",
      "\n",
      "Common columns found: ['specificity_explanation', 'completeness', 'accuracy', 'completeness_explanation', 'specificity', 'coherence', 'coherence_explanation', 'accuracy_explanation']\n",
      "Will attempt to fill missing values in all common columns\n",
      "Columns to fill: ['specificity_explanation', 'completeness', 'accuracy', 'completeness_explanation', 'specificity', 'coherence', 'coherence_explanation', 'accuracy_explanation']\n",
      "\n",
      "Processing column 'specificity_explanation':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "Processing column 'completeness':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "Processing column 'accuracy':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "Processing column 'completeness_explanation':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "Processing column 'specificity':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "Processing column 'coherence':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "Processing column 'coherence_explanation':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "Processing column 'accuracy_explanation':\n",
      "  - Found 20 missing values in primary file\n",
      "  - Successfully filled 20 values\n",
      "\n",
      "==================================================\n",
      "SUMMARY OF CHANGES:\n",
      "Total values filled: 160\n",
      "  - specificity_explanation: 20 values filled\n",
      "  - completeness: 20 values filled\n",
      "  - accuracy: 20 values filled\n",
      "  - completeness_explanation: 20 values filled\n",
      "  - specificity: 20 values filled\n",
      "  - coherence: 20 values filled\n",
      "  - coherence_explanation: 20 values filled\n",
      "  - accuracy_explanation: 20 values filled\n",
      "\n",
      "Saved merged data to: /data/users2/nblair7/analysis_results/MERGESDSCORES.csv\n",
      "Final dataset: 51 rows, 27 columns\n",
      "\n",
      "============================================================\n",
      "STEP 3: Final preview\n",
      "First 5 rows of merged data:\n",
      "   participant_id  iterations_required                  low_scores_detected  \\\n",
      "0             303                  8.0  accuracy, completeness, specificity   \n",
      "1             304                  1.0                             accuracy   \n",
      "2             307                  NaN                                  NaN   \n",
      "3             310                 10.0  accuracy, completeness, specificity   \n",
      "4             312                  1.0                         completeness   \n",
      "\n",
      "   final_coherence_score                        final_coherence_explanation  \\\n",
      "0                    5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "1                    5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "2                    NaN                                                NaN   \n",
      "3                    5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "4                    5.0  Score: 5\\nExplanation: The assessment is inter...   \n",
      "\n",
      "   initial_coherence_score                      initial_coherence_explanation  \\\n",
      "0                      5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "1                      5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "2                      NaN                                                NaN   \n",
      "3                      5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "4                      5.0  Score: 5\\nExplanation: The assessment is remar...   \n",
      "\n",
      "   final_completeness_score  \\\n",
      "0                       4.0   \n",
      "1                       5.0   \n",
      "2                       NaN   \n",
      "3                       3.0   \n",
      "4                       4.0   \n",
      "\n",
      "                      final_completeness_explanation  \\\n",
      "0  Score: 4\\nExplanation: The assessment is gener...   \n",
      "1  Score: 5\\nExplanation: The assessment comprehe...   \n",
      "2                                                NaN   \n",
      "3  Score: 3\\nExplanation: The assessment is a goo...   \n",
      "4  Score: 4\\nExplanation: The assessment is quite...   \n",
      "\n",
      "   initial_completeness_score  ... initial_accuracy_score  \\\n",
      "0                         3.0  ...                    5.0   \n",
      "1                         4.0  ...                    3.0   \n",
      "2                         NaN  ...                    NaN   \n",
      "3                         3.0  ...                    3.0   \n",
      "4                         3.0  ...                    4.0   \n",
      "\n",
      "                        initial_accuracy_explanation coherence  \\\n",
      "0  Score: 5\\nExplanation: The assessment accurate...       5.0   \n",
      "1  Score: 3\\nExplanation: The assessment accurate...       5.0   \n",
      "2                                                NaN       5.0   \n",
      "3  Score: 3\\nExplanation: The assessment accurate...       5.0   \n",
      "4  Score: 4\\nExplanation: The assessment accurate...       5.0   \n",
      "\n",
      "                               coherence_explanation completeness  \\\n",
      "0  Score: 5\\nExplanation: The assessment is remar...          3.0   \n",
      "1  Score: 5\\nExplanation: The assessment is remar...          4.0   \n",
      "2  Score: 5\\nExplanation: The assessment is remar...          2.0   \n",
      "3  Score: 5\\nExplanation: The assessment is remar...          3.0   \n",
      "4  Score: 5\\nExplanation: The assessment is remar...          3.0   \n",
      "\n",
      "                            completeness_explanation specificity  \\\n",
      "0  Score: 3\\nExplanation: The assessment provides...         4.0   \n",
      "1  Score: 4\\nExplanation: The assessment is gener...         3.0   \n",
      "2  Score: 2\\nExplanation: The assessment is a det...         2.0   \n",
      "3  Score: 3\\nExplanation: The assessment provides...         4.0   \n",
      "4  Score: 3\\nExplanation: The assessment provides...         4.0   \n",
      "\n",
      "                             specificity_explanation accuracy  \\\n",
      "0  Score: 4\\nExplanation: The assessment is gener...      5.0   \n",
      "1  Score: 3\\nExplanation: The assessment demonstr...      3.0   \n",
      "2  Score: 2\\nExplanation: The assessment, while d...      4.0   \n",
      "3  Score: 4\\nExplanation: The assessment is gener...      3.0   \n",
      "4  Score: 4\\nExplanation: While the assessment is...      4.0   \n",
      "\n",
      "                                accuracy_explanation  \n",
      "0  Score: 5\\nExplanation: The assessment accurate...  \n",
      "1  Score: 3\\nExplanation: The assessment accurate...  \n",
      "2  Score: 4\\nExplanation: The assessment accurate...  \n",
      "3  Score: 3\\nExplanation: The assessment accurate...  \n",
      "4  Score: 4\\nExplanation: The assessment accurate...  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "\n",
      "Columns in final dataset: ['participant_id', 'iterations_required', 'low_scores_detected', 'final_coherence_score', 'final_coherence_explanation', 'initial_coherence_score', 'initial_coherence_explanation', 'final_completeness_score', 'final_completeness_explanation', 'initial_completeness_score', 'initial_completeness_explanation', 'final_specificity_score', 'final_specificity_explanation', 'initial_specificity_score', 'initial_specificity_explanation', 'final_accuracy_score', 'final_accuracy_explanation', 'initial_accuracy_score', 'initial_accuracy_explanation', 'coherence', 'coherence_explanation', 'completeness', 'completeness_explanation', 'specificity', 'specificity_explanation', 'accuracy', 'accuracy_explanation']\n",
      "\n",
      "Participant 402 after merge:\n",
      "  participant_id: '402'\n",
      "  iterations_required: 'nan'\n",
      "  low_scores_detected: 'nan'\n",
      "  final_coherence_score: 'nan'\n",
      "  final_coherence_explanation: 'nan'\n",
      "  initial_coherence_score: 'nan'\n",
      "  initial_coherence_explanation: 'nan'\n",
      "  final_completeness_score: 'nan'\n",
      "  final_completeness_explanation: 'nan'\n",
      "  initial_completeness_score: 'nan'\n",
      "  initial_completeness_explanation: 'nan'\n",
      "  final_specificity_score: 'nan'\n",
      "  final_specificity_explanation: 'nan'\n",
      "  initial_specificity_score: 'nan'\n",
      "  initial_specificity_explanation: 'nan'\n",
      "  final_accuracy_score: 'nan'\n",
      "  final_accuracy_explanation: 'nan'\n",
      "  initial_accuracy_score: 'nan'\n",
      "  initial_accuracy_explanation: 'nan'\n",
      "  coherence: '5.0'\n",
      "  coherence_explanation: 'Score: 5\n",
      "Explanation: The assessment is remarkably coherent. It consistently and logically connects observations from the transcript to broader themes of mental health (biological, social, risk factors). The use of direct quotes to support each point is excellent and strengthens the assessment's validity. There are no logical inconsistencies or contradictions within the assessment. The summary statements accurately reflect the participant's statements and experiences as presented in the transcript.'\n",
      "  completeness: '4.0'\n",
      "  completeness_explanation: 'Score: 4\n",
      "Explanation: The assessment is quite thorough and captures many of the key themes and details from the transcript. It accurately identifies the participant's strengths (resilience, proactive self-care, supportive relationship) and risk factors (avoidance, history of depression/insomnia). \n",
      "\n",
      "However, it *misses* a direct mention of the participant's suicidal ideation (stated they were feeling suicidal when they first sought help). While it acknowledges a history of depression, the severity of that history isn't fully conveyed by omitting the suicidal thoughts. Additionally, while it notes the participant's positive response to NLP and getting off antidepressants, it doesn't fully explore the potential implications of *stopping* medication – is this a stable situation, or could it be a risk factor if support were to decrease? These are minor omissions, but enough to bring the score down from a 5.'\n",
      "  specificity: '4.0'\n",
      "  specificity_explanation: 'Score: 4\n",
      "Explanation: The assessment is generally specific, drawing connections between statements and identifying potential risk/protective factors. However, there are a few instances where the assessment relies on interpretation without strong textual support. For example, stating the participant's avoidance is \"unsustainable if stressors increase\" is a prediction, not a direct observation from the transcript. Similarly, labeling the fiancé as a \"primary source of social support\" is a reasonable inference, but could be more nuanced given the limited discussion of other relationships. These are minor issues, but enough to prevent a perfect score. The assessment does a good job of backing up its claims with \"exact_quotes\" from the transcript.'\n",
      "  accuracy: '4.0'\n",
      "  accuracy_explanation: 'Score: 4\n",
      "Explanation: The assessment accurately identifies the participant's history of depression, insomnia, and suicidal ideation. It also correctly notes the participant's reliance on self-management and avoidance of certain topics. However, there's a slight overemphasis on \"risk factors\" – while avoidance *is* a coping mechanism, framing it as a significant risk feels a bit strong given the participant's overall stable presentation and proactive approach to well-being. The assessment is thorough and well-supported by quotes from the transcript, but the minor overstatement of risk prevents a perfect score. There is one minor mistake: the assessment states the participant reports \"suicidal ideation\" but the transcript only states they *felt* suicidal in the past.'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def smart_merge_csvs(primary_file, secondary_file, join_column=\"participant_id\", \n",
    "                     columns_to_fill=None, output_file=\"smart_merged_data.csv\"):\n",
    "    \"\"\"\n",
    "    Smart merge that only fills missing values in specified columns from secondary file.\n",
    "    \n",
    "    Parameters:\n",
    "    primary_file (str): Path to the main CSV file (this takes priority)\n",
    "    secondary_file (str): Path to the file with data to fill missing values\n",
    "    join_column (str): Column to join on (default: 'participant_id')\n",
    "    columns_to_fill (list): List of columns to fill missing values for. If None, fills all common columns\n",
    "    output_file (str): Output filename\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The merged dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Smart CSV Merger - Fill Missing Values Only\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Read the files with better empty value handling\n",
    "    try:\n",
    "        print(f\"Reading primary file: {primary_file}\")\n",
    "        df_primary = pd.read_csv(primary_file, na_values=['', ' ', 'nan', 'NaN', 'NULL', 'null'], keep_default_na=True)\n",
    "        df_primary.columns = df_primary.columns.str.strip()\n",
    "        print(f\"  - Loaded {len(df_primary)} rows, {len(df_primary.columns)} columns\")\n",
    "        \n",
    "        print(f\"Reading secondary file: {secondary_file}\")\n",
    "        df_secondary = pd.read_csv(secondary_file, na_values=['', ' ', 'nan', 'NaN', 'NULL', 'null'], keep_default_na=True)\n",
    "        df_secondary.columns = df_secondary.columns.str.strip()\n",
    "        print(f\"  - Loaded {len(df_secondary)} rows, {len(df_secondary.columns)} columns\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading files: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Check if join column exists in both files\n",
    "    if join_column not in df_primary.columns:\n",
    "        print(f\"Error: '{join_column}' not found in primary file\")\n",
    "        print(f\"Available columns in primary: {list(df_primary.columns)}\")\n",
    "        return None\n",
    "    \n",
    "    if join_column not in df_secondary.columns:\n",
    "        print(f\"Error: '{join_column}' not found in secondary file\")\n",
    "        print(f\"Available columns in secondary: {list(df_secondary.columns)}\")\n",
    "        return None\n",
    "    \n",
    "    # Find common columns (excluding join column)\n",
    "    common_columns = list(set(df_primary.columns) & set(df_secondary.columns))\n",
    "    common_columns = [col for col in common_columns if col != join_column]\n",
    "    \n",
    "    print(f\"\\nCommon columns found: {common_columns}\")\n",
    "    \n",
    "    # Determine which columns to fill\n",
    "    if columns_to_fill is None:\n",
    "        columns_to_fill = common_columns\n",
    "        print(f\"Will attempt to fill missing values in all common columns\")\n",
    "    else:\n",
    "        # Validate that specified columns exist in both dataframes\n",
    "        missing_in_primary = [col for col in columns_to_fill if col not in df_primary.columns]\n",
    "        missing_in_secondary = [col for col in columns_to_fill if col not in df_secondary.columns]\n",
    "        \n",
    "        if missing_in_primary:\n",
    "            print(f\"Warning: These columns not found in primary file: {missing_in_primary}\")\n",
    "        if missing_in_secondary:\n",
    "            print(f\"Warning: These columns not found in secondary file: {missing_in_secondary}\")\n",
    "        \n",
    "        # Keep only columns that exist in both\n",
    "        columns_to_fill = [col for col in columns_to_fill \n",
    "                          if col in df_primary.columns and col in df_secondary.columns]\n",
    "    \n",
    "    print(f\"Columns to fill: {columns_to_fill}\")\n",
    "    \n",
    "    # Start with primary dataframe\n",
    "    result_df = df_primary.copy()\n",
    "    \n",
    "    # Track changes for reporting\n",
    "    changes_made = {}\n",
    "    \n",
    "    # For each column to fill\n",
    "    for col in columns_to_fill:\n",
    "        print(f\"\\nProcessing column '{col}':\")\n",
    "        \n",
    "        # Count missing values in primary - be more thorough\n",
    "        missing_mask = (\n",
    "            result_df[col].isna() | \n",
    "            (result_df[col] == '') | \n",
    "            (result_df[col] == 'NaN') |\n",
    "            (result_df[col] == 'nan') |\n",
    "            (result_df[col] == 'NULL') |\n",
    "            (result_df[col] == 'null') |\n",
    "            (result_df[col].astype(str).str.strip() == '')\n",
    "        )\n",
    "        missing_count = missing_mask.sum()\n",
    "        print(f\"  - Found {missing_count} missing values in primary file\")\n",
    "        \n",
    "        if missing_count == 0:\n",
    "            print(f\"  - No missing values to fill in '{col}'\")\n",
    "            continue\n",
    "        \n",
    "        # Create a mapping from secondary file\n",
    "        secondary_mapping = df_secondary.set_index(join_column)[col].to_dict()\n",
    "        \n",
    "        # Track successful fills\n",
    "        fills_made = 0\n",
    "        \n",
    "        # Fill missing values\n",
    "        for idx in result_df.index:\n",
    "            if missing_mask.iloc[idx]:  # If this row has missing value\n",
    "                participant_id = result_df.loc[idx, join_column]\n",
    "                \n",
    "                if participant_id in secondary_mapping:\n",
    "                    secondary_value = secondary_mapping[participant_id]\n",
    "                    \n",
    "                    # Only fill if secondary value is not also missing - be more thorough\n",
    "                    if (pd.notna(secondary_value) and \n",
    "                        secondary_value != '' and \n",
    "                        secondary_value != 'NaN' and\n",
    "                        secondary_value != 'nan' and\n",
    "                        secondary_value != 'NULL' and\n",
    "                        secondary_value != 'null' and\n",
    "                        str(secondary_value).strip() != ''):\n",
    "                        result_df.loc[idx, col] = secondary_value\n",
    "                        fills_made += 1\n",
    "        \n",
    "        print(f\"  - Successfully filled {fills_made} values\")\n",
    "        changes_made[col] = fills_made\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(\"SUMMARY OF CHANGES:\")\n",
    "    total_fills = sum(changes_made.values())\n",
    "    print(f\"Total values filled: {total_fills}\")\n",
    "    \n",
    "    for col, count in changes_made.items():\n",
    "        if count > 0:\n",
    "            print(f\"  - {col}: {count} values filled\")\n",
    "    \n",
    "    # Save result\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSaved merged data to: {output_file}\")\n",
    "    print(f\"Final dataset: {len(result_df)} rows, {len(result_df.columns)} columns\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def debug_specific_row(primary_file, secondary_file, participant_id, join_column=\"participant_id\"):\n",
    "    \"\"\"\n",
    "    Debug a specific participant to see exactly what's happening\n",
    "    \"\"\"\n",
    "    print(f\"\\nDEBUG: Analyzing participant {participant_id}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Read files\n",
    "        df_primary = pd.read_csv(primary_file, na_values=['', ' ', 'nan', 'NaN', 'NULL', 'null'], keep_default_na=True)\n",
    "        df_primary.columns = df_primary.columns.str.strip()\n",
    "        \n",
    "        df_secondary = pd.read_csv(secondary_file, na_values=['', ' ', 'nan', 'NaN', 'NULL', 'null'], keep_default_na=True)\n",
    "        df_secondary.columns = df_secondary.columns.str.strip()\n",
    "        \n",
    "        # Find the participant in both files\n",
    "        primary_row = df_primary[df_primary[join_column] == participant_id]\n",
    "        secondary_row = df_secondary[df_secondary[join_column] == participant_id]\n",
    "        \n",
    "        if len(primary_row) == 0:\n",
    "            print(f\"Participant {participant_id} not found in primary file\")\n",
    "            return\n",
    "        if len(secondary_row) == 0:\n",
    "            print(f\"Participant {participant_id} not found in secondary file\")\n",
    "            return\n",
    "        \n",
    "        primary_row = primary_row.iloc[0]\n",
    "        secondary_row = secondary_row.iloc[0]\n",
    "        \n",
    "        # Find common columns\n",
    "        common_columns = list(set(df_primary.columns) & set(df_secondary.columns))\n",
    "        common_columns = [col for col in common_columns if col != join_column]\n",
    "        \n",
    "        print(f\"Common columns: {common_columns}\")\n",
    "        print(f\"\\nColumn-by-column comparison:\")\n",
    "        \n",
    "        for col in common_columns:\n",
    "            primary_val = primary_row[col]\n",
    "            secondary_val = secondary_row[col]\n",
    "            \n",
    "            # Check if primary is missing\n",
    "            is_primary_missing = (\n",
    "                pd.isna(primary_val) or\n",
    "                primary_val == '' or\n",
    "                primary_val == 'NaN' or\n",
    "                primary_val == 'nan' or\n",
    "                primary_val == 'NULL' or\n",
    "                primary_val == 'null' or\n",
    "                str(primary_val).strip() == ''\n",
    "            )\n",
    "            \n",
    "            print(f\"  {col}:\")\n",
    "            print(f\"    Primary: '{primary_val}' (missing: {is_primary_missing})\")\n",
    "            print(f\"    Secondary: '{secondary_val}'\")\n",
    "            print(f\"    Would fill: {is_primary_missing and pd.notna(secondary_val) and str(secondary_val).strip() != ''}\")\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in debug: {e}\")\n",
    "\n",
    "def preview_missing_data(file_path, join_column=\"participant_id\"):\n",
    "    \"\"\"\n",
    "    Preview which columns have missing data and how much\n",
    "    \"\"\"\n",
    "    print(f\"\\nPREVIEW: Missing data analysis for {file_path}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path, na_values=['', ' ', 'nan', 'NaN', 'NULL', 'null'], keep_default_na=True)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        print(f\"File: {file_path}\")\n",
    "        print(f\"Rows: {len(df)}, Columns: {len(df.columns)}\")\n",
    "        \n",
    "        # Check for missing data in each column\n",
    "        missing_info = []\n",
    "        for col in df.columns:\n",
    "            if col != join_column:  # Skip the join column\n",
    "                missing_count = (\n",
    "                    df[col].isna() | \n",
    "                    (df[col] == '') | \n",
    "                    (df[col] == 'NaN') |\n",
    "                    (df[col] == 'nan') |\n",
    "                    (df[col] == 'NULL') |\n",
    "                    (df[col] == 'null') |\n",
    "                    (df[col].astype(str).str.strip() == '')\n",
    "                ).sum()\n",
    "                missing_pct = (missing_count / len(df)) * 100\n",
    "                missing_info.append((col, missing_count, missing_pct))\n",
    "        \n",
    "        # Sort by missing count (descending)\n",
    "        missing_info.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nMissing data by column:\")\n",
    "        for col, count, pct in missing_info:\n",
    "            if count > 0:\n",
    "                print(f\"  {col}: {count} missing ({pct:.1f}%)\")\n",
    "        \n",
    "        # Show columns with no missing data\n",
    "        complete_columns = [col for col, count, pct in missing_info if count == 0]\n",
    "        if complete_columns:\n",
    "            print(f\"\\nColumns with complete data: {complete_columns}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing file: {e}\")\n",
    "\n",
    "# =====================================================\n",
    "# CONFIGURATION - EDIT THESE SETTINGS\n",
    "# =====================================================\n",
    "\n",
    "# File paths\n",
    "PRIMARY_FILE = \"/data/users2/nblair7/analysis_results/merged_sorted_data.csv\"  # This file takes priority\n",
    "SECONDARY_FILE = \"/data/users2/nblair7/analysis_results/eval_results_new.csv\"  # Fill missing values from here\n",
    "\n",
    "# Join column (usually participant_id)\n",
    "JOIN_COLUMN = \"participant_id\"\n",
    "\n",
    "# Specific columns to fill (set to None to fill all common columns)\n",
    "# Example: COLUMNS_TO_FILL = [\"score1\", \"score2\", \"feedback_rating\"]\n",
    "COLUMNS_TO_FILL = None  # Will fill all common columns\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = \"/data/users2/nblair7/analysis_results/MERGESDSCORES.csv\"\n",
    "\n",
    "# =====================================================\n",
    "# MAIN EXECUTION\n",
    "# =====================================================\n",
    "\n",
    "def main():\n",
    "    # First, let's preview the missing data in both files\n",
    "    print(\"STEP 1: Analyzing missing data patterns...\")\n",
    "    preview_missing_data(PRIMARY_FILE, JOIN_COLUMN)\n",
    "    preview_missing_data(SECONDARY_FILE, JOIN_COLUMN)\n",
    "    \n",
    "    # Debug a specific participant (change this to your problem participant)\n",
    "    print(\"\\nSTEP 1.5: Debugging specific participant...\")\n",
    "    debug_specific_row(PRIMARY_FILE, SECONDARY_FILE, 402, JOIN_COLUMN)  # Change 402 to your participant ID\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 2: Performing smart merge...\")\n",
    "    \n",
    "    # Perform the smart merge\n",
    "    result = smart_merge_csvs(\n",
    "        primary_file=PRIMARY_FILE,\n",
    "        secondary_file=SECONDARY_FILE,\n",
    "        join_column=JOIN_COLUMN,\n",
    "        columns_to_fill=COLUMNS_TO_FILL,\n",
    "        output_file=OUTPUT_FILE\n",
    "    )\n",
    "    \n",
    "    if result is not None:\n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 3: Final preview\")\n",
    "        print(f\"First 5 rows of merged data:\")\n",
    "        print(result.head())\n",
    "        \n",
    "        print(f\"\\nColumns in final dataset: {list(result.columns)}\")\n",
    "        \n",
    "        # Show a specific participant after merge\n",
    "        if 402 in result[JOIN_COLUMN].values:\n",
    "            print(f\"\\nParticipant 402 after merge:\")\n",
    "            participant_402 = result[result[JOIN_COLUMN] == 402].iloc[0]\n",
    "            for col in result.columns:\n",
    "                print(f\"  {col}: '{participant_402[col]}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d3d82e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Column Reorder and Selection Tool\n",
      "============================================================\n",
      "Input file: /data/users2/nblair7/analysis_results/MERGESDSCORES.csv\n",
      "Output file: /data/users2/nblair7/analysis_results/MERGEDS.csv\n",
      "Desired columns: ['participant_id', 'coherence', 'coherence_explanation', 'completeness', 'completeness_explanation', 'specificity', 'specificity_explanation', 'accuracy', 'accuracy_explanation']\n",
      "\n",
      "📋 ORIGINAL DATA STRUCTURE\n",
      "------------------------------------------------------------\n",
      "Shape: (51, 27)\n",
      "Columns: ['participant_id', 'iterations_required', 'low_scores_detected', 'final_coherence_score', 'final_coherence_explanation', 'initial_coherence_score', 'initial_coherence_explanation', 'final_completeness_score', 'final_completeness_explanation', 'initial_completeness_score', 'initial_completeness_explanation', 'final_specificity_score', 'final_specificity_explanation', 'initial_specificity_score', 'initial_specificity_explanation', 'final_accuracy_score', 'final_accuracy_explanation', 'initial_accuracy_score', 'initial_accuracy_explanation', 'coherence', 'coherence_explanation', 'completeness', 'completeness_explanation', 'specificity', 'specificity_explanation', 'accuracy', 'accuracy_explanation']\n",
      "\n",
      "First 2 rows:\n",
      "\n",
      "Row 1:\n",
      "  participant_id: 303\n",
      "  iterations_required: 8.0\n",
      "  low_scores_detected: accuracy, completeness, specificity\n",
      "  final_coherence_score: 5.0\n",
      "  final_coherence_explanation: Score: 5\n",
      "Explanation: The assessment is remarkably coherent. It consistently ...\n",
      "  initial_coherence_score: 5.0\n",
      "  initial_coherence_explanation: Score: 5\n",
      "Explanation: The assessment is remarkably coherent. It consistently ...\n",
      "  final_completeness_score: 4.0\n",
      "  final_completeness_explanation: Score: 4\n",
      "Explanation: The assessment is generally good and covers many of the...\n",
      "  initial_completeness_score: 3.0\n",
      "  initial_completeness_explanation: Score: 3\n",
      "Explanation: The assessment provides a good overview of the particip...\n",
      "  final_specificity_score: 5.0\n",
      "  final_specificity_explanation: Score: 5\n",
      "Explanation: The assessment is highly specific and avoids vague gene...\n",
      "  initial_specificity_score: 4.0\n",
      "  initial_specificity_explanation: Score: 4\n",
      "Explanation: The assessment is generally specific, drawing reasonabl...\n",
      "  final_accuracy_score: 5.0\n",
      "  final_accuracy_explanation: Score: 5\n",
      "Explanation: The assessment accurately reflects the content of the t...\n",
      "  initial_accuracy_score: 5.0\n",
      "  initial_accuracy_explanation: Score: 5\n",
      "Explanation: The assessment accurately reflects the content of the t...\n",
      "  coherence: 5.0\n",
      "  coherence_explanation: Score: 5\n",
      "Explanation: The assessment is remarkably coherent. It consistently ...\n",
      "  completeness: 3.0\n",
      "  completeness_explanation: Score: 3\n",
      "Explanation: The assessment provides a good overview of the particip...\n",
      "  specificity: 4.0\n",
      "  specificity_explanation: Score: 4\n",
      "Explanation: The assessment is generally specific, drawing reasonabl...\n",
      "  accuracy: 5.0\n",
      "  accuracy_explanation: Score: 5\n",
      "Explanation: The assessment accurately reflects the content of the t...\n",
      "\n",
      "Row 2:\n",
      "  participant_id: 304\n",
      "  iterations_required: 1.0\n",
      "  low_scores_detected: accuracy\n",
      "  final_coherence_score: 5.0\n",
      "  final_coherence_explanation: Score: 5\n",
      "Explanation: The assessment is remarkably coherent. It consistently ...\n",
      "  initial_coherence_score: 5.0\n",
      "  initial_coherence_explanation: Score: 5\n",
      "Explanation: The assessment is remarkably coherent. It consistently ...\n",
      "  final_completeness_score: 5.0\n",
      "  final_completeness_explanation: Score: 5\n",
      "Explanation: The assessment comprehensively covers the participant's...\n",
      "  initial_completeness_score: 4.0\n",
      "  initial_completeness_explanation: Score: 4\n",
      "Explanation: The assessment is generally good and captures the parti...\n",
      "  final_specificity_score: 5.0\n",
      "  final_specificity_explanation: Score: 5\n",
      "Explanation: The assessment is remarkably specific and avoids vague ...\n",
      "  initial_specificity_score: 4.0\n",
      "  initial_specificity_explanation: Score: 4\n",
      "Explanation: While the assessment generally avoids overly broad stat...\n",
      "  final_accuracy_score: 5.0\n",
      "  final_accuracy_explanation: Score: 5\n",
      "Explanation: The assessment accurately reflects the participant's st...\n",
      "  initial_accuracy_score: 3.0\n",
      "  initial_accuracy_explanation: Score: 3\n",
      "Explanation: The assessment accurately identifies several themes fro...\n",
      "  coherence: 5.0\n",
      "  coherence_explanation: Score: 5\n",
      "Explanation: The assessment is remarkably coherent. It consistently ...\n",
      "  completeness: 4.0\n",
      "  completeness_explanation: Score: 4\n",
      "Explanation: The assessment is generally good and captures the parti...\n",
      "  specificity: 3.0\n",
      "  specificity_explanation: Score: 3\n",
      "Explanation: The assessment demonstrates several instances of genera...\n",
      "  accuracy: 3.0\n",
      "  accuracy_explanation: Score: 3\n",
      "Explanation: The assessment accurately identifies several themes fro...\n",
      "\n",
      "============================================================\n",
      "PROCESSING...\n",
      "CSV Column Reorder Tool\n",
      "==================================================\n",
      "Reading input file: /data/users2/nblair7/analysis_results/MERGESDSCORES.csv\n",
      "  - Loaded 51 rows, 27 columns\n",
      "  - Original columns: ['participant_id', 'iterations_required', 'low_scores_detected', 'final_coherence_score', 'final_coherence_explanation', 'initial_coherence_score', 'initial_coherence_explanation', 'final_completeness_score', 'final_completeness_explanation', 'initial_completeness_score', 'initial_completeness_explanation', 'final_specificity_score', 'final_specificity_explanation', 'initial_specificity_score', 'initial_specificity_explanation', 'final_accuracy_score', 'final_accuracy_explanation', 'initial_accuracy_score', 'initial_accuracy_explanation', 'coherence', 'coherence_explanation', 'completeness', 'completeness_explanation', 'specificity', 'specificity_explanation', 'accuracy', 'accuracy_explanation']\n",
      "\n",
      "📊 Column Analysis:\n",
      "  ✅ Found 9 of 9 desired columns\n",
      "  ❌ Missing 0 columns\n",
      "\n",
      "✅ Columns that will be included:\n",
      "    - participant_id\n",
      "    - coherence\n",
      "    - coherence_explanation\n",
      "    - completeness\n",
      "    - completeness_explanation\n",
      "    - specificity\n",
      "    - specificity_explanation\n",
      "    - accuracy\n",
      "    - accuracy_explanation\n",
      "\n",
      "📋 Creating output file with 9 columns:\n",
      "  - New column order: ['participant_id', 'coherence', 'coherence_explanation', 'completeness', 'completeness_explanation', 'specificity', 'specificity_explanation', 'accuracy', 'accuracy_explanation']\n",
      "✅ Saved reordered data to: /data/users2/nblair7/analysis_results/MERGEDS.csv\n",
      "\n",
      "📈 Summary:\n",
      "  - Input rows: 51\n",
      "  - Output rows: 51\n",
      "  - Input columns: 27\n",
      "  - Output columns: 9\n",
      "\n",
      "============================================================\n",
      "RESULTS:\n",
      "\n",
      "📋 FINAL REORDERED DATA\n",
      "------------------------------------------------------------\n",
      "Shape: (51, 9)\n",
      "Columns: ['participant_id', 'coherence', 'coherence_explanation', 'completeness', 'completeness_explanation', 'specificity', 'specificity_explanation', 'accuracy', 'accuracy_explanation']\n",
      "\n",
      "First 3 rows:\n",
      "\n",
      "Row 1:\n",
      "  participant_id: 303\n",
      "  coherence: 5.0\n",
      "  coherence_explanation: Score: 5\n",
      "Explanation: The assessment is remarkably coherent. It consistently ...\n",
      "  completeness: 3.0\n",
      "  completeness_explanation: Score: 3\n",
      "Explanation: The assessment provides a good overview of the particip...\n",
      "  specificity: 4.0\n",
      "  specificity_explanation: Score: 4\n",
      "Explanation: The assessment is generally specific, drawing reasonabl...\n",
      "  accuracy: 5.0\n",
      "  accuracy_explanation: Score: 5\n",
      "Explanation: The assessment accurately reflects the content of the t...\n",
      "\n",
      "Row 2:\n",
      "  participant_id: 304\n",
      "  coherence: 5.0\n",
      "  coherence_explanation: Score: 5\n",
      "Explanation: The assessment is remarkably coherent. It consistently ...\n",
      "  completeness: 4.0\n",
      "  completeness_explanation: Score: 4\n",
      "Explanation: The assessment is generally good and captures the parti...\n",
      "  specificity: 3.0\n",
      "  specificity_explanation: Score: 3\n",
      "Explanation: The assessment demonstrates several instances of genera...\n",
      "  accuracy: 3.0\n",
      "  accuracy_explanation: Score: 3\n",
      "Explanation: The assessment accurately identifies several themes fro...\n",
      "\n",
      "Row 3:\n",
      "  participant_id: 307\n",
      "  coherence: 5.0\n",
      "  coherence_explanation: Score: 5\n",
      "Explanation: The assessment is remarkably coherent. It consistently ...\n",
      "  completeness: 2.0\n",
      "  completeness_explanation: Score: 2\n",
      "Explanation: The assessment is a detailed summary and interpretation...\n",
      "  specificity: 2.0\n",
      "  specificity_explanation: Score: 2\n",
      "Explanation: The assessment, while detailed, frequently relies on in...\n",
      "  accuracy: 4.0\n",
      "  accuracy_explanation: Score: 4\n",
      "Explanation: The assessment accurately identifies themes of adjustme...\n",
      "\n",
      "✅ SUCCESS!\n",
      "Your reordered CSV has been saved as: /data/users2/nblair7/analysis_results/MERGEDS.csv\n",
      "The file now has 9 columns in your desired order.\n",
      "\n",
      "📋 New header line:\n",
      "participant_id,coherence,coherence_explanation,completeness,completeness_explanation,specificity,specificity_explanation,accuracy,accuracy_explanation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def reorder_csv_columns(input_file, output_file, desired_columns):\n",
    "    \"\"\"\n",
    "    Reorder and select specific columns from a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    input_file (str): Path to the input CSV file\n",
    "    output_file (str): Path for the output CSV file\n",
    "    desired_columns (list): List of column names in the desired order\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The reordered dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"CSV Column Reorder Tool\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Read the input file\n",
    "        print(f\"Reading input file: {input_file}\")\n",
    "        df = pd.read_csv(input_file)\n",
    "        df.columns = df.columns.str.strip()  # Remove any whitespace from column names\n",
    "        \n",
    "        print(f\"  - Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
    "        print(f\"  - Original columns: {list(df.columns)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Check which desired columns exist in the dataframe\n",
    "    existing_columns = []\n",
    "    missing_columns = []\n",
    "    \n",
    "    for col in desired_columns:\n",
    "        if col in df.columns:\n",
    "            existing_columns.append(col)\n",
    "        else:\n",
    "            missing_columns.append(col)\n",
    "    \n",
    "    print(f\"\\n📊 Column Analysis:\")\n",
    "    print(f\"  ✅ Found {len(existing_columns)} of {len(desired_columns)} desired columns\")\n",
    "    print(f\"  ❌ Missing {len(missing_columns)} columns\")\n",
    "    \n",
    "    if existing_columns:\n",
    "        print(f\"\\n✅ Columns that will be included:\")\n",
    "        for col in existing_columns:\n",
    "            print(f\"    - {col}\")\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"\\n❌ Missing columns (will be skipped):\")\n",
    "        for col in missing_columns:\n",
    "            print(f\"    - {col}\")\n",
    "    \n",
    "    # Create new dataframe with only the existing desired columns\n",
    "    if not existing_columns:\n",
    "        print(\"❌ No desired columns found in the input file!\")\n",
    "        return None\n",
    "    \n",
    "    # Select and reorder columns\n",
    "    result_df = df[existing_columns].copy()\n",
    "    \n",
    "    print(f\"\\n📋 Creating output file with {len(existing_columns)} columns:\")\n",
    "    print(f\"  - New column order: {list(result_df.columns)}\")\n",
    "    \n",
    "    # Save to output file\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"✅ Saved reordered data to: {output_file}\")\n",
    "    \n",
    "    # Show summary\n",
    "    print(f\"\\n📈 Summary:\")\n",
    "    print(f\"  - Input rows: {len(df)}\")\n",
    "    print(f\"  - Output rows: {len(result_df)}\")\n",
    "    print(f\"  - Input columns: {len(df.columns)}\")\n",
    "    print(f\"  - Output columns: {len(result_df.columns)}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def preview_data(df, title, n_rows=3):\n",
    "    \"\"\"Preview sample data from a dataframe\"\"\"\n",
    "    print(f\"\\n📋 {title}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if df is None or len(df) == 0:\n",
    "        print(\"No data to display\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Show first few rows with truncated values for readability\n",
    "    print(f\"\\nFirst {min(n_rows, len(df))} rows:\")\n",
    "    for i in range(min(n_rows, len(df))):\n",
    "        print(f\"\\nRow {i+1}:\")\n",
    "        for col in df.columns:\n",
    "            val = df.iloc[i][col]\n",
    "            if pd.isna(val) or val == '':\n",
    "                print(f\"  {col}: [EMPTY]\")\n",
    "            else:\n",
    "                val_str = str(val)\n",
    "                # Truncate long values for display\n",
    "                if len(val_str) > 80:\n",
    "                    val_str = val_str[:77] + \"...\"\n",
    "                print(f\"  {col}: {val_str}\")\n",
    "\n",
    "# =====================================================\n",
    "# CONFIGURATION - EDIT THESE SETTINGS\n",
    "# =====================================================\n",
    "\n",
    "# Input file path\n",
    "INPUT_FILE = \"/data/users2/nblair7/analysis_results/MERGESDSCORES.csv\"  # Change this to your input file\n",
    "\n",
    "# Output file path\n",
    "OUTPUT_FILE = \"/data/users2/nblair7/analysis_results/MERGEDS.csv\"\n",
    "\n",
    "# Desired columns in the exact order you want them\n",
    "DESIRED_COLUMNS = [\n",
    "    \"participant_id\",\n",
    "    \"coherence\",\n",
    "    \"coherence_explanation\", \n",
    "    \"completeness\",\n",
    "    \"completeness_explanation\",\n",
    "    \"specificity\", \n",
    "    \"specificity_explanation\",\n",
    "    \"accuracy\",\n",
    "    \"accuracy_explanation\"\n",
    "]\n",
    "\n",
    "# Alternative: If you want to try the \"final_\" versions as backup, uncomment this:\n",
    "# BACKUP_COLUMN_MAPPING = {\n",
    "#     \"coherence\": [\"coherence\", \"final_coherence_score\"],\n",
    "#     \"coherence_explanation\": [\"coherence_explanation\", \"final_coherence_explanation\"],\n",
    "#     \"completeness\": [\"completeness\", \"final_completeness_score\"],\n",
    "#     \"completeness_explanation\": [\"completeness_explanation\", \"final_completeness_explanation\"],\n",
    "#     \"specificity\": [\"specificity\", \"final_specificity_score\"],\n",
    "#     \"specificity_explanation\": [\"specificity_explanation\", \"final_specificity_explanation\"],\n",
    "#     \"accuracy\": [\"accuracy\", \"final_accuracy_score\"],\n",
    "#     \"accuracy_explanation\": [\"accuracy_explanation\", \"final_accuracy_explanation\"]\n",
    "# }\n",
    "\n",
    "# =====================================================\n",
    "# MAIN EXECUTION\n",
    "# =====================================================\n",
    "\n",
    "def main():\n",
    "    print(\"CSV Column Reorder and Selection Tool\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"Input file: {INPUT_FILE}\")\n",
    "    print(f\"Output file: {OUTPUT_FILE}\")\n",
    "    print(f\"Desired columns: {DESIRED_COLUMNS}\")\n",
    "    \n",
    "    # Check if input file exists\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"❌ Error: Input file '{INPUT_FILE}' not found!\")\n",
    "        print(\"Please check the file path and try again.\")\n",
    "        return\n",
    "    \n",
    "    # Show original data structure\n",
    "    try:\n",
    "        original_df = pd.read_csv(INPUT_FILE)\n",
    "        original_df.columns = original_df.columns.str.strip()\n",
    "        preview_data(original_df, \"ORIGINAL DATA STRUCTURE\", 2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error previewing original data: {e}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"PROCESSING...\")\n",
    "    \n",
    "    # Reorder the columns\n",
    "    result_df = reorder_csv_columns(INPUT_FILE, OUTPUT_FILE, DESIRED_COLUMNS)\n",
    "    \n",
    "    if result_df is not None:\n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"RESULTS:\")\n",
    "        preview_data(result_df, \"FINAL REORDERED DATA\", 3)\n",
    "        \n",
    "        print(f\"\\n✅ SUCCESS!\")\n",
    "        print(f\"Your reordered CSV has been saved as: {OUTPUT_FILE}\")\n",
    "        print(f\"The file now has {len(result_df.columns)} columns in your desired order.\")\n",
    "        \n",
    "        # Show the header that was created\n",
    "        print(f\"\\n📋 New header line:\")\n",
    "        print(\",\".join(result_df.columns))\n",
    "    else:\n",
    "        print(f\"\\n❌ FAILED to create reordered file.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "116e64de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Append with Replacement Tool\n",
      "============================================================\n",
      "File 1 (base): /data/users2/nblair7/analysis_results/SCORES_R.csv\n",
      "File 2 (replacement/additional): /data/users2/nblair7/analysis_results/eval_results_new.csv\n",
      "Join column: participant_id\n",
      "Output file: /data/users2/nblair7/analysis_results/newfindatascores.csv\n",
      "\n",
      "🔍 SAMPLE REPLACEMENTS (showing up to 3 examples)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Example 1: Participant 385\n",
      "  Will replace File 1 data with File 2 data:\n",
      "    specificity_explanation:\n",
      "      OLD: Score: 2\n",
      "Explanation: The assessment ...\n",
      "      NEW: Score: 4\n",
      "Explanation: While the asses...\n",
      "    completeness:\n",
      "      OLD: 1.0\n",
      "      NEW: 4\n",
      "    accuracy:\n",
      "      OLD: 2.0\n",
      "      NEW: 3\n",
      "    completeness_explanation:\n",
      "      OLD: Score: 1\n",
      "Explanation: This assessment...\n",
      "      NEW: Score: 4\n",
      "Explanation: The assessment ...\n",
      "    specificity:\n",
      "      OLD: 2.0\n",
      "      NEW: 4\n",
      "    ... and 3 more columns\n",
      "\n",
      "Example 2: Participant 386\n",
      "  Will replace File 1 data with File 2 data:\n",
      "    specificity_explanation:\n",
      "      OLD: Score: 2\n",
      "Explanation: The assessment ...\n",
      "      NEW: Score: 5\n",
      "Explanation: The assessment ...\n",
      "    completeness:\n",
      "      OLD: 1.0\n",
      "      NEW: 4\n",
      "    accuracy:\n",
      "      OLD: 1.0\n",
      "      NEW: 5\n",
      "    completeness_explanation:\n",
      "      OLD: Score: 1\n",
      "Explanation: This transcript...\n",
      "      NEW: Score: 4\n",
      "Explanation: The assessment ...\n",
      "    specificity:\n",
      "      OLD: 2.0\n",
      "      NEW: 5\n",
      "    ... and 3 more columns\n",
      "\n",
      "Example 3: Participant 388\n",
      "  Will replace File 1 data with File 2 data:\n",
      "    specificity_explanation:\n",
      "      OLD: Score: 4\n",
      "Explanation: While the asses...\n",
      "      NEW: Score: 3\n",
      "Explanation: The assessment,...\n",
      "    completeness:\n",
      "      OLD: 4.0\n",
      "      NEW: 4\n",
      "    accuracy:\n",
      "      OLD: 4.0\n",
      "      NEW: 4\n",
      "    completeness_explanation:\n",
      "      OLD: Score: 4\n",
      "Explanation: The assessment ...\n",
      "      NEW: Score: 4\n",
      "Explanation: The assessment ...\n",
      "    specificity:\n",
      "      OLD: 4.0\n",
      "      NEW: 3\n",
      "    ... and 3 more columns\n",
      "\n",
      "============================================================\n",
      "PROCESSING...\n",
      "CSV Append with Replacement Tool\n",
      "==================================================\n",
      "Reading File 1 (base): /data/users2/nblair7/analysis_results/SCORES_R.csv\n",
      "  - Loaded 51 rows, 9 columns\n",
      "Reading File 2 (replacement/additional): /data/users2/nblair7/analysis_results/eval_results_new.csv\n",
      "  - Loaded 142 rows, 9 columns\n",
      "\n",
      "📊 Column Analysis:\n",
      "  Common columns: 9\n",
      "    ['specificity_explanation', 'participant_id', 'completeness', 'accuracy', 'completeness_explanation', 'specificity', 'coherence', 'coherence_explanation', 'accuracy_explanation']\n",
      "\n",
      "👥 Participant Analysis:\n",
      "  File 1 participants: 51\n",
      "  File 2 participants: 142\n",
      "  Overlapping (will be replaced): 51\n",
      "  File 1 only (will be kept): 0\n",
      "  File 2 only (will be added): 91\n",
      "  Overlapping participant IDs: [np.int64(303), np.int64(304), np.int64(307), np.int64(310), np.int64(312), np.int64(320), np.int64(324), np.int64(326), np.int64(330), np.int64(335), np.int64(336), np.int64(340), np.int64(343), np.int64(344), np.int64(345), np.int64(347), np.int64(351), np.int64(357), np.int64(364), np.int64(367), np.int64(368), np.int64(371), np.int64(372), np.int64(380), np.int64(383), np.int64(385), np.int64(386), np.int64(388), np.int64(389), np.int64(391), np.int64(392), np.int64(393), np.int64(397), np.int64(402), np.int64(414), np.int64(415), np.int64(418), np.int64(419), np.int64(420), np.int64(425), np.int64(433), np.int64(443), np.int64(447), np.int64(451), np.int64(458), np.int64(464), np.int64(479), np.int64(483), np.int64(485), np.int64(487), np.int64(491)]\n",
      "\n",
      "🔄 Processing...\n",
      "  Step 1: Removed 51 overlapping rows from File 1\n",
      "  Step 2: Standardizing columns - total columns in result: 9\n",
      "  Step 3: Combined dataframes - final result: 142 rows\n",
      "  Step 4: Sorted by participant_id (numeric)\n",
      "\n",
      "✅ Saved combined data to: /data/users2/nblair7/analysis_results/newfindatascores.csv\n",
      "\n",
      "📋 Final Summary:\n",
      "  Total rows: 142\n",
      "  Total columns: 9\n",
      "  Replacements made: 51\n",
      "  New participants added: 91\n",
      "  Participants kept from File 1: 0\n",
      "\n",
      "============================================================\n",
      "✅ SUCCESS!\n",
      "\n",
      "Sample of final combined data:\n",
      "First 3 rows:\n",
      "  Row 1: Participant 302\n",
      "  Row 2: Participant 303\n",
      "  Row 3: Participant 304\n",
      "\n",
      "Final file saved as: /data/users2/nblair7/analysis_results/newfindatascores.csv\n",
      "Total participants in final file: 142\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def append_csvs_with_replacement(file1, file2, join_column=\"participant_id\", output_file=\"combined_data.csv\"):\n",
    "    \"\"\"\n",
    "    Append two CSV files where file2 data replaces any matching rows from file1.\n",
    "    \n",
    "    Parameters:\n",
    "    file1 (str): Path to the first CSV file (base data)\n",
    "    file2 (str): Path to the second CSV file (replacement/additional data)\n",
    "    join_column (str): Column to match on for replacements (default: 'participant_id')\n",
    "    output_file (str): Path for the output CSV file\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The combined dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"CSV Append with Replacement Tool\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Read both files\n",
    "    try:\n",
    "        print(f\"Reading File 1 (base): {file1}\")\n",
    "        df1 = pd.read_csv(file1)\n",
    "        df1.columns = df1.columns.str.strip()\n",
    "        print(f\"  - Loaded {len(df1)} rows, {len(df1.columns)} columns\")\n",
    "        \n",
    "        print(f\"Reading File 2 (replacement/additional): {file2}\")\n",
    "        df2 = pd.read_csv(file2)\n",
    "        df2.columns = df2.columns.str.strip()\n",
    "        print(f\"  - Loaded {len(df2)} rows, {len(df2.columns)} columns\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading files: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Check if join column exists in both files\n",
    "    if join_column not in df1.columns:\n",
    "        print(f\"Error: '{join_column}' not found in File 1\")\n",
    "        print(f\"Available columns in File 1: {list(df1.columns)}\")\n",
    "        return None\n",
    "    \n",
    "    if join_column not in df2.columns:\n",
    "        print(f\"Error: '{join_column}' not found in File 2\")\n",
    "        print(f\"Available columns in File 2: {list(df2.columns)}\")\n",
    "        return None\n",
    "    \n",
    "    # Show column comparison\n",
    "    print(f\"\\n📊 Column Analysis:\")\n",
    "    file1_cols = set(df1.columns)\n",
    "    file2_cols = set(df2.columns)\n",
    "    \n",
    "    common_cols = file1_cols & file2_cols\n",
    "    file1_only = file1_cols - file2_cols\n",
    "    file2_only = file2_cols - file1_cols\n",
    "    \n",
    "    print(f\"  Common columns: {len(common_cols)}\")\n",
    "    if common_cols:\n",
    "        print(f\"    {list(common_cols)}\")\n",
    "    \n",
    "    if file1_only:\n",
    "        print(f\"  File 1 only: {len(file1_only)}\")\n",
    "        print(f\"    {list(file1_only)}\")\n",
    "    \n",
    "    if file2_only:\n",
    "        print(f\"  File 2 only: {len(file2_only)}\")\n",
    "        print(f\"    {list(file2_only)}\")\n",
    "    \n",
    "    # Find overlapping participants\n",
    "    participants_file1 = set(df1[join_column].values)\n",
    "    participants_file2 = set(df2[join_column].values)\n",
    "    \n",
    "    overlapping = participants_file1 & participants_file2\n",
    "    file1_only_participants = participants_file1 - participants_file2\n",
    "    file2_only_participants = participants_file2 - participants_file1\n",
    "    \n",
    "    print(f\"\\n👥 Participant Analysis:\")\n",
    "    print(f\"  File 1 participants: {len(participants_file1)}\")\n",
    "    print(f\"  File 2 participants: {len(participants_file2)}\")\n",
    "    print(f\"  Overlapping (will be replaced): {len(overlapping)}\")\n",
    "    print(f\"  File 1 only (will be kept): {len(file1_only_participants)}\")\n",
    "    print(f\"  File 2 only (will be added): {len(file2_only_participants)}\")\n",
    "    \n",
    "    if overlapping:\n",
    "        print(f\"  Overlapping participant IDs: {sorted(list(overlapping))}\")\n",
    "    \n",
    "    # Step 1: Remove overlapping participants from file1\n",
    "    print(f\"\\n🔄 Processing...\")\n",
    "    df1_filtered = df1[~df1[join_column].isin(participants_file2)].copy()\n",
    "    print(f\"  Step 1: Removed {len(df1) - len(df1_filtered)} overlapping rows from File 1\")\n",
    "    \n",
    "    # Step 2: Combine the dataframes\n",
    "    # First ensure both dataframes have the same columns\n",
    "    all_columns = sorted(list(file1_cols | file2_cols))\n",
    "    print(f\"  Step 2: Standardizing columns - total columns in result: {len(all_columns)}\")\n",
    "    \n",
    "    # Add missing columns with NaN values\n",
    "    for col in all_columns:\n",
    "        if col not in df1_filtered.columns:\n",
    "            df1_filtered[col] = pd.NA\n",
    "        if col not in df2.columns:\n",
    "            df2[col] = pd.NA\n",
    "    \n",
    "    # Reorder columns to match\n",
    "    df1_filtered = df1_filtered[all_columns]\n",
    "    df2 = df2[all_columns]\n",
    "    \n",
    "    # Step 3: Concatenate\n",
    "    result_df = pd.concat([df1_filtered, df2], ignore_index=True)\n",
    "    print(f\"  Step 3: Combined dataframes - final result: {len(result_df)} rows\")\n",
    "    \n",
    "    # Step 4: Sort by participant_id if it's numeric\n",
    "    try:\n",
    "        result_df[f'{join_column}_numeric'] = pd.to_numeric(result_df[join_column], errors='coerce')\n",
    "        if not result_df[f'{join_column}_numeric'].isna().all():\n",
    "            result_df = result_df.sort_values(f'{join_column}_numeric', na_position='last')\n",
    "            print(f\"  Step 4: Sorted by {join_column} (numeric)\")\n",
    "        else:\n",
    "            result_df = result_df.sort_values(join_column, key=lambda x: x.astype(str))\n",
    "            print(f\"  Step 4: Sorted by {join_column} (alphabetic)\")\n",
    "        \n",
    "        # Drop the temporary numeric column\n",
    "        if f'{join_column}_numeric' in result_df.columns:\n",
    "            result_df = result_df.drop(f'{join_column}_numeric', axis=1)\n",
    "    except:\n",
    "        result_df = result_df.sort_values(join_column, key=lambda x: x.astype(str))\n",
    "        print(f\"  Step 4: Sorted by {join_column}\")\n",
    "    \n",
    "    # Save result\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n✅ Saved combined data to: {output_file}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n📋 Final Summary:\")\n",
    "    print(f\"  Total rows: {len(result_df)}\")\n",
    "    print(f\"  Total columns: {len(result_df.columns)}\")\n",
    "    print(f\"  Replacements made: {len(overlapping)}\")\n",
    "    print(f\"  New participants added: {len(file2_only_participants)}\")\n",
    "    print(f\"  Participants kept from File 1: {len(file1_only_participants)}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def show_sample_replacements(file1, file2, join_column=\"participant_id\", n_samples=3):\n",
    "    \"\"\"\n",
    "    Show examples of what replacements will be made\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 SAMPLE REPLACEMENTS (showing up to {n_samples} examples)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        df1 = pd.read_csv(file1)\n",
    "        df1.columns = df1.columns.str.strip()\n",
    "        df2 = pd.read_csv(file2)\n",
    "        df2.columns = df2.columns.str.strip()\n",
    "        \n",
    "        # Find overlapping participants\n",
    "        participants_file1 = set(df1[join_column].values)\n",
    "        participants_file2 = set(df2[join_column].values)\n",
    "        overlapping = list(participants_file1 & participants_file2)\n",
    "        \n",
    "        if not overlapping:\n",
    "            print(\"No overlapping participants found - this will be a simple append.\")\n",
    "            return\n",
    "        \n",
    "        # Show a few examples\n",
    "        for i, participant_id in enumerate(overlapping[:n_samples]):\n",
    "            print(f\"\\nExample {i+1}: Participant {participant_id}\")\n",
    "            \n",
    "            # Get data from both files\n",
    "            row1 = df1[df1[join_column] == participant_id].iloc[0]\n",
    "            row2 = df2[df2[join_column] == participant_id].iloc[0]\n",
    "            \n",
    "            # Find common columns\n",
    "            common_cols = list(set(df1.columns) & set(df2.columns))\n",
    "            common_cols = [col for col in common_cols if col != join_column]\n",
    "            \n",
    "            print(f\"  Will replace File 1 data with File 2 data:\")\n",
    "            for col in common_cols[:5]:  # Show first 5 columns\n",
    "                val1 = row1[col] if col in df1.columns else \"[Missing]\"\n",
    "                val2 = row2[col] if col in df2.columns else \"[Missing]\"\n",
    "                \n",
    "                # Truncate long values\n",
    "                if pd.notna(val1) and len(str(val1)) > 40:\n",
    "                    val1 = str(val1)[:37] + \"...\"\n",
    "                if pd.notna(val2) and len(str(val2)) > 40:\n",
    "                    val2 = str(val2)[:37] + \"...\"\n",
    "                \n",
    "                print(f\"    {col}:\")\n",
    "                print(f\"      OLD: {val1}\")\n",
    "                print(f\"      NEW: {val2}\")\n",
    "            \n",
    "            if len(common_cols) > 5:\n",
    "                print(f\"    ... and {len(common_cols) - 5} more columns\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error showing sample replacements: {e}\")\n",
    "\n",
    "# =====================================================\n",
    "# CONFIGURATION - EDIT THESE SETTINGS\n",
    "# =====================================================\n",
    "\n",
    "# File paths\n",
    "FILE2 = \"/data/users2/nblair7/analysis_results/eval_results_new.csv\"      # Base file (data will be kept unless replaced)\n",
    "FILE1 = \"/data/users2/nblair7/analysis_results/SCORES_R.csv\"         # Replacement/additional file (this data takes priority)\n",
    "\n",
    "# Join column for matching rows\n",
    "JOIN_COLUMN = \"participant_id\"\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = \"/data/users2/nblair7/analysis_results/newfindatascores.csv\"\n",
    "\n",
    "# =====================================================\n",
    "# MAIN EXECUTION\n",
    "# =====================================================\n",
    "\n",
    "def main():\n",
    "    print(\"CSV Append with Replacement Tool\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"File 1 (base): {FILE1}\")\n",
    "    print(f\"File 2 (replacement/additional): {FILE2}\")\n",
    "    print(f\"Join column: {JOIN_COLUMN}\")\n",
    "    print(f\"Output file: {OUTPUT_FILE}\")\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(FILE1):\n",
    "        print(f\"❌ Error: File 1 '{FILE1}' not found!\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(FILE2):\n",
    "        print(f\"❌ Error: File 2 '{FILE2}' not found!\")\n",
    "        return\n",
    "    \n",
    "    # Show what replacements will be made\n",
    "    show_sample_replacements(FILE1, FILE2, JOIN_COLUMN)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"PROCESSING...\")\n",
    "    \n",
    "    # Perform the append with replacement\n",
    "    result = append_csvs_with_replacement(FILE1, FILE2, JOIN_COLUMN, OUTPUT_FILE)\n",
    "    \n",
    "    if result is not None:\n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"✅ SUCCESS!\")\n",
    "        \n",
    "        # Show sample of final data\n",
    "        print(f\"\\nSample of final combined data:\")\n",
    "        print(f\"First 3 rows:\")\n",
    "        for i in range(min(3, len(result))):\n",
    "            participant_id = result.iloc[i][JOIN_COLUMN]\n",
    "            print(f\"  Row {i+1}: Participant {participant_id}\")\n",
    "            \n",
    "        print(f\"\\nFinal file saved as: {OUTPUT_FILE}\")\n",
    "        print(f\"Total participants in final file: {len(result)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92591290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def append_csvs_with_replacement(file1, file2, join_column=\"participant_id\", output_file=\"combined_data.csv\"):\n",
    "    \"\"\"\n",
    "    Append two CSV files where file2 data replaces any matching rows from file1.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"CSV Append with Replacement Tool\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Read both files\n",
    "    try:\n",
    "        print(f\"Reading File 1 (base): {file1}\")\n",
    "        df1 = pd.read_csv(file1)\n",
    "        df1.columns = df1.columns.str.strip()\n",
    "        print(f\"  - Loaded {len(df1)} rows, {len(df1.columns)} columns\")\n",
    "        \n",
    "        print(f\"Reading File 2 (replacement/additional): {file2}\")\n",
    "        df2 = pd.read_csv(file2)\n",
    "        df2.columns = df2.columns.str.strip()\n",
    "        print(f\"  - Loaded {len(df2)} rows, {len(df2.columns)} columns\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading files: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Check if join column exists in both files\n",
    "    if join_column not in df1.columns:\n",
    "        print(f\"Error: '{join_column}' not found in File 1\")\n",
    "        print(f\"Available columns in File 1: {list(df1.columns)}\")\n",
    "        return None\n",
    "    \n",
    "    if join_column not in df2.columns:\n",
    "        print(f\"Error: '{join_column}' not found in File 2\")\n",
    "        print(f\"Available columns in File 2: {list(df2.columns)}\")\n",
    "        return None\n",
    "    \n",
    "    # Show column comparison\n",
    "    print(f\"\\n📊 Column Analysis:\")\n",
    "    file1_cols = set(df1.columns)\n",
    "    file2_cols = set(df2.columns)\n",
    "    \n",
    "    common_cols = file1_cols & file2_cols\n",
    "    file1_only = file1_cols - file2_cols\n",
    "    file2_only = file2_cols - file1_cols\n",
    "    \n",
    "    print(f\"  Common columns: {len(common_cols)}\")\n",
    "    if common_cols:\n",
    "        print(f\"    {list(common_cols)}\")\n",
    "    \n",
    "    if file1_only:\n",
    "        print(f\"  File 1 only: {len(file1_only)}\")\n",
    "        print(f\"    {list(file1_only)}\")\n",
    "    \n",
    "    if file2_only:\n",
    "        print(f\"  File 2 only: {len(file2_only)}\")\n",
    "        print(f\"    {list(file2_only)}\")\n",
    "    \n",
    "    # Find overlapping participants\n",
    "    participants_file1 = set(df1[join_column].values)\n",
    "    participants_file2 = set(df2[join_column].values)\n",
    "    \n",
    "    overlapping = participants_file1 & participants_file2\n",
    "    file1_only_participants = participants_file1 - participants_file2\n",
    "    file2_only_participants = participants_file2 - participants_file1\n",
    "    \n",
    "    print(f\"\\n👥 Participant Analysis:\")\n",
    "    print(f\"  File 1 participants: {len(participants_file1)}\")\n",
    "    print(f\"  File 2 participants: {len(participants_file2)}\")\n",
    "    print(f\"  Overlapping (will be replaced): {len(overlapping)}\")\n",
    "    print(f\"  File 1 only (will be kept): {len(file1_only_participants)}\")\n",
    "    print(f\"  File 2 only (will be added): {len(file2_only_participants)}\")\n",
    "    \n",
    "    if overlapping:\n",
    "        print(f\"  Overlapping participant IDs: {sorted(list(overlapping))}\")\n",
    "    \n",
    "    # Step 1: Remove overlapping participants from file1\n",
    "    print(f\"\\n🔄 Processing...\")\n",
    "    df1_filtered = df1[~df1[join_column].isin(participants_file2)].copy()\n",
    "    print(f\"  Step 1: Removed {len(df1) - len(df1_filtered)} overlapping rows from File 1\")\n",
    "    \n",
    "    # Step 2: Standardize columns - ensure participant_id comes first\n",
    "    all_columns = [join_column] + sorted([col for col in (file1_cols | file2_cols) if col != join_column])\n",
    "    print(f\"  Step 2: Standardizing columns - total columns in result: {len(all_columns)}\")\n",
    "    \n",
    "    # Add missing columns with NaN values\n",
    "    for col in all_columns:\n",
    "        if col not in df1_filtered.columns:\n",
    "            df1_filtered[col] = pd.NA\n",
    "        if col not in df2.columns:\n",
    "            df2[col] = pd.NA\n",
    "    \n",
    "    # Reorder columns\n",
    "    df1_filtered = df1_filtered[all_columns]\n",
    "    df2 = df2[all_columns]\n",
    "    \n",
    "    # Step 3: Concatenate\n",
    "    result_df = pd.concat([df1_filtered, df2], ignore_index=True)\n",
    "    print(f\"  Step 3: Combined dataframes - final result: {len(result_df)} rows\")\n",
    "    \n",
    "    # Step 4: Sort by participant_id if it's numeric\n",
    "    try:\n",
    "        result_df[f'{join_column}_numeric'] = pd.to_numeric(result_df[join_column], errors='coerce')\n",
    "        if not result_df[f'{join_column}_numeric'].isna().all():\n",
    "            result_df = result_df.sort_values(f'{join_column}_numeric', na_position='last')\n",
    "            print(f\"  Step 4: Sorted by {join_column} (numeric)\")\n",
    "        else:\n",
    "            result_df = result_df.sort_values(join_column, key=lambda x: x.astype(str))\n",
    "            print(f\"  Step 4: Sorted by {join_column} (alphabetic)\")\n",
    "        \n",
    "        # Drop the temporary numeric column\n",
    "        if f'{join_column}_numeric' in result_df.columns:\n",
    "            result_df = result_df.drop(f'{join_column}_numeric', axis=1)\n",
    "    except:\n",
    "        result_df = result_df.sort_values(join_column, key=lambda x: x.astype(str))\n",
    "        print(f\"  Step 4: Sorted by {join_column}\")\n",
    "    \n",
    "    # Save result\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n✅ Saved combined data to: {output_file}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n📋 Final Summary:\")\n",
    "    print(f\"  Total rows: {len(result_df)}\")\n",
    "    print(f\"  Total columns: {len(result_df.columns)}\")\n",
    "    print(f\"  Replacements made: {len(overlapping)}\")\n",
    "    print(f\"  New participants added: {len(file2_only_participants)}\")\n",
    "    print(f\"  Participants kept from File 1: {len(file1_only_participants)}\")\n",
    "    \n",
    "    return result_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipsy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
