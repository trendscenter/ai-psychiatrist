{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38cad4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOW SCORE CASES ANALYZER\n",
      "================================================================================\n",
      "Input file: /data/users2/nblair7/analysis_results/eval_results_new.csv\n",
      "Output CSV: /data/users2/nblair7/analysis_results/low_score_cases.csv\n",
      "Report file: /data/users2/nblair7/analysis_results/low_score_analysis_report.txt\n",
      "Score threshold: ≤ 2\n",
      "\n",
      "Step 1: Extracting low score cases...\n",
      "=== LOW SCORE EXTRACTOR (Threshold: ≤2) ===\n",
      "Reading from: /data/users2/nblair7/analysis_results/eval_results_new.csv\n",
      "Output will be saved to: /data/users2/nblair7/analysis_results/low_score_cases.csv\n",
      "Loaded 142 participants from input file\n",
      "Columns found: ['participant_id', 'coherence', 'coherence_explanation', 'completeness', 'completeness_explanation', 'specificity', 'specificity_explanation', 'accuracy', 'accuracy_explanation']\n",
      "\n",
      "Scanning for low scores...\n",
      "  Found: Participant 326, completeness = 2\n",
      "  Found: Participant 326, specificity = 2\n",
      "  Found: Participant 330, specificity = 2\n",
      "  Found: Participant 336, accuracy = 2\n",
      "  Found: Participant 340, completeness = 2\n",
      "  Found: Participant 343, specificity = 2\n",
      "  Found: Participant 371, specificity = 2\n",
      "  Found: Participant 391, specificity = 2\n",
      "  Found: Participant 443, specificity = 2\n",
      "  Found: Participant 479, specificity = 2\n",
      "  Found: Participant 485, specificity = 2\n",
      "  Found: Participant 307, specificity = 2\n",
      "  Found: Participant 420, specificity = 2\n",
      "  Found: Participant 451, specificity = 2\n",
      "  Found: Participant 458, specificity = 2\n",
      "\n",
      "=== SUMMARY STATISTICS ===\n",
      "Total participants with low scores (≤2): 14\n",
      "Total low score instances: 15\n",
      "\n",
      "Breakdown by metric:\n",
      "  specificity: 12 cases (80.0%)\n",
      "  completeness: 2 cases (13.3%)\n",
      "  accuracy: 1 cases (6.7%)\n",
      "\n",
      "Breakdown by score:\n",
      "  Score 2: 15 cases (100.0%)\n",
      "\n",
      "Participants with multiple low scores: 1\n",
      "Most problematic participants:\n",
      "  Participant 326: 2 low scores (completeness, specificity)\n",
      "  Participant 307: 1 low scores (specificity)\n",
      "  Participant 330: 1 low scores (specificity)\n",
      "  Participant 336: 1 low scores (accuracy)\n",
      "  Participant 340: 1 low scores (completeness)\n",
      "  Participant 343: 1 low scores (specificity)\n",
      "  Participant 371: 1 low scores (specificity)\n",
      "  Participant 391: 1 low scores (specificity)\n",
      "  Participant 420: 1 low scores (specificity)\n",
      "  Participant 443: 1 low scores (specificity)\n",
      "\n",
      "Low score cases saved to: /data/users2/nblair7/analysis_results/low_score_cases.csv\n",
      "CSV columns: participant_id, metric, score, explanation\n",
      "\n",
      "Step 2: Analyzing explanation patterns...\n",
      "\n",
      "=== ANALYZING EXPLANATIONS BY METRIC ===\n",
      "\n",
      "--- COMPLETENESS ANALYSIS (2 cases) ---\n",
      "  Total cases: 2\n",
      "  Average score: 2.00\n",
      "  Score distribution: {2: np.int64(2)}\n",
      "  Most common issue indicators:\n",
      "    'PHQ-8': appears in 2/2 explanations (100.0%)\n",
      "    'symptoms': appears in 2/2 explanations (100.0%)\n",
      "    'lacks': appears in 1/2 explanations (50.0%)\n",
      "    'duration': appears in 1/2 explanations (50.0%)\n",
      "    'frequency': appears in 1/2 explanations (50.0%)\n",
      "\n",
      "--- SPECIFICITY ANALYSIS (12 cases) ---\n",
      "  Total cases: 12\n",
      "  Average score: 2.00\n",
      "  Score distribution: {2: np.int64(12)}\n",
      "  Most common issue indicators:\n",
      "    'specific': appears in 10/12 explanations (83.3%)\n",
      "    'general': appears in 6/12 explanations (50.0%)\n",
      "    'broad': appears in 4/12 explanations (33.3%)\n",
      "    'vague': appears in 3/12 explanations (25.0%)\n",
      "    'superficial': appears in 1/12 explanations (8.3%)\n",
      "\n",
      "--- ACCURACY ANALYSIS (1 cases) ---\n",
      "  Total cases: 1\n",
      "  Average score: 2.00\n",
      "  Score distribution: {2: np.int64(1)}\n",
      "  No common keywords found\n",
      "\n",
      "Step 3: Creating comprehensive report...\n",
      "\n",
      "Creating detailed analysis report...\n",
      "Detailed analysis report saved to: /data/users2/nblair7/analysis_results/low_score_analysis_report.txt\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "Files created:\n",
      "1. Low score cases CSV: /data/users2/nblair7/analysis_results/low_score_cases.csv\n",
      "   - Contains 15 low score cases\n",
      "   - Columns: participant_id, metric, score, explanation\n",
      "2. Analysis report: /data/users2/nblair7/analysis_results/low_score_analysis_report.txt\n",
      "   - Comprehensive analysis with patterns and samples\n",
      "\n",
      "Key findings:\n",
      "- 14 participants have scores ≤ 2\n",
      "- 15 total low score instances\n",
      "- Most problematic metric: specificity (12 cases)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration - UPDATE THESE PATHS\n",
    "INPUT_CSV_PATH = \"/data/users2/nblair7/analysis_results/eval_results_new.csv\"  # TODO: Update with your evaluation results file\n",
    "OUTPUT_CSV_PATH = \"/data/users2/nblair7/analysis_results/low_score_cases.csv\"   # TODO: Update with desired output path\n",
    "ANALYSIS_REPORT_PATH = \"/data/users2/nblair7/analysis_results/low_score_analysis_report.txt\"  # TODO: Update with desired report path\n",
    "\n",
    "# Configuration\n",
    "THRESHOLD = 2  # Extract scores <= 2 (includes scores 1 and 2)\n",
    "\n",
    "def extract_low_scores(input_csv_path, output_csv_path, threshold=2):\n",
    "    \"\"\"\n",
    "    Extract participants with scores <= threshold and create analysis CSV\n",
    "    \n",
    "    Parameters:\n",
    "    - input_csv_path: Path to the input CSV file with evaluation results\n",
    "    - output_csv_path: Path to save the low score cases\n",
    "    - threshold: Score threshold (default 2, includes scores 1 and 2)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"=== LOW SCORE EXTRACTOR (Threshold: ≤{threshold}) ===\")\n",
    "    print(f\"Reading from: {input_csv_path}\")\n",
    "    print(f\"Output will be saved to: {output_csv_path}\")\n",
    "    \n",
    "    # Load the data\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "        print(f\"Loaded {len(df)} participants from input file\")\n",
    "        print(f\"Columns found: {list(df.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading input file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Define the scoring metrics\n",
    "    metrics = ['coherence', 'completeness', 'specificity', 'accuracy']\n",
    "    \n",
    "    # Initialize list to store low score cases\n",
    "    low_score_cases = []\n",
    "    \n",
    "    print(\"\\nScanning for low scores...\")\n",
    "    \n",
    "    # Process each participant\n",
    "    for index, row in df.iterrows():\n",
    "        participant_id = row['participant_id']\n",
    "        \n",
    "        # Check each metric\n",
    "        for metric in metrics:\n",
    "            score_col = metric\n",
    "            explanation_col = f\"{metric}_explanation\"\n",
    "            \n",
    "            # Check if columns exist\n",
    "            if score_col in df.columns and explanation_col in df.columns:\n",
    "                score = row[score_col]\n",
    "                explanation = row[explanation_col]\n",
    "                \n",
    "                # Check if score is low (≤ threshold)\n",
    "                if pd.notna(score) and score <= threshold:\n",
    "                    low_score_cases.append({\n",
    "                        'participant_id': participant_id,\n",
    "                        'metric': metric,\n",
    "                        'score': int(score),\n",
    "                        'explanation': explanation\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"  Found: Participant {participant_id}, {metric} = {score}\")\n",
    "            else:\n",
    "                if index == 0:  # Only warn once\n",
    "                    print(f\"  Warning: Column {score_col} or {explanation_col} not found\")\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    if low_score_cases:\n",
    "        low_scores_df = pd.DataFrame(low_score_cases)\n",
    "        low_scores_df.to_csv(output_csv_path, index=False)\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\n=== SUMMARY STATISTICS ===\")\n",
    "        print(f\"Total participants with low scores (≤{threshold}): {low_scores_df['participant_id'].nunique()}\")\n",
    "        print(f\"Total low score instances: {len(low_scores_df)}\")\n",
    "        \n",
    "        # Breakdown by metric\n",
    "        print(f\"\\nBreakdown by metric:\")\n",
    "        metric_counts = low_scores_df['metric'].value_counts()\n",
    "        for metric, count in metric_counts.items():\n",
    "            percentage = (count / len(low_scores_df)) * 100\n",
    "            print(f\"  {metric}: {count} cases ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Breakdown by score\n",
    "        print(f\"\\nBreakdown by score:\")\n",
    "        score_counts = low_scores_df['score'].value_counts().sort_index()\n",
    "        for score, count in score_counts.items():\n",
    "            percentage = (count / len(low_scores_df)) * 100\n",
    "            print(f\"  Score {score}: {count} cases ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Participants with multiple low scores\n",
    "        participant_low_counts = low_scores_df.groupby('participant_id').size()\n",
    "        multi_low_participants = participant_low_counts[participant_low_counts > 1]\n",
    "        \n",
    "        print(f\"\\nParticipants with multiple low scores: {len(multi_low_participants)}\")\n",
    "        \n",
    "        if len(multi_low_participants) > 0:\n",
    "            print(\"Most problematic participants:\")\n",
    "            top_problematic = participant_low_counts.sort_values(ascending=False).head(10)\n",
    "            for pid, count in top_problematic.items():\n",
    "                affected_metrics = low_scores_df[low_scores_df['participant_id'] == pid]['metric'].tolist()\n",
    "                print(f\"  Participant {pid}: {count} low scores ({', '.join(affected_metrics)})\")\n",
    "        \n",
    "        print(f\"\\nLow score cases saved to: {output_csv_path}\")\n",
    "        print(f\"CSV columns: participant_id, metric, score, explanation\")\n",
    "        \n",
    "        return low_scores_df\n",
    "        \n",
    "    else:\n",
    "        print(f\"No participants found with scores ≤ {threshold}\")\n",
    "        return None\n",
    "\n",
    "def analyze_explanations_by_metric(low_scores_df):\n",
    "    \"\"\"\n",
    "    Analyze explanations grouped by metric to find common patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    if low_scores_df is None or len(low_scores_df) == 0:\n",
    "        print(\"No low score data to analyze\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"\\n=== ANALYZING EXPLANATIONS BY METRIC ===\")\n",
    "    \n",
    "    analysis_results = {}\n",
    "    \n",
    "    # Define keywords to search for in each metric's explanations\n",
    "    metric_keywords = {\n",
    "        'coherence': [\n",
    "            'contradiction', 'inconsistent', 'logical', 'conflicting', 'unclear', \n",
    "            'contradicts', 'confusing', 'inconsistency', 'incoherent'\n",
    "        ],\n",
    "        'completeness': [\n",
    "            'missing', 'omitted', 'lacks', 'incomplete', 'PHQ-8', 'symptoms', \n",
    "            'duration', 'frequency', 'missed', 'absent', 'overlooks', 'fails to'\n",
    "        ],\n",
    "        'specificity': [\n",
    "            'vague', 'generic', 'general', 'specific', 'seems', 'appears', \n",
    "            'unclear', 'broad', 'superficial', 'lacks detail'\n",
    "        ],\n",
    "        'accuracy': [\n",
    "            'incorrect', 'wrong', 'DSM-5', 'PHQ-8', 'inaccurate', 'misaligned', \n",
    "            'error', 'mistaken', 'false', 'erroneous'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for metric in low_scores_df['metric'].unique():\n",
    "        metric_data = low_scores_df[low_scores_df['metric'] == metric]\n",
    "        explanations = metric_data['explanation'].tolist()\n",
    "        \n",
    "        print(f\"\\n--- {metric.upper()} ANALYSIS ({len(metric_data)} cases) ---\")\n",
    "        \n",
    "        # Count keyword occurrences in explanations\n",
    "        keyword_counts = {}\n",
    "        if metric in metric_keywords:\n",
    "            for keyword in metric_keywords[metric]:\n",
    "                count = sum(1 for exp in explanations if keyword.lower() in str(exp).lower())\n",
    "                if count > 0:\n",
    "                    keyword_counts[keyword] = count\n",
    "        \n",
    "        # Score distribution for this metric\n",
    "        score_dist = metric_data['score'].value_counts().sort_index()\n",
    "        \n",
    "        # Get sample explanations (first 3 different ones)\n",
    "        sample_explanations = []\n",
    "        for i in range(min(3, len(explanations))):\n",
    "            participant_id = metric_data.iloc[i]['participant_id']\n",
    "            score = metric_data.iloc[i]['score']\n",
    "            explanation = explanations[i]\n",
    "            sample_explanations.append({\n",
    "                'participant_id': participant_id,\n",
    "                'score': score,\n",
    "                'explanation': explanation[:300] + \"...\" if len(explanation) > 300 else explanation\n",
    "            })\n",
    "        \n",
    "        # Store results\n",
    "        analysis_results[metric] = {\n",
    "            'total_cases': len(metric_data),\n",
    "            'score_distribution': dict(score_dist),\n",
    "            'avg_score': metric_data['score'].mean(),\n",
    "            'keyword_frequencies': keyword_counts,\n",
    "            'sample_explanations': sample_explanations,\n",
    "            'participants': metric_data['participant_id'].tolist()\n",
    "        }\n",
    "        \n",
    "        # Print summary for this metric\n",
    "        print(f\"  Total cases: {len(metric_data)}\")\n",
    "        print(f\"  Average score: {metric_data['score'].mean():.2f}\")\n",
    "        print(f\"  Score distribution: {dict(score_dist)}\")\n",
    "        \n",
    "        if keyword_counts:\n",
    "            # Sort keywords by frequency\n",
    "            sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            print(f\"  Most common issue indicators:\")\n",
    "            for keyword, count in sorted_keywords[:5]:\n",
    "                print(f\"    '{keyword}': appears in {count}/{len(metric_data)} explanations ({count/len(metric_data)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"  No common keywords found\")\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "def create_detailed_report(low_scores_df, analysis_results, report_path):\n",
    "    \"\"\"\n",
    "    Create a comprehensive text report with all findings\n",
    "    \"\"\"\n",
    "    \n",
    "    if low_scores_df is None:\n",
    "        print(\"No data available to create report\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nCreating detailed analysis report...\")\n",
    "    \n",
    "    try:\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"LOW SCORE ANALYSIS REPORT\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            f.write(f\"Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Threshold: Scores ≤ {THRESHOLD}\\n\\n\")\n",
    "            \n",
    "            # EXECUTIVE SUMMARY\n",
    "            f.write(\"EXECUTIVE SUMMARY\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Total participants with low scores: {low_scores_df['participant_id'].nunique()}\\n\")\n",
    "            f.write(f\"Total low score instances: {len(low_scores_df)}\\n\")\n",
    "            f.write(f\"Metrics analyzed: {', '.join(low_scores_df['metric'].unique())}\\n\\n\")\n",
    "            \n",
    "            # OVERALL STATISTICS\n",
    "            f.write(\"OVERALL STATISTICS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            # Metric breakdown\n",
    "            metric_counts = low_scores_df['metric'].value_counts()\n",
    "            f.write(\"Issues by metric:\\n\")\n",
    "            for metric, count in metric_counts.items():\n",
    "                percentage = (count / len(low_scores_df)) * 100\n",
    "                f.write(f\"  {metric.capitalize()}: {count} cases ({percentage:.1f}%)\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Score distribution\n",
    "            score_counts = low_scores_df['score'].value_counts().sort_index()\n",
    "            f.write(\"Score distribution:\\n\")\n",
    "            for score, count in score_counts.items():\n",
    "                percentage = (count / len(low_scores_df)) * 100\n",
    "                f.write(f\"  Score {score}: {count} cases ({percentage:.1f}%)\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # DETAILED METRIC ANALYSIS\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"DETAILED ANALYSIS BY METRIC\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            for metric, results in analysis_results.items():\n",
    "                f.write(f\"{metric.upper()} - {results['total_cases']} LOW SCORE CASES\\n\")\n",
    "                f.write(\"-\" * 60 + \"\\n\")\n",
    "                f.write(f\"Average score: {results['avg_score']:.2f}\\n\")\n",
    "                f.write(f\"Score distribution: {results['score_distribution']}\\n\")\n",
    "                f.write(f\"Affected participants: {len(set(results['participants']))}\\n\\n\")\n",
    "                \n",
    "                if results['keyword_frequencies']:\n",
    "                    f.write(\"Common problem indicators (keyword analysis):\\n\")\n",
    "                    sorted_keywords = sorted(results['keyword_frequencies'].items(), \n",
    "                                           key=lambda x: x[1], reverse=True)\n",
    "                    for keyword, count in sorted_keywords:\n",
    "                        percentage = (count / results['total_cases']) * 100\n",
    "                        f.write(f\"  '{keyword}': {count} cases ({percentage:.1f}%)\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "                else:\n",
    "                    f.write(\"No common problem indicators identified.\\n\\n\")\n",
    "                \n",
    "                f.write(\"SAMPLE EXPLANATIONS:\\n\")\n",
    "                f.write(\".\" * 40 + \"\\n\")\n",
    "                for i, sample in enumerate(results['sample_explanations'], 1):\n",
    "                    f.write(f\"{i}. Participant {sample['participant_id']} (Score: {sample['score']})\\n\")\n",
    "                    f.write(f\"   {sample['explanation']}\\n\\n\")\n",
    "                \n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # MOST PROBLEMATIC PARTICIPANTS\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"PARTICIPANTS WITH MULTIPLE ISSUES\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            participant_counts = low_scores_df.groupby('participant_id').size().sort_values(ascending=False)\n",
    "            multi_problem = participant_counts[participant_counts > 1]\n",
    "            \n",
    "            if len(multi_problem) > 0:\n",
    "                f.write(f\"Found {len(multi_problem)} participants with multiple low scores:\\n\\n\")\n",
    "                for pid, count in multi_problem.items():\n",
    "                    participant_data = low_scores_df[low_scores_df['participant_id'] == pid]\n",
    "                    f.write(f\"Participant {pid}: {count} low scores\\n\")\n",
    "                    for _, row in participant_data.iterrows():\n",
    "                        f.write(f\"  - {row['metric']}: Score {row['score']}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "            else:\n",
    "                f.write(\"No participants have multiple low scores.\\n\")\n",
    "                f.write(\"Each low score case is from a different participant.\\n\")\n",
    "        \n",
    "        print(f\"Detailed analysis report saved to: {report_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating report: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"LOW SCORE CASES ANALYZER\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Input file: {INPUT_CSV_PATH}\")\n",
    "    print(f\"Output CSV: {OUTPUT_CSV_PATH}\")\n",
    "    print(f\"Report file: {ANALYSIS_REPORT_PATH}\")\n",
    "    print(f\"Score threshold: ≤ {THRESHOLD}\")\n",
    "    print()\n",
    "    \n",
    "    # Check if input file exists\n",
    "    if not os.path.exists(INPUT_CSV_PATH):\n",
    "        print(f\"ERROR: Input file not found: {INPUT_CSV_PATH}\")\n",
    "        print(\"Please update the INPUT_CSV_PATH variable with the correct path.\")\n",
    "        return\n",
    "    \n",
    "    # Step 1: Extract low score cases\n",
    "    print(\"Step 1: Extracting low score cases...\")\n",
    "    low_scores_df = extract_low_scores(INPUT_CSV_PATH, OUTPUT_CSV_PATH, THRESHOLD)\n",
    "    \n",
    "    if low_scores_df is not None and len(low_scores_df) > 0:\n",
    "        # Step 2: Analyze explanations by metric\n",
    "        print(\"\\nStep 2: Analyzing explanation patterns...\")\n",
    "        analysis_results = analyze_explanations_by_metric(low_scores_df)\n",
    "        \n",
    "        # Step 3: Create detailed report\n",
    "        print(\"\\nStep 3: Creating comprehensive report...\")\n",
    "        create_detailed_report(low_scores_df, analysis_results, ANALYSIS_REPORT_PATH)\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ANALYSIS COMPLETE\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Files created:\")\n",
    "        print(f\"1. Low score cases CSV: {OUTPUT_CSV_PATH}\")\n",
    "        print(f\"   - Contains {len(low_scores_df)} low score cases\")\n",
    "        print(f\"   - Columns: participant_id, metric, score, explanation\")\n",
    "        print(f\"2. Analysis report: {ANALYSIS_REPORT_PATH}\")\n",
    "        print(f\"   - Comprehensive analysis with patterns and samples\")\n",
    "        print()\n",
    "        print(\"Key findings:\")\n",
    "        print(f\"- {low_scores_df['participant_id'].nunique()} participants have scores ≤ {THRESHOLD}\")\n",
    "        print(f\"- {len(low_scores_df)} total low score instances\")\n",
    "        \n",
    "        metric_counts = low_scores_df['metric'].value_counts()\n",
    "        worst_metric = metric_counts.index[0]\n",
    "        print(f\"- Most problematic metric: {worst_metric} ({metric_counts[worst_metric]} cases)\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nNo low score cases found with threshold ≤ {THRESHOLD}\")\n",
    "        print(\"Analysis complete - no issues detected.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipsy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
